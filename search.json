[
  {
    "objectID": "notes/ProbabilityDistributionNotes.html",
    "href": "notes/ProbabilityDistributionNotes.html",
    "title": "Probability Distribution Notes",
    "section": "",
    "text": "Bernoulli\nBeta\nCauchy\nChi Squared\nExponential\nGamma\nLognormal\nMultinomial\nNormal\nPareto\nPoisson\nt-distribution\nUniform\n\n\n\n\n\nTable\n\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport random as rnd\nimport math\nimport matplotlib.pyplot as plt\nfrom scipy.stats import *\nimport seaborn as sns \nsns.set()\n\n\n\n\nCode\ndef plot1(x,f):\n    \"\"\"Convenient plotting of discrete distributions\n    x : x-axis values\n    f : scipy.stats pmf or pdf with parameters\"\"\" \n    plt.xlabel('x')\n    plt.ylabel('P(x)')\n    plt.vlines(x,0,f, lw=4, color='b')\n    plt.show()\n\ndef plot2(x, f, labels=[]):\n    \"\"\"Convenient plotting of continuous distributions with varying parameters\n    x : x-axis values\n    f : list of scipy.stats pmf or pdf with parameters\n    labels: parameter annotations\"\"\" \n    plt.xlabel('x')\n    plt.ylabel('P(x)')\n    for i,label in zip(f,labels):\n        plt.plot(x,i, label=label)\n    plt.legend()\n    plt.show()\n\n\n\n\n\n\n\nFor \\(k \\ge 1\\) as a given integer, \\(X\\) is uniformly distributed on \\(\\{1, ..., k\\}\\) if:\n\\[\\begin{equation}\n    f(x) = \\begin{cases}\n               1/k               & \\text{for }  x = 1, ..., k\\\\\n                0              & \\text{otherwise}\n           \\end{cases}\n\\end{equation}\\]\n\n\nCode\nk = 10\nlow, high = 1, k\nx = np.arange(1,k)\nf = randint.pmf(x,low,high)\nplot1(x,f)\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet $ X $ represent an all or nothing event. Then \\(P(X=1) =\np\\) and \\(P(X=0) = 1-p\\) for some \\(p \\in [0, 1]\\). \\(X\\sim\\) Bernoulli if:\n\\[f(x) = p^x(1-p)^{1-x} \\text{ for } x \\in \\{0, 1\\}\\]\n\n\nCode\nx = np.arange(-1,2,1)\np = .7\nf = bernoulli.pmf(x,p)\nplot1(x,f)\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe binomial distribution answers the question, given a probabilities \\(p\\) and \\(1-p\\) for outcomes 1 and 2 of a Bernoulli trial, with N trials, what are the odds of seeing \\(x\\) and \\(N-x\\) of outcomes 1 and 2, respectively?\nSuppose we have a coin which falls on heads with probability \\(p \\in [0,1]\\) and flip it \\(n\\) times. Assuming the tosses are independent of each other we can represent the number of heads as \\(X\\) and probability of getting \\(x\\) heads as \\(f(X=x)\\). Then:\n\\[\\begin{equation}\n    f(x) = \\begin{cases}\n               \\binom{n}{x}p^x(1-p)^{n-x}   & \\text{for }  x = 0, ..., n\\\\\n                0              & \\text{otherwise}\n           \\end{cases}\n\\end{equation}\\]\nWe say \\(X \\text{~ Binomial}(n,p)\\) where \\(X\\) is the random variable and \\(n\\) and \\(p\\) are the parameters. Adapting our notation, another form is:\n\\[\\begin{equation}\n    f(x|n, p) = \\begin{cases}\n               \\binom{n}{x}p^x(1-p)^{n-x}   & \\text{for }  x = 0, ..., n\\\\\n                0              & \\text{otherwise}\n           \\end{cases}\n\\end{equation}\\]\n\n\nCode\nn=10\nx = np.arange(1,11,1)\np = .5\nf = binom.pmf(x,n,p)\nplot1(x,f)\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is a generalization of the binomial distribution, where now, instead of two outcomes, there are \\(m\\) outcomes, with probabilities \\(p_i\\) where \\(i \\in [1,2,... m]\\). If there are a total of \\(N\\) trials, then the probability of having the \\(N\\) trials split into a specific set of outcomes \\(x_i\\) is given by:\n\\[f(x_1, x_2,...,x_m) = \\dfrac{m!}{x_1!x_2!...x_m!}p_1^{x_1}p_2^{x_2}...p_m^{x_m} = m!\\prod^m_{i=1}\\dfrac{p_i^{x_i}}{x_i!}\\]\nSince it’s a multidimensional function, it’s difficult to visualize. Written in vector notation with outcome vector \\(\\bf x\\) and probability vector \\(\\bf p\\) as parameter, both of dimension \\(m\\):\n\\[f(\\mathbf{x} | \\mathbf{p}) = m!\\prod^m_{i=1}\\dfrac{p_i^{x_i}}{x_i!}\\]\n\n\n\n\nFor events like flipping a coin till first heads, where \\(X\\) is the number of flips, the probability to meet the condition with \\(k\\) flips is given by:\n\\[P(X=k) = p(1-p)^{k-1}, \\text{     } k\\ge1\\]\nIt’s also worth noting that:\n\\[\\sum^{\\infty}_{k=1}P(X=k) = p\\sum^{\\infty}_{k=0}(1-p)^k = \\dfrac{p}{1-(1-p)} = 1\\]\n\n\nCode\np = .1\nf = geom.pmf(x,p)\nplot1(x,f)\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn contrast to the binomial distribution, for draws without replacement, the probability of a successful draw changes with each draw. Given a total of \\(N\\) objects with \\(K\\) success objects and \\(N-K\\) others, the probability of \\(k\\) successes in \\(n\\) draws is given by the total combinations of drawing \\(k\\) out of \\(K\\) successes and \\(n-k\\) out of \\(N-K\\) other objects out of the total ways to draw \\(n\\) out of \\(N\\) objects:\n\\[ P(X = k|n,N,K) = \\dfrac{\\binom{K}{k}\\binom{N-K}{n-k}}{\\binom{N}{n}}\\]\n\n\nCode\nN, K, n = 100, 5, 10\nk = np.arange(0,K+1,1)\nf = hypergeom.pmf(k, N, n, K)\nplot1(k,f)\n\n\n\n\n\n\n\n\n\n\n\n\n\nOften used to model rare events, \\(X \\text{~ Poisson }(\\lambda)\\) is defined by:\n\\[ f(x) = \\exp(-\\lambda)\\frac{\\lambda^x}{x!}\\]\nIt’s also worth noting that:\n\\[\\sum^{\\infty}_{x=0}f(x) = \\exp(-\\lambda) \\sum^{\\infty}_{x=0}\\frac{\\lambda^x}{x!} = \\exp(-\\lambda)\\exp(\\lambda)= 1\\]\n\n\nCode\nx = np.arange(1,200,1)\nk= 50\nf = poisson.pmf(x,k)\nplot1(x,f)\nplot1(x,poisson.pmf(x,5))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(X\\) has a Uniform \\((a,b)\\) distribution, or \\(X \\text{ ~Uniform }(a,b)\\) if:\n\\[\\begin{equation}\n    f(x) = \\begin{cases}\n               \\frac{1}{b-a}   & \\text{for }  x \\in[a,b]\\\\\n                0              & \\text{otherwise}\n           \\end{cases}\n\\end{equation}\\]\nwhere \\(a &lt; b\\). The distribution function is:\n\\[\\begin{equation}\n    F(x) = \\begin{cases}\n               0               & x &lt; a\\\\\n              \\frac{x-a}{b-a}  & x \\in [a,b]\\\\\n              1                & x &gt; b\n              \\end{cases}\n\\end{equation}\\]\n\n\nCode\nloc = 0\nscale = 11\nx = np.arange(loc,loc+scale)\nf = uniform.pdf(x, loc,scale)\nplot2(x,[f], [None])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(X\\) has a Normal or Gaussian distribution with parameters \\(\\mu\\) and \\(\\sigma\\), denoted by \\(X \\text{ ~ }N(\\mu, \\sigma^2)\\) if:\n\\[f(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp\\Big\\{-\\frac{1}{2\\sigma^2}(x-\\mu)^2\\Big\\}, \\quad\\text{  } x \\in \\mathbb{R} \\]\n\n\nCode\nu_ = [0, 0, 0, 0, 1, 2,]\nsigma_ = [1,2,5,1,1]\nx = np.arange(-10,10,.1)\nf = [norm.pdf(x,u,sigma) for u,sigma in zip(u_, sigma_)]\nlabels = [r'$\\mu = %d$, $\\sigma = %d$' %(u,sigma) for u, sigma in zip(u_, sigma_)]\nplot2(x,f, labels)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(X\\) has an Exponential distribution with parameber \\(\\beta\\), noted by \\(X \\text{ ~ Exp}(\\beta)\\) if:\n\\[f(x) = \\dfrac{1}{\\beta}\\exp\\Big(-\\frac{x}{\\beta}\\Big), \\quad \\text{     } x &gt;0\\]\n\n\nCode\nx = np.arange(.1,10,.01)\nb = np.arange(0,5, .5)\nf = [expon.pdf(x, B) for B in b]\nlabels = [r'$\\beta = %.1f$' % B for B in b]\nplot2(x,f,labels)\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor \\(\\alpha &gt; 0\\), the Gamma function is defined as:\n\\[ \\Gamma(\\alpha) \\equiv \\int^\\infty_0 y^{\\alpha -1}\\exp(-y)dy\\]\n\\(X\\) has a Gamma distribution with parameters \\(\\alpha\\) and \\(\\beta\\), or \\(X \\text{ ~ Gamma}(\\alpha, \\beta)\\), if:\n\\[ f(x) = \\dfrac{1}{\\beta^\\alpha \\Gamma(\\alpha)}x^{\\alpha-1}\\exp\\Big(-\\dfrac{x}{\\beta}\\Big), \\quad \\text{  } x&gt;0\\]\nSpecial cases: - The Exponential distribution is the special case of \\(\\text{Gamma}(1, \\beta)\\) - The Chi-square distribution is a special case of \\(\\text{Gamma}(p/2, 2)\\) - Using \\(y = u^2\\), \\(dy = 2udu\\), and the result $^_{-}({-u^2}) = $\n\n\nCode\nx = np.arange(.1,100,.01)\nA = np.arange(2,10,1)# shape\nB = np.arange(4,5,1) # scale\nf = [gamma.pdf(x,a=a,scale=b) for a in A for b in B]\nlabels = [r'$\\alpha = %.1f$, $\\beta = %.1f$' % (a,b) for a in A for b in B]\nplot2(x,f, labels)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(X\\) has a \\(\\chi^2\\) distribution with \\(p\\) degrees of freedom, denoted by \\(X \\text{ ~ }\\chi_p^2\\) if:\n\\[f(x) = \\dfrac{1}{\\Gamma(p/2) 2^{p/2}}x^{(p/2)-1}\\exp(-x/2) \\text{, } \\quad x&gt;0\\]\n\n\nThis distribution is related to the normal distribution in that if a variable \\(X\\) has a standard normal distribution, its square follows a chi-square distribution with one degree of freedom: \\(Y = X^2 \\text{~} \\chi^2_1\\).\nOften, we compare how far a value is from an expected one by finding the square of their difference. If the variable converges to a normal distribution, the square of the difference can be considered a chi squared distribution.\n\n\nCode\nx= np.arange(.01,20,.01)\ndf_ = np.arange(3,6,1)\nx = np.arange(0.01,15,.01)\nf = [chi2.pdf(x,df) for df in df_]\nlabels = [r'$\\nu = %.1f$' %df for df in df_]\nplot2(x,f, labels)\n\n\n\n\n\n\n\n\n\n ### Beta Distribution\nThe Beta function is defined as \\[\\text{Beta}(a,b) \\equiv \\int^\\infty_0 t^{a-1}(1-t)^{b-1} = \\dfrac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}\\]\nSee here for the proof of the last equality which involves change of variables in evaluating the product of two Gamma functions.\n\\(X\\) has a Beta distribution with parameters \\(\\alpha &gt; 0\\) and \\(\\beta &gt; 0\\), denoted by \\(X \\text{ ~ Beta} (\\alpha, \\beta)\\) if:\n\\[f(x) = \\dfrac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1}, \\quad\\text{ } 0&lt;x&lt;1\\]\nAlternatively, we can view the the beta distribution as a distribution over probabilities, \\(p\\) and parameters of number of outcomes \\(u_1\\) and \\(u_2\\):\n\\[f(p|u_1,u_2) = \\dfrac{\\Gamma(u_1 + u_2)}{\\Gamma(u_1)\\Gamma(u_2)}p^{u_1-1}(1-p)^{u_2-1}, \\quad\\text{ } 0&lt;p&lt;1\\]\n\n\nCode\nx = np.arange(0.1,.99,.001)\nA = np.arange(1,6,1)# shape\n#B = np.arange(1,6,1) # scale\nB = 6-A\n#f = [beta.pdf(x,a=a,b=b) for a in A for b in B]\nf = [beta.pdf(x, a=a,b=b) for a,b in zip(A,B)]\n#labels = [r'$\\alpha = %.1f$, $\\beta = %.1f$' % (a,b) for a in A for b in B]\nlabels = [r'$\\alpha = %.1f$, $\\beta = %.1f$' % (a,b) for a,b in zip(A,B)]\n\nplot2(x,f, labels)\n\n\n\n\n\n\n\n\n\n ### Dirichlet Distribution\nThe Dirichlet distribution is an extention of the beta distribution to more dimensions. It’s a density over a K-dimensional vector, \\(\\mathbf{p}\\), with \\(\\sum p_i = 1\\). It is defined as:\n\\[f(\\mathbf{p}|\\alpha \\mathbf{m}) = \\dfrac{1}{Z(\\alpha \\mathbf{m})}\\prod_{i=1}^K p^{\\alpha m_i - 1}_i \\equiv \\text{Dirichlet}^{(K)}(\\mathbf{p}|\\alpha\\mathbf{m}), \\quad\\text{ } 0&lt;p&lt;1\\]\nwhere \\(\\mathbf{m}\\) is normalized such that \\(\\sum m_i = 1\\) and \\(\\alpha &gt; 0\\). The normalizing constant is defined as:\n\\[Z(\\alpha\\mathbf{m}) = \\prod_i \\Gamma(\\alpha m_i)/\\Gamma(\\alpha)\\]\nThe parameter \\(\\mathbf{m}\\) is the mean of the probability distribution while the constant \\(\\alpha\\)\n\n\n\n\n \n\\(X\\) has a \\(t\\) distribution with \\(v\\) degrees of freedom, denoted by \\(X \\text{ ~ }t_v\\) - if:\n\\[f(x) = \\dfrac{\\Gamma\\Big(\\dfrac{v+1}{2}\\Big)}{\\Gamma \\Big(\\dfrac{v}{2}\\Big)}\\dfrac{1}{\\sqrt{\\nu \\pi}} \\dfrac{1}{\\Big(1+\\dfrac{x^2}{v}\\Big)^{(v+1)/2}}\\]\nThe Cauchy is a special case of the \\(t\\) distribution with \\(v = 1\\). The density is:\n\\[f(x) = \\dfrac{1}{\\pi (1+x^2)}\\]\nWe can see that it’s a probability density since it’s never negative, and it integrates to unity:\n\\[\\int^\\infty_{-\\infty} f(x)dx = \\dfrac{1}{\\pi}\\int^\\infty_{-\\infty}\\dfrac{1}{\\pi (1+x^2)} = \\dfrac{1}{\\pi}\\int^\\infty_{-\\infty}\\dfrac{d\\arctan}{dx} = \\dfrac{1}{\\pi}[\\arctan(\\infty) - \\arctan(-\\infty)] = \\dfrac{1}{\\pi}\\Big[\\dfrac{\\pi}{2}-\\Big(-\\dfrac{\\pi}{2}\\Big)\\Big]=1\\]\nWhile the mean and variance do not exist for the Cauchy distribution, it often is seen with location parameter \\(\\mu\\) and scale parameter \\(\\sigma\\) by the transformation \\(x \\rightarrow \\dfrac{x-\\mu}{\\sigma}\\), and \\(dx \\rightarrow \\frac{1}{\\sigma}\\) to obtain the modified pdf:\n\\[f(x) = \\dfrac{1}{\\sigma\\pi \\Big(1+\\big[\\frac{x-\\mu}{\\sigma}\\big]^2\\Big)}\\]\n\n\nCode\ndf_ = np.arange(1,5,1)\nx = np.arange(-10,10,.01)\nf = [t.pdf(x,df) for df in df_]\nlabels = [r'$\\nu = %.1f$' %df for df in df_]\nplot2(x,f, labels)\n\n\n\n\n\n\n\n\n\n\n\nLet \\(X|W\\) ~ \\(N(0,W)\\) and let \\(W ~ Gamma(\\nu/2, \\nu/2)\\). Then the distribution of \\(X\\) is:\n\\[\\begin{align}\nf_X(x) &= \\int^{\\infty}_0 \\\\\n    &\\propto \\frac{1}{\\sqrt{w}}\\exp\\big(-\\frac{x^2}{2w}\\big)\\\\\n\\end{align}\\]\n\n\n\n\n\n\\[\\begin{equation}\n    f(x) = \\begin{cases}\n               \\dfrac{\\alpha x_m^\\alpha}{x^{\\alpha+1}}   & \\text{for }  x \\ge x_m\\\\\n                0              & \\text{for } x &lt; x_m\n           \\end{cases}\n\\end{equation}\\]\n\n\nCode\nx = np.arange(0.01,10,.01)\na_ = np.arange(0,6,1)\nf = [pareto.pdf(x,a, scale=1) for a in a_]\nlabels = [r'$\\alpha = %.1f$' %a for a in a_]\nplot2(x,f, labels)\n\n\nNameError: name 'np' is not defined\n\n\n ### Log-normal Distribution\n\\(X\\) has a Log-normal distribution with parameters \\(\\mu\\) and \\(\\sigma\\), denoted by $ X (, )$ if \\(\\ln X \\sim N(\\mu, \\sigma^2)\\). This results in a probability distribution function of:\n\\[f(x) = \\frac{1}{x\\sigma \\sqrt{2 \\pi}} \\exp\\Big\\{-\\frac{1}{2\\sigma^2}(\\ln x-\\mu)^2\\Big\\}, \\quad\\text{  } x \\in \\mathbb{R} \\]\n\n\nCode\nu_ = [0,0,1,1]\nsigma_ = [.1,.25, .5, 1]\nx = np.arange(0,5,.01)\nf = [lognorm.pdf(x,loc=u,s=sigma) for u,sigma in zip(u_, sigma_)]\nlabels = [r'$\\mu = %d$, $\\sigma = %.2f$' %(u,sigma) for u, sigma in zip(u_, sigma_)]\nplot2(x,f, labels)\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor convenience \\(\\overline{X} = \\dfrac{\\sum^n_{i=1}X_i}{n}\\) and \\(S^2 = \\dfrac{\\sum^n_{i=1} (X_i-\\overline{X})^2}{n}\\). \n\n\n\nDistribution\n\n\nMean\n\n\nVariance\n\n\nMaximum Likelihood Estimator\n\n\nMoment Generating Function\n\n\n\n\nPoint mass at a\n\n\na\n\n\n0\n\n\nN\n\n\nN\n\n\n\n\nBernoulli (p)\n\n\np\n\n\np(1-p)\n\n\n\\(\\hat{p} =\\overline{X}\\)\n\n\n\\(1 + pe^t - p\\)\n\n\n\n\nBinomial (n, p)\n\n\nnp\n\n\nnp(1-p)\n\n\n\\(\\hat{p} =\\overline{X}\\)\n\n\n\\((1 + pe^t - p)^n\\)\n\n\n\n\nGeometric\n\n\n1/p\n\n\n\\(\\dfrac{(1-p)}{p^2}\\)\n\n\n\\(\\hat{p} =\\dfrac{1}{\\overline{X}}\\)\n\n\nN/a\n\n\n\n\nHypergeometric\n\n\n\\(n\\dfrac{K}{N}\\)\n\n\n\\(n\\dfrac{K}{N}\\dfrac{(N-K)}{N}\\dfrac{N-n}{N-1}\\)\n\n\n\n\n\n\n\n\nPoisson \\((\\lambda)\\)\n\n\n\\(\\lambda\\)\n\n\n\\(\\lambda\\)\n\n\n\\(\\hat{\\lambda} = \\overline{X}\\)\n\n\n\\(\\exp\\Big(\\lambda[e^t-1]\\Big)\\)\n\n\n\n\nUniform \\((a, b)\\)\n\n\n\\[\\dfrac{(a+b)}{2}\\]\n\n\n\\[\\dfrac{(b-a)^2}{12}\\]\n\n\n\\[\\begin{align} \\text{for U}&(0,\\theta) \\\\ \\hat{\\theta} &= \\max\\{X_i\\}\\end{align}\\]\n\n\n\n\nNormal \\((\\mu, \\sigma^2)\\)\n\n\n\\(\\mu\\)\n\n\n\\(\\sigma^2\\)\n\n\n\\[\\begin{align}\\hat{p}&=\\overline{X}\\\\\n   \\quad \\hat{\\sigma} &=S\\end{align}\\]\n\n\n\\(\\exp\\Big(\\mu t + \\frac{\\sigma^2 t^2}{2}\\Big)\\)\n\n\n\n\nExponential \\((\\beta)\\)\n\n\n\\(\\beta\\)\n\n\n\\(\\beta^2\\)\n\n\n\\[\\hat{\\beta}= \\overline{X}\\]\n\n\n\\[\\dfrac{1}{(1-\\beta t)}\\]\n\n\n\n\nGamma \\((\\alpha, \\beta)\\)\n\n\n\\(\\alpha \\beta\\)\n\n\n\\(\\alpha \\beta^2\\)\n\n\n\\(\\hat{\\beta}= \\dfrac{\\overline{X}}{\\alpha}\\)\n\n\n\\(\\dfrac{1}{(1-\\beta t)^\\alpha}\\)\n\n\n\n\nBeta \\((\\alpha, \\beta)\\)\n\n\n\\(\\dfrac{\\alpha}{(\\alpha + \\beta)}\\)\n\n\n\\(\\dfrac{\\alpha \\beta}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)}\\)\n\n\n\n\n\\(t_{\\nu}\\)\n\n\n0 if \\((\\nu &gt; 1)\\)\n\n\n\\(\\dfrac{\\nu}{\\nu - 2}\\)\n\n\n\n\n\\(\\chi^2_p\\)\n\n\np\n\n\n2p\n\n\n\n\n\\(\\dfrac{1}{(1-2 t)^{p/2}}\\)\n\n\n\n\nPareto\n\n\n\\(\\dfrac{\\alpha x^\\alpha_m}{\\alpha-1}\\)\n\n\n\\(\\dfrac{\\alpha x^\\alpha_m}{(\\alpha-2)(\\alpha-1)^2}\\)\n\n\n\\[\\begin{align}\\hat{x}_m &= x_{(1)}\\\\\n          \\hat{\\alpha}&= \\dfrac{1}{\\frac{1}{n}\\sum^n_{i=1}{\\ln x_i}- \\ln(x_1)} \\end{align}\\]\n\n\n\n\nMultinomial \\((n, \\mathbf{p})\\)\n\n\n\\(n\\mathbf{p}\\)\n\n\n\\[\\begin{align}M_{ii} &= np_i(1-p_i)\\\\\n          M_{ij}&= -np_i p_j \\end{align}\\]\n\n\n\\(\\hat{\\mathbf{p}} =\\overline{X}\\)\n\n\n\n\nMultivariate Normal \\((\\mu, \\Sigma)\\)\n\n\n\\(\\mu\\)\n\n\nMatrix \\(\\Sigma_{ij} = \\text{Cov}(X_i, X_j)\\)"
  },
  {
    "objectID": "notes/ProbabilityDistributionNotes.html#distribution-links",
    "href": "notes/ProbabilityDistributionNotes.html#distribution-links",
    "title": "Probability Distribution Notes",
    "section": "",
    "text": "Bernoulli\nBeta\nCauchy\nChi Squared\nExponential\nGamma\nLognormal\nMultinomial\nNormal\nPareto\nPoisson\nt-distribution\nUniform"
  },
  {
    "objectID": "notes/ProbabilityDistributionNotes.html#means-and-variances-table",
    "href": "notes/ProbabilityDistributionNotes.html#means-and-variances-table",
    "title": "Probability Distribution Notes",
    "section": "",
    "text": "Table\n\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport random as rnd\nimport math\nimport matplotlib.pyplot as plt\nfrom scipy.stats import *\nimport seaborn as sns \nsns.set()\n\n\n\n\nCode\ndef plot1(x,f):\n    \"\"\"Convenient plotting of discrete distributions\n    x : x-axis values\n    f : scipy.stats pmf or pdf with parameters\"\"\" \n    plt.xlabel('x')\n    plt.ylabel('P(x)')\n    plt.vlines(x,0,f, lw=4, color='b')\n    plt.show()\n\ndef plot2(x, f, labels=[]):\n    \"\"\"Convenient plotting of continuous distributions with varying parameters\n    x : x-axis values\n    f : list of scipy.stats pmf or pdf with parameters\n    labels: parameter annotations\"\"\" \n    plt.xlabel('x')\n    plt.ylabel('P(x)')\n    for i,label in zip(f,labels):\n        plt.plot(x,i, label=label)\n    plt.legend()\n    plt.show()"
  },
  {
    "objectID": "notes/ProbabilityDistributionNotes.html#discrete-distributions",
    "href": "notes/ProbabilityDistributionNotes.html#discrete-distributions",
    "title": "Probability Distribution Notes",
    "section": "",
    "text": "For \\(k \\ge 1\\) as a given integer, \\(X\\) is uniformly distributed on \\(\\{1, ..., k\\}\\) if:\n\\[\\begin{equation}\n    f(x) = \\begin{cases}\n               1/k               & \\text{for }  x = 1, ..., k\\\\\n                0              & \\text{otherwise}\n           \\end{cases}\n\\end{equation}\\]\n\n\nCode\nk = 10\nlow, high = 1, k\nx = np.arange(1,k)\nf = randint.pmf(x,low,high)\nplot1(x,f)\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet $ X $ represent an all or nothing event. Then \\(P(X=1) =\np\\) and \\(P(X=0) = 1-p\\) for some \\(p \\in [0, 1]\\). \\(X\\sim\\) Bernoulli if:\n\\[f(x) = p^x(1-p)^{1-x} \\text{ for } x \\in \\{0, 1\\}\\]\n\n\nCode\nx = np.arange(-1,2,1)\np = .7\nf = bernoulli.pmf(x,p)\nplot1(x,f)\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe binomial distribution answers the question, given a probabilities \\(p\\) and \\(1-p\\) for outcomes 1 and 2 of a Bernoulli trial, with N trials, what are the odds of seeing \\(x\\) and \\(N-x\\) of outcomes 1 and 2, respectively?\nSuppose we have a coin which falls on heads with probability \\(p \\in [0,1]\\) and flip it \\(n\\) times. Assuming the tosses are independent of each other we can represent the number of heads as \\(X\\) and probability of getting \\(x\\) heads as \\(f(X=x)\\). Then:\n\\[\\begin{equation}\n    f(x) = \\begin{cases}\n               \\binom{n}{x}p^x(1-p)^{n-x}   & \\text{for }  x = 0, ..., n\\\\\n                0              & \\text{otherwise}\n           \\end{cases}\n\\end{equation}\\]\nWe say \\(X \\text{~ Binomial}(n,p)\\) where \\(X\\) is the random variable and \\(n\\) and \\(p\\) are the parameters. Adapting our notation, another form is:\n\\[\\begin{equation}\n    f(x|n, p) = \\begin{cases}\n               \\binom{n}{x}p^x(1-p)^{n-x}   & \\text{for }  x = 0, ..., n\\\\\n                0              & \\text{otherwise}\n           \\end{cases}\n\\end{equation}\\]\n\n\nCode\nn=10\nx = np.arange(1,11,1)\np = .5\nf = binom.pmf(x,n,p)\nplot1(x,f)\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is a generalization of the binomial distribution, where now, instead of two outcomes, there are \\(m\\) outcomes, with probabilities \\(p_i\\) where \\(i \\in [1,2,... m]\\). If there are a total of \\(N\\) trials, then the probability of having the \\(N\\) trials split into a specific set of outcomes \\(x_i\\) is given by:\n\\[f(x_1, x_2,...,x_m) = \\dfrac{m!}{x_1!x_2!...x_m!}p_1^{x_1}p_2^{x_2}...p_m^{x_m} = m!\\prod^m_{i=1}\\dfrac{p_i^{x_i}}{x_i!}\\]\nSince it’s a multidimensional function, it’s difficult to visualize. Written in vector notation with outcome vector \\(\\bf x\\) and probability vector \\(\\bf p\\) as parameter, both of dimension \\(m\\):\n\\[f(\\mathbf{x} | \\mathbf{p}) = m!\\prod^m_{i=1}\\dfrac{p_i^{x_i}}{x_i!}\\]\n\n\n\n\nFor events like flipping a coin till first heads, where \\(X\\) is the number of flips, the probability to meet the condition with \\(k\\) flips is given by:\n\\[P(X=k) = p(1-p)^{k-1}, \\text{     } k\\ge1\\]\nIt’s also worth noting that:\n\\[\\sum^{\\infty}_{k=1}P(X=k) = p\\sum^{\\infty}_{k=0}(1-p)^k = \\dfrac{p}{1-(1-p)} = 1\\]\n\n\nCode\np = .1\nf = geom.pmf(x,p)\nplot1(x,f)\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn contrast to the binomial distribution, for draws without replacement, the probability of a successful draw changes with each draw. Given a total of \\(N\\) objects with \\(K\\) success objects and \\(N-K\\) others, the probability of \\(k\\) successes in \\(n\\) draws is given by the total combinations of drawing \\(k\\) out of \\(K\\) successes and \\(n-k\\) out of \\(N-K\\) other objects out of the total ways to draw \\(n\\) out of \\(N\\) objects:\n\\[ P(X = k|n,N,K) = \\dfrac{\\binom{K}{k}\\binom{N-K}{n-k}}{\\binom{N}{n}}\\]\n\n\nCode\nN, K, n = 100, 5, 10\nk = np.arange(0,K+1,1)\nf = hypergeom.pmf(k, N, n, K)\nplot1(k,f)\n\n\n\n\n\n\n\n\n\n\n\n\n\nOften used to model rare events, \\(X \\text{~ Poisson }(\\lambda)\\) is defined by:\n\\[ f(x) = \\exp(-\\lambda)\\frac{\\lambda^x}{x!}\\]\nIt’s also worth noting that:\n\\[\\sum^{\\infty}_{x=0}f(x) = \\exp(-\\lambda) \\sum^{\\infty}_{x=0}\\frac{\\lambda^x}{x!} = \\exp(-\\lambda)\\exp(\\lambda)= 1\\]\n\n\nCode\nx = np.arange(1,200,1)\nk= 50\nf = poisson.pmf(x,k)\nplot1(x,f)\nplot1(x,poisson.pmf(x,5))"
  },
  {
    "objectID": "notes/ProbabilityDistributionNotes.html#continuous-distributions",
    "href": "notes/ProbabilityDistributionNotes.html#continuous-distributions",
    "title": "Probability Distribution Notes",
    "section": "",
    "text": "\\(X\\) has a Uniform \\((a,b)\\) distribution, or \\(X \\text{ ~Uniform }(a,b)\\) if:\n\\[\\begin{equation}\n    f(x) = \\begin{cases}\n               \\frac{1}{b-a}   & \\text{for }  x \\in[a,b]\\\\\n                0              & \\text{otherwise}\n           \\end{cases}\n\\end{equation}\\]\nwhere \\(a &lt; b\\). The distribution function is:\n\\[\\begin{equation}\n    F(x) = \\begin{cases}\n               0               & x &lt; a\\\\\n              \\frac{x-a}{b-a}  & x \\in [a,b]\\\\\n              1                & x &gt; b\n              \\end{cases}\n\\end{equation}\\]\n\n\nCode\nloc = 0\nscale = 11\nx = np.arange(loc,loc+scale)\nf = uniform.pdf(x, loc,scale)\nplot2(x,[f], [None])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(X\\) has a Normal or Gaussian distribution with parameters \\(\\mu\\) and \\(\\sigma\\), denoted by \\(X \\text{ ~ }N(\\mu, \\sigma^2)\\) if:\n\\[f(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp\\Big\\{-\\frac{1}{2\\sigma^2}(x-\\mu)^2\\Big\\}, \\quad\\text{  } x \\in \\mathbb{R} \\]\n\n\nCode\nu_ = [0, 0, 0, 0, 1, 2,]\nsigma_ = [1,2,5,1,1]\nx = np.arange(-10,10,.1)\nf = [norm.pdf(x,u,sigma) for u,sigma in zip(u_, sigma_)]\nlabels = [r'$\\mu = %d$, $\\sigma = %d$' %(u,sigma) for u, sigma in zip(u_, sigma_)]\nplot2(x,f, labels)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(X\\) has an Exponential distribution with parameber \\(\\beta\\), noted by \\(X \\text{ ~ Exp}(\\beta)\\) if:\n\\[f(x) = \\dfrac{1}{\\beta}\\exp\\Big(-\\frac{x}{\\beta}\\Big), \\quad \\text{     } x &gt;0\\]\n\n\nCode\nx = np.arange(.1,10,.01)\nb = np.arange(0,5, .5)\nf = [expon.pdf(x, B) for B in b]\nlabels = [r'$\\beta = %.1f$' % B for B in b]\nplot2(x,f,labels)\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor \\(\\alpha &gt; 0\\), the Gamma function is defined as:\n\\[ \\Gamma(\\alpha) \\equiv \\int^\\infty_0 y^{\\alpha -1}\\exp(-y)dy\\]\n\\(X\\) has a Gamma distribution with parameters \\(\\alpha\\) and \\(\\beta\\), or \\(X \\text{ ~ Gamma}(\\alpha, \\beta)\\), if:\n\\[ f(x) = \\dfrac{1}{\\beta^\\alpha \\Gamma(\\alpha)}x^{\\alpha-1}\\exp\\Big(-\\dfrac{x}{\\beta}\\Big), \\quad \\text{  } x&gt;0\\]\nSpecial cases: - The Exponential distribution is the special case of \\(\\text{Gamma}(1, \\beta)\\) - The Chi-square distribution is a special case of \\(\\text{Gamma}(p/2, 2)\\) - Using \\(y = u^2\\), \\(dy = 2udu\\), and the result $^_{-}({-u^2}) = $\n\n\nCode\nx = np.arange(.1,100,.01)\nA = np.arange(2,10,1)# shape\nB = np.arange(4,5,1) # scale\nf = [gamma.pdf(x,a=a,scale=b) for a in A for b in B]\nlabels = [r'$\\alpha = %.1f$, $\\beta = %.1f$' % (a,b) for a in A for b in B]\nplot2(x,f, labels)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(X\\) has a \\(\\chi^2\\) distribution with \\(p\\) degrees of freedom, denoted by \\(X \\text{ ~ }\\chi_p^2\\) if:\n\\[f(x) = \\dfrac{1}{\\Gamma(p/2) 2^{p/2}}x^{(p/2)-1}\\exp(-x/2) \\text{, } \\quad x&gt;0\\]\n\n\nThis distribution is related to the normal distribution in that if a variable \\(X\\) has a standard normal distribution, its square follows a chi-square distribution with one degree of freedom: \\(Y = X^2 \\text{~} \\chi^2_1\\).\nOften, we compare how far a value is from an expected one by finding the square of their difference. If the variable converges to a normal distribution, the square of the difference can be considered a chi squared distribution.\n\n\nCode\nx= np.arange(.01,20,.01)\ndf_ = np.arange(3,6,1)\nx = np.arange(0.01,15,.01)\nf = [chi2.pdf(x,df) for df in df_]\nlabels = [r'$\\nu = %.1f$' %df for df in df_]\nplot2(x,f, labels)\n\n\n\n\n\n\n\n\n\n ### Beta Distribution\nThe Beta function is defined as \\[\\text{Beta}(a,b) \\equiv \\int^\\infty_0 t^{a-1}(1-t)^{b-1} = \\dfrac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}\\]\nSee here for the proof of the last equality which involves change of variables in evaluating the product of two Gamma functions.\n\\(X\\) has a Beta distribution with parameters \\(\\alpha &gt; 0\\) and \\(\\beta &gt; 0\\), denoted by \\(X \\text{ ~ Beta} (\\alpha, \\beta)\\) if:\n\\[f(x) = \\dfrac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1}, \\quad\\text{ } 0&lt;x&lt;1\\]\nAlternatively, we can view the the beta distribution as a distribution over probabilities, \\(p\\) and parameters of number of outcomes \\(u_1\\) and \\(u_2\\):\n\\[f(p|u_1,u_2) = \\dfrac{\\Gamma(u_1 + u_2)}{\\Gamma(u_1)\\Gamma(u_2)}p^{u_1-1}(1-p)^{u_2-1}, \\quad\\text{ } 0&lt;p&lt;1\\]\n\n\nCode\nx = np.arange(0.1,.99,.001)\nA = np.arange(1,6,1)# shape\n#B = np.arange(1,6,1) # scale\nB = 6-A\n#f = [beta.pdf(x,a=a,b=b) for a in A for b in B]\nf = [beta.pdf(x, a=a,b=b) for a,b in zip(A,B)]\n#labels = [r'$\\alpha = %.1f$, $\\beta = %.1f$' % (a,b) for a in A for b in B]\nlabels = [r'$\\alpha = %.1f$, $\\beta = %.1f$' % (a,b) for a,b in zip(A,B)]\n\nplot2(x,f, labels)\n\n\n\n\n\n\n\n\n\n ### Dirichlet Distribution\nThe Dirichlet distribution is an extention of the beta distribution to more dimensions. It’s a density over a K-dimensional vector, \\(\\mathbf{p}\\), with \\(\\sum p_i = 1\\). It is defined as:\n\\[f(\\mathbf{p}|\\alpha \\mathbf{m}) = \\dfrac{1}{Z(\\alpha \\mathbf{m})}\\prod_{i=1}^K p^{\\alpha m_i - 1}_i \\equiv \\text{Dirichlet}^{(K)}(\\mathbf{p}|\\alpha\\mathbf{m}), \\quad\\text{ } 0&lt;p&lt;1\\]\nwhere \\(\\mathbf{m}\\) is normalized such that \\(\\sum m_i = 1\\) and \\(\\alpha &gt; 0\\). The normalizing constant is defined as:\n\\[Z(\\alpha\\mathbf{m}) = \\prod_i \\Gamma(\\alpha m_i)/\\Gamma(\\alpha)\\]\nThe parameter \\(\\mathbf{m}\\) is the mean of the probability distribution while the constant \\(\\alpha\\)\n\n\n\n\n \n\\(X\\) has a \\(t\\) distribution with \\(v\\) degrees of freedom, denoted by \\(X \\text{ ~ }t_v\\) - if:\n\\[f(x) = \\dfrac{\\Gamma\\Big(\\dfrac{v+1}{2}\\Big)}{\\Gamma \\Big(\\dfrac{v}{2}\\Big)}\\dfrac{1}{\\sqrt{\\nu \\pi}} \\dfrac{1}{\\Big(1+\\dfrac{x^2}{v}\\Big)^{(v+1)/2}}\\]\nThe Cauchy is a special case of the \\(t\\) distribution with \\(v = 1\\). The density is:\n\\[f(x) = \\dfrac{1}{\\pi (1+x^2)}\\]\nWe can see that it’s a probability density since it’s never negative, and it integrates to unity:\n\\[\\int^\\infty_{-\\infty} f(x)dx = \\dfrac{1}{\\pi}\\int^\\infty_{-\\infty}\\dfrac{1}{\\pi (1+x^2)} = \\dfrac{1}{\\pi}\\int^\\infty_{-\\infty}\\dfrac{d\\arctan}{dx} = \\dfrac{1}{\\pi}[\\arctan(\\infty) - \\arctan(-\\infty)] = \\dfrac{1}{\\pi}\\Big[\\dfrac{\\pi}{2}-\\Big(-\\dfrac{\\pi}{2}\\Big)\\Big]=1\\]\nWhile the mean and variance do not exist for the Cauchy distribution, it often is seen with location parameter \\(\\mu\\) and scale parameter \\(\\sigma\\) by the transformation \\(x \\rightarrow \\dfrac{x-\\mu}{\\sigma}\\), and \\(dx \\rightarrow \\frac{1}{\\sigma}\\) to obtain the modified pdf:\n\\[f(x) = \\dfrac{1}{\\sigma\\pi \\Big(1+\\big[\\frac{x-\\mu}{\\sigma}\\big]^2\\Big)}\\]\n\n\nCode\ndf_ = np.arange(1,5,1)\nx = np.arange(-10,10,.01)\nf = [t.pdf(x,df) for df in df_]\nlabels = [r'$\\nu = %.1f$' %df for df in df_]\nplot2(x,f, labels)\n\n\n\n\n\n\n\n\n\n\n\nLet \\(X|W\\) ~ \\(N(0,W)\\) and let \\(W ~ Gamma(\\nu/2, \\nu/2)\\). Then the distribution of \\(X\\) is:\n\\[\\begin{align}\nf_X(x) &= \\int^{\\infty}_0 \\\\\n    &\\propto \\frac{1}{\\sqrt{w}}\\exp\\big(-\\frac{x^2}{2w}\\big)\\\\\n\\end{align}\\]\n\n\n\n\n\n\\[\\begin{equation}\n    f(x) = \\begin{cases}\n               \\dfrac{\\alpha x_m^\\alpha}{x^{\\alpha+1}}   & \\text{for }  x \\ge x_m\\\\\n                0              & \\text{for } x &lt; x_m\n           \\end{cases}\n\\end{equation}\\]\n\n\nCode\nx = np.arange(0.01,10,.01)\na_ = np.arange(0,6,1)\nf = [pareto.pdf(x,a, scale=1) for a in a_]\nlabels = [r'$\\alpha = %.1f$' %a for a in a_]\nplot2(x,f, labels)\n\n\nNameError: name 'np' is not defined\n\n\n ### Log-normal Distribution\n\\(X\\) has a Log-normal distribution with parameters \\(\\mu\\) and \\(\\sigma\\), denoted by $ X (, )$ if \\(\\ln X \\sim N(\\mu, \\sigma^2)\\). This results in a probability distribution function of:\n\\[f(x) = \\frac{1}{x\\sigma \\sqrt{2 \\pi}} \\exp\\Big\\{-\\frac{1}{2\\sigma^2}(\\ln x-\\mu)^2\\Big\\}, \\quad\\text{  } x \\in \\mathbb{R} \\]\n\n\nCode\nu_ = [0,0,1,1]\nsigma_ = [.1,.25, .5, 1]\nx = np.arange(0,5,.01)\nf = [lognorm.pdf(x,loc=u,s=sigma) for u,sigma in zip(u_, sigma_)]\nlabels = [r'$\\mu = %d$, $\\sigma = %.2f$' %(u,sigma) for u, sigma in zip(u_, sigma_)]\nplot2(x,f, labels)"
  },
  {
    "objectID": "notes/ProbabilityDistributionNotes.html#means-and-variance-of-common-distributions",
    "href": "notes/ProbabilityDistributionNotes.html#means-and-variance-of-common-distributions",
    "title": "Probability Distribution Notes",
    "section": "",
    "text": "For convenience \\(\\overline{X} = \\dfrac{\\sum^n_{i=1}X_i}{n}\\) and \\(S^2 = \\dfrac{\\sum^n_{i=1} (X_i-\\overline{X})^2}{n}\\). \n\n\n\nDistribution\n\n\nMean\n\n\nVariance\n\n\nMaximum Likelihood Estimator\n\n\nMoment Generating Function\n\n\n\n\nPoint mass at a\n\n\na\n\n\n0\n\n\nN\n\n\nN\n\n\n\n\nBernoulli (p)\n\n\np\n\n\np(1-p)\n\n\n\\(\\hat{p} =\\overline{X}\\)\n\n\n\\(1 + pe^t - p\\)\n\n\n\n\nBinomial (n, p)\n\n\nnp\n\n\nnp(1-p)\n\n\n\\(\\hat{p} =\\overline{X}\\)\n\n\n\\((1 + pe^t - p)^n\\)\n\n\n\n\nGeometric\n\n\n1/p\n\n\n\\(\\dfrac{(1-p)}{p^2}\\)\n\n\n\\(\\hat{p} =\\dfrac{1}{\\overline{X}}\\)\n\n\nN/a\n\n\n\n\nHypergeometric\n\n\n\\(n\\dfrac{K}{N}\\)\n\n\n\\(n\\dfrac{K}{N}\\dfrac{(N-K)}{N}\\dfrac{N-n}{N-1}\\)\n\n\n\n\n\n\n\n\nPoisson \\((\\lambda)\\)\n\n\n\\(\\lambda\\)\n\n\n\\(\\lambda\\)\n\n\n\\(\\hat{\\lambda} = \\overline{X}\\)\n\n\n\\(\\exp\\Big(\\lambda[e^t-1]\\Big)\\)\n\n\n\n\nUniform \\((a, b)\\)\n\n\n\\[\\dfrac{(a+b)}{2}\\]\n\n\n\\[\\dfrac{(b-a)^2}{12}\\]\n\n\n\\[\\begin{align} \\text{for U}&(0,\\theta) \\\\ \\hat{\\theta} &= \\max\\{X_i\\}\\end{align}\\]\n\n\n\n\nNormal \\((\\mu, \\sigma^2)\\)\n\n\n\\(\\mu\\)\n\n\n\\(\\sigma^2\\)\n\n\n\\[\\begin{align}\\hat{p}&=\\overline{X}\\\\\n   \\quad \\hat{\\sigma} &=S\\end{align}\\]\n\n\n\\(\\exp\\Big(\\mu t + \\frac{\\sigma^2 t^2}{2}\\Big)\\)\n\n\n\n\nExponential \\((\\beta)\\)\n\n\n\\(\\beta\\)\n\n\n\\(\\beta^2\\)\n\n\n\\[\\hat{\\beta}= \\overline{X}\\]\n\n\n\\[\\dfrac{1}{(1-\\beta t)}\\]\n\n\n\n\nGamma \\((\\alpha, \\beta)\\)\n\n\n\\(\\alpha \\beta\\)\n\n\n\\(\\alpha \\beta^2\\)\n\n\n\\(\\hat{\\beta}= \\dfrac{\\overline{X}}{\\alpha}\\)\n\n\n\\(\\dfrac{1}{(1-\\beta t)^\\alpha}\\)\n\n\n\n\nBeta \\((\\alpha, \\beta)\\)\n\n\n\\(\\dfrac{\\alpha}{(\\alpha + \\beta)}\\)\n\n\n\\(\\dfrac{\\alpha \\beta}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)}\\)\n\n\n\n\n\\(t_{\\nu}\\)\n\n\n0 if \\((\\nu &gt; 1)\\)\n\n\n\\(\\dfrac{\\nu}{\\nu - 2}\\)\n\n\n\n\n\\(\\chi^2_p\\)\n\n\np\n\n\n2p\n\n\n\n\n\\(\\dfrac{1}{(1-2 t)^{p/2}}\\)\n\n\n\n\nPareto\n\n\n\\(\\dfrac{\\alpha x^\\alpha_m}{\\alpha-1}\\)\n\n\n\\(\\dfrac{\\alpha x^\\alpha_m}{(\\alpha-2)(\\alpha-1)^2}\\)\n\n\n\\[\\begin{align}\\hat{x}_m &= x_{(1)}\\\\\n          \\hat{\\alpha}&= \\dfrac{1}{\\frac{1}{n}\\sum^n_{i=1}{\\ln x_i}- \\ln(x_1)} \\end{align}\\]\n\n\n\n\nMultinomial \\((n, \\mathbf{p})\\)\n\n\n\\(n\\mathbf{p}\\)\n\n\n\\[\\begin{align}M_{ii} &= np_i(1-p_i)\\\\\n          M_{ij}&= -np_i p_j \\end{align}\\]\n\n\n\\(\\hat{\\mathbf{p}} =\\overline{X}\\)\n\n\n\n\nMultivariate Normal \\((\\mu, \\Sigma)\\)\n\n\n\\(\\mu\\)\n\n\nMatrix \\(\\Sigma_{ij} = \\text{Cov}(X_i, X_j)\\)"
  },
  {
    "objectID": "notes/EntropyNotes.html",
    "href": "notes/EntropyNotes.html",
    "title": "Entropy Notes",
    "section": "",
    "text": "From David McKay’s Information Theory, Inference, and Learning Algorithms, we acknowledge the following notation:\n\nensemble or random variable X: consists of the triple \\((x, A_x, P_x)\\)\n\n\\(x\\) is the outcome of the random variable \\(X\\)\n\\(A_x\\) is the alphabet or set of possible values: \\(\\{a_1, a_2, ..., a_i, ..., a_I\\}\\)\n\nthis is sometimes noted as \\(\\{x_1, x_2, ..., x_i, ..., x_I\\}\\)\n\n\\(P_x\\) are the associated probabilities, \\(P(x=a_i) = p_i\\) of these outcomes: \\(\\{p_1, p_2, ..., p_i, ..., p_I\\}\\)\n\n\n\n\n\n\\[h(x_i) = \\log\\Big(\\frac{1}{P(x_i)}\\Big)\\tag{1}\\]\nwhere \\(P(x_i)\\) is probability of outcome \\(x_i\\).\n\n\n\n\\[H(X) = E\\big[h(x_i)\\big] = \\sum_{x \\in A_X} P(x)\\log\\Big(\\frac{1}{P(x)}\\Big)\\tag{2}\\]\nwhere \\(A_X\\) is the alphabet or set of outcomes for random variable \\(X\\). It’s also known as the marginal entropy.\n\n\n\n\\[H(X,Y) = \\sum_{x, y \\in A_XA_Y} P(x,y)\\log\\Big(\\frac{1}{P(x, y)}\\Big)\\tag{3}\\]\n\n\n\\[\\begin{align}\nH(X,Y) &= \\sum_{x, y \\in A_XA_Y} P(x,y)\\log\\Big(\\frac{1}{P(x, y)}\\Big)\\\\\n       &= \\sum_{x \\in A_X}\\sum_{y \\in A_Y} P(x)P(y)\\log\\Big(\\frac{1}{P(x)P(y)}\\Big)\\\\\n       &= \\sum_{x \\in A_X}\\sum_{y \\in A_Y} P(x)P(y)\\log\\Big(\\frac{1}{P(x)}\\Big) + \\sum_{x \\in A_X}\\sum_{y \\in A_Y} P(x)P(y)\\log\\Big(\\frac{1}{P(y)}\\Big)\\\\\n       &= \\sum_{x \\in A_X} P(x)\\log\\Big(\\frac{1}{P(x)}\\Big) + \\sum_{y \\in A_Y} P(y)\\log\\Big(\\frac{1}{P(y)}\\Big)\\\\\n       &= H(X) + H(Y)\n\\end{align}\\]\n\n\n\n\n\n\n\\[H(X|y=b_k) = \\sum_i P(x_i|y=b_k)\\log\\Big(\\frac{1}{P(x_i|y=b_k)}\\Big)\\tag{4}\\]\nwhich is simply the entropy of distribution \\(P(X|y=b_k)\\).\n\n\n\nis the average of (4) over \\(y\\):\n\\[\\begin{align}\nH(X|Y) &= \\sum_{y \\in A_y}P(y)\\Bigg[\\sum_{x \\in A_x} P(x|y)\\log\\Big(\\frac{1}{P(x|y)}\\Big)\\Bigg]\\\\ \\tag{5}\n&= \\sum_{x,y \\in A_XA_Y} P(x, y)\\log\\Big(\\frac{1}{P(x|y)}\\Big)\n\\end{align}\\]\nwhich measures the uncertainty that remains about \\(x\\) when \\(y\\) is known.\n\n\n\n\n\\[\\begin{align}H(X,Y) &= \\sum_{x, y \\in A_XA_Y} P(x,y)\\log\\Big(\\frac{1}{P(x, y)}\\Big) \\\\\n&=\\sum_{x, y \\in A_XA_Y} P(x, y)\\log\\Big(\\frac{1}{P(x|y)P(y)}\\Big) \\\\\n&= \\sum_{x, y \\in A_XA_Y} P(x, y)\\log\\Big(\\frac{1}{P(x|y)}\\Big) +\\sum_{x, y \\in A_XA_Y} P(x, y)\\log\\Big(\\frac{1}{P(y)}\\Big)\\\\\n&= H(X|Y) + H(Y)\\\\\n\\end{align}\\]\nSince entropy is symmetric, just like joint probability, the full relations are:\n\\[ H(X,Y) = H(Y,X) = H(X|Y) + H(Y) = H(Y|X) + H(X)\\tag{6}\\]\n\n\n\nWe can define mutual information as the reduction of uncertainty of one variable when another is known.\n\\[\\begin{align}\nI(X;Y) &\\equiv H(X)- H(X|Y)\\tag{7}\\\\\n&= H(X) +H(Y) - H(X,Y)\\tag{from 6}\\\\\n&= I(Y;X)\\\\\n&=H(Y)- H(Y|X)\\tag{from symmetry}\\\\\n\\end{align}\\]\nWhen put in terms of probabilities, we have the following equivalent definition:\n\\[\\begin{align}I(X;Y) &\\equiv \\sum_{x, y \\in A_XA_Y} P(x,y)\\log\\Big(\\frac{P(x, y)}{P(x)P(y)}\\Big)\\tag{8a} \\\\\n&=\\sum_{x \\in A_X}\\sum_{y \\in A_Y} P(x,y)\\log\\Big(\\frac{P(x, y)}{P(x)P(y)}\\Big) \\\\\n&=\\sum_{x \\in A_X}\\sum_{y \\in A_Y} P(x,y)\\Big[\\log\\Big(\\frac{1}{P(x)}\\Big) + \\log\\Big(\\frac{1}{P(y)}\\Big) + \\log \\Big(P(x, y)\\Big)\\Big] \\\\\n&= H(X) +H(Y) - H(X,Y)\n\\end{align}\\]\nJust as entropy was the expected value of Shannon information, mutual information is the expected value of pairwise mutual information between points \\(x_i\\) and \\(y_j\\):\n\\[M(x_i, y_j) = \\log\\Big(\\frac{P(x_i, y_j)}{P(x_i)P(y_j)}\\Big) \\tag{8b}\\]\nWe can also combine (6) and (7) to rewrite joint entropy in terms of conditional entropies and mutual information:\n\\[\\begin{align}\nH(X, Y) &= H(X|Y) + H(Y) \\\\\n&= H(X|Y) + H(Y|X) + I(X;Y)\\tag{9}\\\\\n\\end{align}\\]\n\n\n\nThe cross entropy of distributions \\(Q(x)\\) relative to distribution \\(P(x)\\) is similar to (2), but using different distributions:\n\\[H(p,q) = \\sum_x P(x)\\log\\Big(\\frac{1}{Q(x)}\\Big)\\tag{10}\\]\nThe relative entropy or Kullback-Leibler divergence is a measure of the distance two distributions are from each other, defined as:\n\\[ D_{KL}(P||Q) = \\sum_x P(x) \\log\\Big(\\frac{P(x)}{Q(x)}\\Big) \\tag{11}\\]\nBy separating the logarithm in (8) and using (2), we can see the relation between (8) and (9) as:\n\\[\\begin{align}\nD_{KL}(P||Q) &= \\sum_x P(x) \\log\\Big(\\frac{P(x)}{Q(x)}\\Big)\\\\\n&= \\sum_x P(x) \\log\\Big(\\frac{1}{Q(x)}\\Big) + \\sum_x P(x) \\log(P(x))\\\\\n&= H(p,q) - H(X)\\\\\n\\end{align}\n\\]\nOr better yet:\n\\[H(p,q) = H(X) + D_{KL}(P||Q)\\tag{12}\\]\nThis is why the cross entropy is used as a loss function in machine learning applications, comparing the distance between ground truth distribution \\(y\\) and estimate \\(\\hat{y}\\). It inherently has the KL divergence within it.\nMutual information can be seen as the KL divergence between probablity distributions \\(P(x, y)\\) and \\(P(x)P(y)\\):\n\\[\\begin{align}I(X;Y) &\\equiv \\sum_{x, y \\in A_XA_Y} P(x,y)\\log\\Big(\\frac{P(x, y)}{P(x)P(y)}\\Big) \\equiv D_{KL} \\Big(P(x,y)||P(x)P(y)\\Big)\\\\ \\tag{13a}\n\\end{align}\\]\nWe can define a quantity called the Kullback-Leibler information which which expresses the reduction of uncertainty of \\(Y\\) after event \\(x_i\\) has been observed. It can be written as the KL Divergence between \\(P(Y|x_i)\\) and \\(P(Y)\\):\n\\[ D_{KL} \\Big(P(Y|x_i)||P(Y)\\Big) = \\sum_{ y_j \\in A_Y} P(y_j|x_i)\\log\\Big(\\frac{P(y_j|x_i)}{P(y_j)}\\Big) \\tag{13b}\\]\nSimilarly, we have the symmetric case of between \\(X\\) and event \\(y_j\\):\n\\[ D_{KL} \\Big(P(X|y_j)||P(X)\\Big) = \\sum_{ x_i \\in A_X} P(x_i|y_j)\\log\\Big(\\frac{P(x_i|y_j)}{P(x_i)}\\Big) \\tag{13c}\\]\nNoticing that joint and conditional probabilities are related by averaging and the log arguments in (13b, 13c) are both equal and equivalent to \\(\\frac{P(x_i,y_j)}{P(x_i)P(y_j)}\\) we see can that mutual information and KL-information are related by:\n\\[I(X;Y) = \\sum_{ x_i \\in A_X} P(x_i)D_{KL} \\Big(P(Y|x_i)||P(Y)\\Big) = \\sum_{ y_j \\in A_Y}P(y_j)D_{KL} \\Big(P(X|y_j)||P(X)\\Big)\\tag{13d}\\]\n\n\n\nKL Divergence is not a true distance in the sense that it lacks symmetry: \\(D_{KL}(P||Q) \\ne D_{KL}(Q||P)\\).\nIf we define a distance by defining an average distributions \\(M = \\frac{1}{2}(P + Q)\\), we can define a symmetric divergence as\n\\[ JSD = \\frac{1}{2} \\Big(D_{KL}(P||M) + D_{KL}(Q||M) \\Big)\\tag{14}\\]\n\n\n\nOf all functional forms \\(f(x)\\), what merits are there to having \\(h(x=a_i) =\\log_2\\frac{1}{p_i}\\)? These are similar arguments for the functional form of entropy in physics.\n\n\n\\[H(X,Y) = H(X) + H(Y) \\tag{15}\\]\n\n\nThis stems from the additive properties of logarithms and marginlizing over joint distributions.\n\\[\\begin{align}\nH(X,Y) &= \\sum_{x, y \\in A_XA_Y}P(x,y)\\log\\Big(\\frac{1}{P(x, y)}\\Big)\\\\\n       &= \\sum_{x, y \\in A_XA_Y}P(x)P(y)\\log\\Big(\\frac{1}{P(x)P(y)}\\Big)\\\\\n       &= \\sum_{x, y \\in A_XA_Y}P(x)P(y)\\log\\Big(\\frac{1}{P(x)}\\Big) + \\sum_{x, y \\in A_XA_Y}P(x)P(y)\\log\\Big(\\frac{1}{P(x)}\\Big)\\\\\n       &= \\sum_{x \\in A_X}P(x)\\log\\Big(\\frac{1}{P(x)}\\Big) + \\sum_{y \\in A_Y}P(y)\\log\\Big(\\frac{1}{P(y)}\\Big)\\\\\n       &= H(X) + H(Y)\\\\\n\\end{align}\\]\n\n\n\n\n\n\nFor convex functions:\n\\[E[f(x)] \\ge f(E[x])\\tag{16}\\]\nFor concave functions:\n\\[E[f(x)] \\le f(E[x])\\tag{17}\\]\n\n\n\n\\[E(1/P(x)) = \\sum_{x \\in A_x} P(x)\\frac{1}{P(x)} = |A_x|\\tag{18}\\] where \\(|A_x|\\) is the size of the set, ie. its cardinality.\n\n\n\nLetting \\(x = \\frac{1}{P(x)}\\) and \\(f = \\log\\), which is a concave, by Jensen’s equality we have:\n\\[E\\Big[\\log\\Big(\\frac{1}{P(x)}\\Big)\\Big] \\le \\log\\Big(E\\big[\\frac{1}{P(x)}\\big]\\Big)\\tag{19}\\]\nThe left hand side is the entropy and since we’ve already found the expectation of inverse probability we have:\n\\[H(X) \\le \\log A_x\\tag{20}\\]\nIn a uniform distribution, \\(P(x) = \\frac{1}{A_x}\\) so we’d meet equality if that were the distribution of \\(X\\):\n\\[\\begin{align}\nH(X) &=  \\sum_{x \\in A_X} P(x)\\log\\Big(\\frac{1}{P(x)}\\Big)\\\\\n     &= \\sum_{x \\in A_X} \\frac{1}{A_x}\\log(A_x)\\\\\n     &= \\log(A_x)\\tag{21}\\\\\n\\end{align}\\]\nIn contrast, for a discrete delta distribution, with only one possible outcome \\(a, P(a)=1\\) we have:\n\\[\\begin{align}\nH(X) &=  \\sum_{x \\in A_X} P(x)\\log\\Big(\\frac{1}{P(x)}\\Big)\\\\\n     &= 1*\\log(1)\\\\\n     &= 0\\tag{22}\\\\\n\\end{align}\\]\nThis shows how entropy can be used to describe how concentrated or spread out a distribution can be with values ranging from \\(0\\) to \\(\\log(A_x)\\). This can be interpreted as the uncertainty of value of the random variable.\n\n\n\n\n\nWikipedia’s article has a nice exposition:\nIn a classification problem we denote the estimated probability of outcome \\(i\\) as \\(q_i\\) and the empirical probability from the training set of the same event as \\(p_i\\), where \\(i \\in \\{1,..., N\\}\\), where the samples are conditionally independent,\nthe likelihood is:\n\\[ \\mathcal{L} _N(\\theta) = \\prod_i^N\\text{probability of }i^{\\text{number of occurences of }i} = \\prod_i q_i^{Np_i}\\tag{23}\\]\nThe log-likelihood is then:\n\\[l(\\theta) = \\log \\mathcal{L}(\\theta) = N\\sum^N_{i=1} p_i\\log q_i\\tag{24}\\]\nDividing the log-likelihood \\(l\\) by \\(N\\) gives us:\n\\[ \\frac{1}{N}l(\\theta) = \\sum^N_{i=1} p_i\\log q_i = -H(p, q)\\tag{25}\\]\nMaximizing the log-likelihood is then equivalent to minizing the cross entropy, and thus the difference between \\(q_i\\) and \\(p_i\\).\n\n\n\n\nFor the task of comparing different clusterings, \\(C, C'\\) of the same data \\(D\\), some useful metrics build upon the concept of entropy. For a clustering \\(C\\), the probability of a data point being in a cluster \\(C_k\\) is given by:\n\\[P(k) = \\frac{n_k}{n}\\tag{26}\\]\nwhere \\(n = |D|\\) and \\(n_k = |C_k|\\).\nThe joint probability in this context refers to the probability a data point belongs to cluster \\(C_k\\) in clustering \\(C\\) and cluster \\(C'_{k'}\\) in clustering \\(C'\\):\n\\[P(k,k') = \\frac{|C_k \\cap C'_{k'}|}{n}\\]\nThis leads to defining the conditional probability between a data point belonging to cluster \\(C_k\\) given it’s in \\(C'_{k'}\\) as:\n\\[P(k | k') = \\frac{P(k, k')}{P(k')} = \\frac{|C_k \\cap C'_{k'}|}{n_{k'}}\\tag{27}\\]\nThe associated entropy with the clustering is then:\n\\[ H(C) = -\\sum^K_{k=1}P(k)\\log\\Big(P(k)\\Big)\\tag{28}\\]\nMutual information between clusterings is then:\n\\[I(C; C') = -\\sum^K_{k=1}\\sum^{K'}_{k'=1}P(k, k')\\log\\Big(\\frac{P(k,k')}{P(k)P(k')}\\Big)\\tag{29}\\]\nTreating clusterings \\(C, C'\\) as probability distributions of data points, their associated conditional entropies are given by:\n\\[H(C|C') = -\\sum^K_{k=1}\\sum^{K'}_{k'=1}P(k, k')\\log\\Big(\\frac{P(k, k')}{P(k')}\\Big) \\tag{30}\\]\n\n\n\nThe variation of information (VI) builds on these clustering entropies:\n\\[ VI(C,C')\\equiv H(C) + H(C') - 2I(C;C') \\tag{31}\\]\nGrouping mutual information difference to each marginal entropy, we see VI is a sum of conditional entropies:\n\\[\\begin{align}\nVI(C,C')&\\equiv H(C) - I(C;C') + H(C') - I(C;C') \\tag{32}\\\\\n&= H(C|C') + H(C'|C)\\\\\n\\end{align}\\]\nThis can also be rewritten in terms of joint entropy using (9) to substitute out the conditional entropy terms in (32):\n\\[VI(C,C') = H(C, C') - I(C; C')\\tag{33}\\]\nA computationally useful form is (32) in terms of explicit probabilities:\n\\[\\begin{align}\nVI(C, C') &= H(C|C') + H(C'|C)\\\\\n&= -\\sum^K_{k=1}\\sum^{K'}_{k'=1}P(k, k')\\log\\Big(\\frac{P(k, k')}{P(k')}\\Big) + P(k, k')\\log\\Big(\\frac{P(k, k')}{P(k)}\\Big) \\\\\n&= -\\sum^K_{k=1}\\sum^{K'}_{k'=1}P(k, k')\\Bigg[\\log\\Big(\\frac{P(k, k')}{P(k')}\\Big) + \\log\\Big(\\frac{P(k, k')}{P(k)}\\Big)\\Bigg]\\\\\n\\end{align}\\tag{34}\\]\n\n\n\nTo make better sense of the VI distance, it helps to consider the total set of clusterings of a dataset \\(D\\). The most extreme members, at least in regards to entropy values are: - \\(\\hat{1}\\):\n- \\(K = 1\\) clusters - \\(H(\\hat{1}) = 0\\)\n- \\(\\hat{0}\\): - \\(K = n\\) clusters - with \\(H(\\hat{0}) = \\log n\\) - \\(C^U_K\\): - \\(K\\) equal sized clusters, \\(K \\ge 1\\) - with \\(H(C^U_K) = \\log K\\)\nJoint probability between any clustering \\(C\\) with index \\(k\\) and \\(\\hat{1}\\) index \\(k' = 1\\) is given depends on terms \\(P(k, k') = \\frac{|C_k \\cap C'_{k'}|}{n} = \\frac{|C_k|}{n} = P(k) * 1\\).\nThis implies: - the clusterings \\(\\hat{1}\\) and \\(C\\) are independent of each other - \\(I(C; \\hat{1}) = 0\\) - \\(H(C, \\hat{1})  = H(C) + H(\\hat{1}) = H(C)\\) - \\(VI(C, \\hat{1}) = H(C)\\)\n\n\\(VI(C,C') \\le \\log n\\), where equality is in the case of clusters \\(\\hat{1}\\) (only one cluster) and \\(\\hat{0}\\) (\\(n\\) clusters).\nIf \\(C\\) and \\(C'\\) have at most \\(K\\) clusters each, with \\(K \\le \\sqrt{n}\\), then \\(VI(C, C') \\le 2 \\log K\\)\n\n\n\n\n\n\nfrom scipy.stats import entropy\nfrom scipy.special import rel_entr, kl_div\nfrom scipy.spatial.distance import jensenshannon\nimport pandas as pd \nimport numpy as np\nfrom igraph.clustering import compare_communities, Clustering \nfrom typing import List\nfrom collections import defaultdict\nfrom itertools import chain\n\n\n\n\ndef jsd(a: np.array, b: np.array, base: int = None):\n    \"\"\"Calculate and return the Jensen-Shannon Divergence\"\"\"\n    m = (a + b) / 2\n    return (entropy(a, m, base=base) + entropy(b, m, base=base)) / 2\n\n\nseq = [2, 1, 2, 2, 2, 2, 2]\n\ns = pd.Series(seq).value_counts()\nvals = s.index\nprobs = s.values/len(seq)\n\n\na = np.array([9, 12, 4])\nb = np.array([1, 1, 1])\np = np.array([0.36, 0.48, 0.16])\nq = np.array([0.30, 0.50, 0.20])\nm = (a + b)/2\n\n\nnp.sqrt(jsd(p, q))\n\n0.050803321756356906\n\n\n\nentropy(a, b)\n\n0.0852996013183706\n\n\n\nkl_div(a, b)\n\narray([11.7750212 , 18.8188798 ,  2.54517744])\n\n\n\nrel_entr(a, b)\n\narray([19.7750212 , 29.8188798 ,  5.54517744])\n\n\n\njsd(a, b, base=None)\n\n0.03793843282690725\n\n\n\nnp.sqrt(jsd(a, b, base=None))\n\n0.19477790641370815\n\n\n\njensenshannon(p, q, base=None)\n\n0.050803321756356906\n\n\n\n\n\n\nVariation of information (VI) can be computed by using (34) and looping through the \\(n_k \\times n_{k'}\\) cluster intersections, setting entropies of disjoint clusters to zero.\nThe igraph library function compare_communities has an option to compute VI, with the method='vi' option.\n\nigraph accepts clusterings as membership lists, where the list’s indices correspond to nodes, and the values correspond to cluster indices.\nMembership lists constrain cluster indices to range from \\(\\{0, 1, ..., n-1\\}\\) where \\(n\\) is the number of nodes or data points\nMembership lists can be converted to a cluster list, a list of lists where each inner list corresponds to a cluster of data points, represented by unique integer indices\nConverting a cluster list to a membership list requires the following:\n\neach data point must be deterministically assigned to a list index, such as sorting\neach cluster must be given a cluster id in the range \\(\\{0, 1, ..., n-1\\}\\)\neach list index must be assigned the cluster id to which it is a member of\nthe VI calculation should be invariant to cluster id assignment within the same clusterings, (swapping what cluster is labeled 0 vs 1 should have no effect)\n\n\n\nMeilă, Marina. “Comparing Clusterings—an Information Based Distance.” Journal of Multivariate Analysis 98, no. 5 (May 1, 2007): 873–95. https://doi.org/10.1016/j.jmva.2006.11.013.\n\ndef var_info(C1: List[List[int]], C2: List[List[int]], base: float = None):\n    \"\"\"Variation of information between two clusterings\"\"\"\n    n = sum(len(i) for i in C1)\n    assert n == sum(len(j) for j in C2)\n    total = 0.0\n    for i in C1:\n        p_i = len(i) / n\n        for j in C2:\n            p_j = len(j) / n\n            p_ij = len(set(i) & set(j)) / n\n            if p_ij &gt; 0.0:\n                total -= p_ij * (np.log(p_ij / p_i) + np.log(p_ij / p_j))\n    if base is not None:\n        total /= np.log(base)\n    return total\n\ndef create_membership_lists(clusters: List[List[int]]):\n    \"\"\"Return a list of cluster indices each element of a set belongs to\"\"\"\n    data = sorted(list(chain(*clusters)))\n    mem_list = [-1 for i in data]\n    for cluster_id, cluster in enumerate(clusters):\n        for data_point in cluster:\n            mem_list_idx = data.index(data_point)\n            mem_list[mem_list_idx] = cluster_id\n    return mem_list\n\ndef create_cluster_lists(mem_list: List[int]):\n    \"\"\"Convert membership list to list of lists corresponding to clusters\"\"\"\n    d = defaultdict(list)\n    for idx, cluster in enumerate(mem_list):\n        d[cluster].append(idx)\n    return list(d.values())        \n\ndef map_indices(data: List) -&gt; List[int]:\n    \"\"\"Dictionary of indices of ordered data\"\"\"\n    assert len(data) == len(set(data))\n    return {val: idx for idx, val in enumerate(sorted(data))}\n\ndef standardize_cluster_list(clusters: List[List[int]]) -&gt; List[List[int]]:\n    \"\"\"Replace a set of values with ordered indices in cluster membership lists\"\"\"\n    data = list(chain(*clusters))\n    data_idx = map_indices(data)\n    standardized = []\n    for cluster in clusters:\n        standardized_cluster = [data_idx[val] for val in cluster]\n        standardized.append(standardized_cluster)\n    return standardized\n        \n        \n    \n\n\n# Cluster lists\nX1 = [ [1,2,3,4,5], [6,7,8,9,10] ]\nY1 = [ [1,2,3,4,5], [6,7,8,9,10] ]\n\nX2 = [ [1,2,3,4], [5,6,7,8,9,10] ]\nY2 = [ [1,2,3,4,5,6], [7,8,9,10] ]\n\nX3 = [ [1,2], [3,4,5], [6,7,8], [9,10]]\nY3 = [ [10,2,3], [4,5,6,7], [8,9,1] ]\n\nX4 = [ [1,2,3,4,5,6,7,8,9,10] ]\nY4 = [ [1], [2], [3], [4], [5], [6], [7], [8], [9], [10] ]\n\nX5 = [ [10,2,3], [4,5,6,7], [8,9,1] ]\nY5 = [[4,5,6,7], [8,9,1], [10,2,3]]\n\n\nX_ = [X1, X2, X3, X4]\nY_ = [Y1, Y2, Y3, Y4]\n\n\na = create_membership_lists(X5)\nb = create_membership_lists(Y5)\nprint(a)\nprint(b)\n\ncompare_communities(a, b, method='vi')\n\n[2, 0, 0, 1, 1, 1, 1, 2, 2, 0]\n[1, 2, 2, 0, 0, 0, 0, 1, 1, 2]\n\n\n0.0\n\n\n\nvar_info(X5, Y5)\n\n0.0\n\n\n\ncompare_communities([1, 2, 0], [2, 2, 0], method='vi')\n\n0.4620981203732968\n\n\n\nd = { 'X': X_,\n      'Y': Y_,\n      'VI custom (nats)': [var_info(X, Y, 2) for X, Y in zip(X_, Y_)],\n      'VI-igraph (nats)': [compare_communities(create_membership_lists(X), create_membership_lists(Y), method='vi')\n                           for X, Y in zip(X_, Y_)]\n     }\n\ndf = pd.DataFrame(d)\ndf\n\n\n\n\n\n\n\n\n\nX\nY\nVI custom (nats)\nVI-igraph (nats)\n\n\n\n\n0\n[[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]\n[[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]\n0.000000\n0.000000\n\n\n1\n[[1, 2, 3, 4], [5, 6, 7, 8, 9, 10]]\n[[1, 2, 3, 4, 5, 6], [7, 8, 9, 10]]\n1.101955\n1.101955\n\n\n2\n[[1, 2], [3, 4, 5], [6, 7, 8], [9, 10]]\n[[10, 2, 3], [4, 5, 6, 7], [8, 9, 1]]\n2.301955\n2.301955\n\n\n3\n[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]]\n[[1], [2], [3], [4], [5], [6], [7], [8], [9], ...\n3.321928\n3.321928\n\n\n\n\n\n\n\n\n\nY3 = [ [10,2,3], [4,5,6,7], [8,9,1] ]\nY31 = [[9, 1, 2], [3,4,5,6], [7, 8, 0]]\nY3_ = [2, 0, 0, 1, 1, 1, 1, 2, 2, 0]\ncreated = create_cluster_lists(Y3_)\n\n\ncreated == Y31\n\nFalse\n\n\n\ncreated\n\n[[0, 7, 8], [1, 2, 9], [3, 4, 5, 6]]\n\n\n\nvar_info(created, Y31)\n\n0.0"
  },
  {
    "objectID": "notes/EntropyNotes.html#why-shannon-entropy",
    "href": "notes/EntropyNotes.html#why-shannon-entropy",
    "title": "Entropy Notes",
    "section": "",
    "text": "Of all functional forms \\(f(x)\\), what merits are there to having \\(h(x=a_i) =\\log_2\\frac{1}{p_i}\\)? These are similar arguments for the functional form of entropy in physics.\n\n\n\\[H(X,Y) = H(X) + H(Y) \\tag{15}\\]\n\n\nThis stems from the additive properties of logarithms and marginlizing over joint distributions.\n\\[\\begin{align}\nH(X,Y) &= \\sum_{x, y \\in A_XA_Y}P(x,y)\\log\\Big(\\frac{1}{P(x, y)}\\Big)\\\\\n       &= \\sum_{x, y \\in A_XA_Y}P(x)P(y)\\log\\Big(\\frac{1}{P(x)P(y)}\\Big)\\\\\n       &= \\sum_{x, y \\in A_XA_Y}P(x)P(y)\\log\\Big(\\frac{1}{P(x)}\\Big) + \\sum_{x, y \\in A_XA_Y}P(x)P(y)\\log\\Big(\\frac{1}{P(x)}\\Big)\\\\\n       &= \\sum_{x \\in A_X}P(x)\\log\\Big(\\frac{1}{P(x)}\\Big) + \\sum_{y \\in A_Y}P(y)\\log\\Big(\\frac{1}{P(y)}\\Big)\\\\\n       &= H(X) + H(Y)\\\\\n\\end{align}\\]\n\n\n\n\n\n\nFor convex functions:\n\\[E[f(x)] \\ge f(E[x])\\tag{16}\\]\nFor concave functions:\n\\[E[f(x)] \\le f(E[x])\\tag{17}\\]\n\n\n\n\\[E(1/P(x)) = \\sum_{x \\in A_x} P(x)\\frac{1}{P(x)} = |A_x|\\tag{18}\\] where \\(|A_x|\\) is the size of the set, ie. its cardinality.\n\n\n\nLetting \\(x = \\frac{1}{P(x)}\\) and \\(f = \\log\\), which is a concave, by Jensen’s equality we have:\n\\[E\\Big[\\log\\Big(\\frac{1}{P(x)}\\Big)\\Big] \\le \\log\\Big(E\\big[\\frac{1}{P(x)}\\big]\\Big)\\tag{19}\\]\nThe left hand side is the entropy and since we’ve already found the expectation of inverse probability we have:\n\\[H(X) \\le \\log A_x\\tag{20}\\]\nIn a uniform distribution, \\(P(x) = \\frac{1}{A_x}\\) so we’d meet equality if that were the distribution of \\(X\\):\n\\[\\begin{align}\nH(X) &=  \\sum_{x \\in A_X} P(x)\\log\\Big(\\frac{1}{P(x)}\\Big)\\\\\n     &= \\sum_{x \\in A_X} \\frac{1}{A_x}\\log(A_x)\\\\\n     &= \\log(A_x)\\tag{21}\\\\\n\\end{align}\\]\nIn contrast, for a discrete delta distribution, with only one possible outcome \\(a, P(a)=1\\) we have:\n\\[\\begin{align}\nH(X) &=  \\sum_{x \\in A_X} P(x)\\log\\Big(\\frac{1}{P(x)}\\Big)\\\\\n     &= 1*\\log(1)\\\\\n     &= 0\\tag{22}\\\\\n\\end{align}\\]\nThis shows how entropy can be used to describe how concentrated or spread out a distribution can be with values ranging from \\(0\\) to \\(\\log(A_x)\\). This can be interpreted as the uncertainty of value of the random variable."
  },
  {
    "objectID": "notes/EntropyNotes.html#relation-to-log-likelihood",
    "href": "notes/EntropyNotes.html#relation-to-log-likelihood",
    "title": "Entropy Notes",
    "section": "",
    "text": "Wikipedia’s article has a nice exposition:\nIn a classification problem we denote the estimated probability of outcome \\(i\\) as \\(q_i\\) and the empirical probability from the training set of the same event as \\(p_i\\), where \\(i \\in \\{1,..., N\\}\\), where the samples are conditionally independent,\nthe likelihood is:\n\\[ \\mathcal{L} _N(\\theta) = \\prod_i^N\\text{probability of }i^{\\text{number of occurences of }i} = \\prod_i q_i^{Np_i}\\tag{23}\\]\nThe log-likelihood is then:\n\\[l(\\theta) = \\log \\mathcal{L}(\\theta) = N\\sum^N_{i=1} p_i\\log q_i\\tag{24}\\]\nDividing the log-likelihood \\(l\\) by \\(N\\) gives us:\n\\[ \\frac{1}{N}l(\\theta) = \\sum^N_{i=1} p_i\\log q_i = -H(p, q)\\tag{25}\\]\nMaximizing the log-likelihood is then equivalent to minizing the cross entropy, and thus the difference between \\(q_i\\) and \\(p_i\\).\n\n\n\n\nFor the task of comparing different clusterings, \\(C, C'\\) of the same data \\(D\\), some useful metrics build upon the concept of entropy. For a clustering \\(C\\), the probability of a data point being in a cluster \\(C_k\\) is given by:\n\\[P(k) = \\frac{n_k}{n}\\tag{26}\\]\nwhere \\(n = |D|\\) and \\(n_k = |C_k|\\).\nThe joint probability in this context refers to the probability a data point belongs to cluster \\(C_k\\) in clustering \\(C\\) and cluster \\(C'_{k'}\\) in clustering \\(C'\\):\n\\[P(k,k') = \\frac{|C_k \\cap C'_{k'}|}{n}\\]\nThis leads to defining the conditional probability between a data point belonging to cluster \\(C_k\\) given it’s in \\(C'_{k'}\\) as:\n\\[P(k | k') = \\frac{P(k, k')}{P(k')} = \\frac{|C_k \\cap C'_{k'}|}{n_{k'}}\\tag{27}\\]\nThe associated entropy with the clustering is then:\n\\[ H(C) = -\\sum^K_{k=1}P(k)\\log\\Big(P(k)\\Big)\\tag{28}\\]\nMutual information between clusterings is then:\n\\[I(C; C') = -\\sum^K_{k=1}\\sum^{K'}_{k'=1}P(k, k')\\log\\Big(\\frac{P(k,k')}{P(k)P(k')}\\Big)\\tag{29}\\]\nTreating clusterings \\(C, C'\\) as probability distributions of data points, their associated conditional entropies are given by:\n\\[H(C|C') = -\\sum^K_{k=1}\\sum^{K'}_{k'=1}P(k, k')\\log\\Big(\\frac{P(k, k')}{P(k')}\\Big) \\tag{30}\\]\n\n\n\nThe variation of information (VI) builds on these clustering entropies:\n\\[ VI(C,C')\\equiv H(C) + H(C') - 2I(C;C') \\tag{31}\\]\nGrouping mutual information difference to each marginal entropy, we see VI is a sum of conditional entropies:\n\\[\\begin{align}\nVI(C,C')&\\equiv H(C) - I(C;C') + H(C') - I(C;C') \\tag{32}\\\\\n&= H(C|C') + H(C'|C)\\\\\n\\end{align}\\]\nThis can also be rewritten in terms of joint entropy using (9) to substitute out the conditional entropy terms in (32):\n\\[VI(C,C') = H(C, C') - I(C; C')\\tag{33}\\]\nA computationally useful form is (32) in terms of explicit probabilities:\n\\[\\begin{align}\nVI(C, C') &= H(C|C') + H(C'|C)\\\\\n&= -\\sum^K_{k=1}\\sum^{K'}_{k'=1}P(k, k')\\log\\Big(\\frac{P(k, k')}{P(k')}\\Big) + P(k, k')\\log\\Big(\\frac{P(k, k')}{P(k)}\\Big) \\\\\n&= -\\sum^K_{k=1}\\sum^{K'}_{k'=1}P(k, k')\\Bigg[\\log\\Big(\\frac{P(k, k')}{P(k')}\\Big) + \\log\\Big(\\frac{P(k, k')}{P(k)}\\Big)\\Bigg]\\\\\n\\end{align}\\tag{34}\\]\n\n\n\nTo make better sense of the VI distance, it helps to consider the total set of clusterings of a dataset \\(D\\). The most extreme members, at least in regards to entropy values are: - \\(\\hat{1}\\):\n- \\(K = 1\\) clusters - \\(H(\\hat{1}) = 0\\)\n- \\(\\hat{0}\\): - \\(K = n\\) clusters - with \\(H(\\hat{0}) = \\log n\\) - \\(C^U_K\\): - \\(K\\) equal sized clusters, \\(K \\ge 1\\) - with \\(H(C^U_K) = \\log K\\)\nJoint probability between any clustering \\(C\\) with index \\(k\\) and \\(\\hat{1}\\) index \\(k' = 1\\) is given depends on terms \\(P(k, k') = \\frac{|C_k \\cap C'_{k'}|}{n} = \\frac{|C_k|}{n} = P(k) * 1\\).\nThis implies: - the clusterings \\(\\hat{1}\\) and \\(C\\) are independent of each other - \\(I(C; \\hat{1}) = 0\\) - \\(H(C, \\hat{1})  = H(C) + H(\\hat{1}) = H(C)\\) - \\(VI(C, \\hat{1}) = H(C)\\)\n\n\\(VI(C,C') \\le \\log n\\), where equality is in the case of clusters \\(\\hat{1}\\) (only one cluster) and \\(\\hat{0}\\) (\\(n\\) clusters).\nIf \\(C\\) and \\(C'\\) have at most \\(K\\) clusters each, with \\(K \\le \\sqrt{n}\\), then \\(VI(C, C') \\le 2 \\log K\\)\n\n\n\n\n\n\nfrom scipy.stats import entropy\nfrom scipy.special import rel_entr, kl_div\nfrom scipy.spatial.distance import jensenshannon\nimport pandas as pd \nimport numpy as np\nfrom igraph.clustering import compare_communities, Clustering \nfrom typing import List\nfrom collections import defaultdict\nfrom itertools import chain\n\n\n\n\ndef jsd(a: np.array, b: np.array, base: int = None):\n    \"\"\"Calculate and return the Jensen-Shannon Divergence\"\"\"\n    m = (a + b) / 2\n    return (entropy(a, m, base=base) + entropy(b, m, base=base)) / 2\n\n\nseq = [2, 1, 2, 2, 2, 2, 2]\n\ns = pd.Series(seq).value_counts()\nvals = s.index\nprobs = s.values/len(seq)\n\n\na = np.array([9, 12, 4])\nb = np.array([1, 1, 1])\np = np.array([0.36, 0.48, 0.16])\nq = np.array([0.30, 0.50, 0.20])\nm = (a + b)/2\n\n\nnp.sqrt(jsd(p, q))\n\n0.050803321756356906\n\n\n\nentropy(a, b)\n\n0.0852996013183706\n\n\n\nkl_div(a, b)\n\narray([11.7750212 , 18.8188798 ,  2.54517744])\n\n\n\nrel_entr(a, b)\n\narray([19.7750212 , 29.8188798 ,  5.54517744])\n\n\n\njsd(a, b, base=None)\n\n0.03793843282690725\n\n\n\nnp.sqrt(jsd(a, b, base=None))\n\n0.19477790641370815\n\n\n\njensenshannon(p, q, base=None)\n\n0.050803321756356906\n\n\n\n\n\n\nVariation of information (VI) can be computed by using (34) and looping through the \\(n_k \\times n_{k'}\\) cluster intersections, setting entropies of disjoint clusters to zero.\nThe igraph library function compare_communities has an option to compute VI, with the method='vi' option.\n\nigraph accepts clusterings as membership lists, where the list’s indices correspond to nodes, and the values correspond to cluster indices.\nMembership lists constrain cluster indices to range from \\(\\{0, 1, ..., n-1\\}\\) where \\(n\\) is the number of nodes or data points\nMembership lists can be converted to a cluster list, a list of lists where each inner list corresponds to a cluster of data points, represented by unique integer indices\nConverting a cluster list to a membership list requires the following:\n\neach data point must be deterministically assigned to a list index, such as sorting\neach cluster must be given a cluster id in the range \\(\\{0, 1, ..., n-1\\}\\)\neach list index must be assigned the cluster id to which it is a member of\nthe VI calculation should be invariant to cluster id assignment within the same clusterings, (swapping what cluster is labeled 0 vs 1 should have no effect)\n\n\n\nMeilă, Marina. “Comparing Clusterings—an Information Based Distance.” Journal of Multivariate Analysis 98, no. 5 (May 1, 2007): 873–95. https://doi.org/10.1016/j.jmva.2006.11.013.\n\ndef var_info(C1: List[List[int]], C2: List[List[int]], base: float = None):\n    \"\"\"Variation of information between two clusterings\"\"\"\n    n = sum(len(i) for i in C1)\n    assert n == sum(len(j) for j in C2)\n    total = 0.0\n    for i in C1:\n        p_i = len(i) / n\n        for j in C2:\n            p_j = len(j) / n\n            p_ij = len(set(i) & set(j)) / n\n            if p_ij &gt; 0.0:\n                total -= p_ij * (np.log(p_ij / p_i) + np.log(p_ij / p_j))\n    if base is not None:\n        total /= np.log(base)\n    return total\n\ndef create_membership_lists(clusters: List[List[int]]):\n    \"\"\"Return a list of cluster indices each element of a set belongs to\"\"\"\n    data = sorted(list(chain(*clusters)))\n    mem_list = [-1 for i in data]\n    for cluster_id, cluster in enumerate(clusters):\n        for data_point in cluster:\n            mem_list_idx = data.index(data_point)\n            mem_list[mem_list_idx] = cluster_id\n    return mem_list\n\ndef create_cluster_lists(mem_list: List[int]):\n    \"\"\"Convert membership list to list of lists corresponding to clusters\"\"\"\n    d = defaultdict(list)\n    for idx, cluster in enumerate(mem_list):\n        d[cluster].append(idx)\n    return list(d.values())        \n\ndef map_indices(data: List) -&gt; List[int]:\n    \"\"\"Dictionary of indices of ordered data\"\"\"\n    assert len(data) == len(set(data))\n    return {val: idx for idx, val in enumerate(sorted(data))}\n\ndef standardize_cluster_list(clusters: List[List[int]]) -&gt; List[List[int]]:\n    \"\"\"Replace a set of values with ordered indices in cluster membership lists\"\"\"\n    data = list(chain(*clusters))\n    data_idx = map_indices(data)\n    standardized = []\n    for cluster in clusters:\n        standardized_cluster = [data_idx[val] for val in cluster]\n        standardized.append(standardized_cluster)\n    return standardized\n        \n        \n    \n\n\n# Cluster lists\nX1 = [ [1,2,3,4,5], [6,7,8,9,10] ]\nY1 = [ [1,2,3,4,5], [6,7,8,9,10] ]\n\nX2 = [ [1,2,3,4], [5,6,7,8,9,10] ]\nY2 = [ [1,2,3,4,5,6], [7,8,9,10] ]\n\nX3 = [ [1,2], [3,4,5], [6,7,8], [9,10]]\nY3 = [ [10,2,3], [4,5,6,7], [8,9,1] ]\n\nX4 = [ [1,2,3,4,5,6,7,8,9,10] ]\nY4 = [ [1], [2], [3], [4], [5], [6], [7], [8], [9], [10] ]\n\nX5 = [ [10,2,3], [4,5,6,7], [8,9,1] ]\nY5 = [[4,5,6,7], [8,9,1], [10,2,3]]\n\n\nX_ = [X1, X2, X3, X4]\nY_ = [Y1, Y2, Y3, Y4]\n\n\na = create_membership_lists(X5)\nb = create_membership_lists(Y5)\nprint(a)\nprint(b)\n\ncompare_communities(a, b, method='vi')\n\n[2, 0, 0, 1, 1, 1, 1, 2, 2, 0]\n[1, 2, 2, 0, 0, 0, 0, 1, 1, 2]\n\n\n0.0\n\n\n\nvar_info(X5, Y5)\n\n0.0\n\n\n\ncompare_communities([1, 2, 0], [2, 2, 0], method='vi')\n\n0.4620981203732968\n\n\n\nd = { 'X': X_,\n      'Y': Y_,\n      'VI custom (nats)': [var_info(X, Y, 2) for X, Y in zip(X_, Y_)],\n      'VI-igraph (nats)': [compare_communities(create_membership_lists(X), create_membership_lists(Y), method='vi')\n                           for X, Y in zip(X_, Y_)]\n     }\n\ndf = pd.DataFrame(d)\ndf\n\n\n\n\n\n\n\n\n\nX\nY\nVI custom (nats)\nVI-igraph (nats)\n\n\n\n\n0\n[[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]\n[[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]\n0.000000\n0.000000\n\n\n1\n[[1, 2, 3, 4], [5, 6, 7, 8, 9, 10]]\n[[1, 2, 3, 4, 5, 6], [7, 8, 9, 10]]\n1.101955\n1.101955\n\n\n2\n[[1, 2], [3, 4, 5], [6, 7, 8], [9, 10]]\n[[10, 2, 3], [4, 5, 6, 7], [8, 9, 1]]\n2.301955\n2.301955\n\n\n3\n[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]]\n[[1], [2], [3], [4], [5], [6], [7], [8], [9], ...\n3.321928\n3.321928\n\n\n\n\n\n\n\n\n\nY3 = [ [10,2,3], [4,5,6,7], [8,9,1] ]\nY31 = [[9, 1, 2], [3,4,5,6], [7, 8, 0]]\nY3_ = [2, 0, 0, 1, 1, 1, 1, 2, 2, 0]\ncreated = create_cluster_lists(Y3_)\n\n\ncreated == Y31\n\nFalse\n\n\n\ncreated\n\n[[0, 7, 8], [1, 2, 9], [3, 4, 5, 6]]\n\n\n\nvar_info(created, Y31)\n\n0.0"
  },
  {
    "objectID": "notes/MultivariateStatisticsNotes.html",
    "href": "notes/MultivariateStatisticsNotes.html",
    "title": "Multivariate Statistics",
    "section": "",
    "text": "Expectation value and variance have clear definitions for scalar quantities, but what about the analogous definitions for vectors and matrices?\n\n\nFor a given random variable \\(X\\), with probability distribution \\(f(x)\\) we have the following definitions:\nExpectation Value:\n\\[E(X) = \\int^\\infty_{-\\infty} xf(x)dx = \\mu\\]\nVariance:\n\\[V(X) = E\\Big([X-E(X)]^2\\Big) = E(X^2) - \\mu^2\\] Covariance:\n\\[Cov(X, Y) = E\\Big([X-E(X)][Y-E(Y)]\\Big) = E(XY) - \\mu_X\\mu_Y\\]\n\\[Cov(aX, bY) = E\\Big([aX-aE(X)][bY-bE(Y)]\\Big) = abE(XY) - a\\mu_Xb\\mu_Y = abCov(X,Y)\\]\n\n\n\nFor a given random vector \\(X =\n\\begin{pmatrix}\nX_i\\\\\n\\vdots \\\\\nX_n\\\\\n\\end{pmatrix}\\)\nExpectation Value: \\[E(X) = \\begin{pmatrix}\nE(X_i)\\\\\n\\vdots \\\\\nE(X_n)\\\\\n\\end{pmatrix}\n\\]\nVariance: The variance of a random vector \\(X\\) with expectation \\(\\mu\\) is defined to be a matrix \\(\\Sigma\\) whose elements are given by \\[\\Sigma_{ij} = E([X-\\mu][X-\\mu]^T)_{ij} = E([X-\\mu]_i[X-\\mu]_j)\\]\nwhere the subscript \\(T\\) denotes the transpose of a vector. The diagonal elements are variances whereas the off-diagonal elements are covariances:\n\\[Var(X) = \\begin{pmatrix}\nV(X_1) & Cov(X_1, X_2) &\\dots & Cov(X_1,X_n)\\\\\nCov(X_2,X_1) & V(X_2) & \\dots  & Cov(X_2,X_n) \\\\\n\\vdots & \\dots & \\ddots & \\vdots \\\\\nCov(X_n,X_1) & Cov(X_n, X_2) & \\dots & V(X_n)\\\\\n\\end{pmatrix}\n\\]"
  },
  {
    "objectID": "notes/MultivariateStatisticsNotes.html#random-scalars",
    "href": "notes/MultivariateStatisticsNotes.html#random-scalars",
    "title": "Multivariate Statistics",
    "section": "",
    "text": "For a given random variable \\(X\\), with probability distribution \\(f(x)\\) we have the following definitions:\nExpectation Value:\n\\[E(X) = \\int^\\infty_{-\\infty} xf(x)dx = \\mu\\]\nVariance:\n\\[V(X) = E\\Big([X-E(X)]^2\\Big) = E(X^2) - \\mu^2\\] Covariance:\n\\[Cov(X, Y) = E\\Big([X-E(X)][Y-E(Y)]\\Big) = E(XY) - \\mu_X\\mu_Y\\]\n\\[Cov(aX, bY) = E\\Big([aX-aE(X)][bY-bE(Y)]\\Big) = abE(XY) - a\\mu_Xb\\mu_Y = abCov(X,Y)\\]"
  },
  {
    "objectID": "notes/MultivariateStatisticsNotes.html#random-vectors",
    "href": "notes/MultivariateStatisticsNotes.html#random-vectors",
    "title": "Multivariate Statistics",
    "section": "",
    "text": "For a given random vector \\(X =\n\\begin{pmatrix}\nX_i\\\\\n\\vdots \\\\\nX_n\\\\\n\\end{pmatrix}\\)\nExpectation Value: \\[E(X) = \\begin{pmatrix}\nE(X_i)\\\\\n\\vdots \\\\\nE(X_n)\\\\\n\\end{pmatrix}\n\\]\nVariance: The variance of a random vector \\(X\\) with expectation \\(\\mu\\) is defined to be a matrix \\(\\Sigma\\) whose elements are given by \\[\\Sigma_{ij} = E([X-\\mu][X-\\mu]^T)_{ij} = E([X-\\mu]_i[X-\\mu]_j)\\]\nwhere the subscript \\(T\\) denotes the transpose of a vector. The diagonal elements are variances whereas the off-diagonal elements are covariances:\n\\[Var(X) = \\begin{pmatrix}\nV(X_1) & Cov(X_1, X_2) &\\dots & Cov(X_1,X_n)\\\\\nCov(X_2,X_1) & V(X_2) & \\dots  & Cov(X_2,X_n) \\\\\n\\vdots & \\dots & \\ddots & \\vdots \\\\\nCov(X_n,X_1) & Cov(X_n, X_2) & \\dots & V(X_n)\\\\\n\\end{pmatrix}\n\\]"
  },
  {
    "objectID": "notes/MultivariateStatisticsNotes.html#expectation-is-a-linear-operator",
    "href": "notes/MultivariateStatisticsNotes.html#expectation-is-a-linear-operator",
    "title": "Multivariate Statistics",
    "section": "Expectation is a Linear Operator",
    "text": "Expectation is a Linear Operator\nFor a scalar \\(a\\) and random variable \\(X\\):\n\\[\\begin{align} E(aX) &= \\int^\\infty_{-\\infty} axf(x)dx =  a\\int^\\infty_{-\\infty} xf(x)dx \\\\ &= a E(X)\\\\&= a\\mu \\end{align}\\]\nGiven vectors \\(a\\) and \\(X\\) of length \\(n\\), the expectation of their scalar product \\(E(a^T X) = E(X^Ta)\\) is given by:\n\\[E(a^TX) = E\\Big(\\sum^n_{i=1} a_iX_i\\Big) = \\sum^n_{i=1} E(a_iX_i) = \\sum^n_{i=1} a_iE(X_i) = a^TE(X)\\] \\[E(X^Ta) = E\\Big(\\sum^n_{i=1} X_ia_i\\Big) = \\sum^n_{i=1} E(X_ia_i) = \\sum^n_{i=1} E(X_i)a_i = E(X^T)a\\]\nGiven a matrix \\(A\\) acting on vector \\(X\\), the expectation of their product \\(E(AX)\\) is given by:\n\\[E(AX)_{ij} = E\\Big( \\sum_{k} A_{ik}X_{kj}  \\Big) = \\sum_{k} E(A_{ik}X_{kj}) = \\sum_{k} A_{ik}E(X_{kj}) = AE(X)_{ij}\\]\nwhich implies \\[ E(AX) = A\\mu\\]\nSimilarly:\n\\[E(XA)_{ij} = E\\Big( \\sum_{k} X_{ik}A_{kj}  \\Big) = \\sum_{k} E(X_{ik}A_{kj}) = \\sum_{k} E(X_{ik})A_{kj} = (E(X)A)_{ij}\\]\nwhich implies \\[E(XA) = \\mu A\\]"
  },
  {
    "objectID": "notes/MultivariateStatisticsNotes.html#constants-get-squared-in-variance",
    "href": "notes/MultivariateStatisticsNotes.html#constants-get-squared-in-variance",
    "title": "Multivariate Statistics",
    "section": "Constants Get Squared in Variance",
    "text": "Constants Get Squared in Variance\nFor a scalar \\(a\\) and random variable \\(X\\):\n\\[\\begin{align}V(aX) &= E\\Big([aX-E(aX)]^2\\Big) = E((aX)^2) - (a\\mu)^2 = a^2(E(X^2) - \\mu^2) \\\\ &= a^2V(X)\\end{align}\\]\nFor matrix \\(A\\) acting on vector \\(X\\), the variance \\(V(AX)\\) is given by:\n\\[\\begin{align} V(AX) &= E\\Big(\\big[AX - A\\mu\\big]\\big[AX - A\\mu\\big]^T\\Big) \\\\ &= E\\Big(\\big[A(X - \\mu)\\big]\\big[A(X - \\mu)\\big]^T\\Big)\\\\ &=\nE\\Big(\\big[A(X - \\mu)\\big]\\big[(X - \\mu)^TA^T\\big]\\Big)\\\\ &= AE\\Big((X-\\mu)(X-\\mu)^T\\Big)A^T\\\\ &= A\\Sigma A^T \\end{align}\\]"
  },
  {
    "objectID": "notes/MultivariateStatisticsNotes.html#variance-of-a-sum",
    "href": "notes/MultivariateStatisticsNotes.html#variance-of-a-sum",
    "title": "Multivariate Statistics",
    "section": "Variance of a Sum",
    "text": "Variance of a Sum\nGiven random variables \\(X\\) and \\(Y\\) and their respective means \\(\\mu_X\\) and \\(\\mu_Y\\):\n\\[ \\begin{align}\nVar(X + Y) &= E(X + Y - \\mu_X - \\mu_Y)^2\\\\\n&= E\\big[(X-\\mu_X)^2 + (Y-\\mu_Y)^2 + 2(X-\\mu_X)(Y-\\mu_Y)\\big]\\\\\n&= Var(X) + Var(Y) + 2Cov(X,Y)\\\\\n\\end{align}\\]\nWith constants in the mix, this becomes:\n\\[ \\begin{align}\nVar(aX + bY) &=  Var(aX) + Var(bY) + 2Cov(aX,bY)\\\\\n&= a^2Var(X) + b^2Var(Y) + 2abCov(X,Y)\\\\\n\\end{align}\\]\nThis is especially helpful to understand variances of differences, or when coefficients are negative in general."
  },
  {
    "objectID": "notes/MultivariateStatisticsNotes.html#expectation-of-a-quadratic-form",
    "href": "notes/MultivariateStatisticsNotes.html#expectation-of-a-quadratic-form",
    "title": "Multivariate Statistics",
    "section": "Expectation of a Quadratic Form",
    "text": "Expectation of a Quadratic Form\nGiven a random vector \\(\\mathbf{y}\\) of size \\(n\\) mean \\(\\mathbf{\\mu}\\), and covariance matrix \\(Var(\\mathbf{y})=\\Sigma\\), the expectation of the product \\(\\mathbf{y}^T\\mathbf{A}\\mathbf{y}\\) can be found by looking at the expectation of each matrix element.\nThe matrix elements of a product of the matrices \\(\\mathbf{ABC}\\) can be expressed as:\n\\[\\begin{align}\n(\\mathbf{ABC})_{ij} &= \\sum_{k}(\\mathbf{AB})_{ik}c_{kj}\\\\\n&= \\sum_{k}(\\mathbf{AB})_{ik}c_{kj}\\\\\n&= \\sum_{k} \\big(\\sum_{l} a_{il}b_{lk}\\big)c_{kj}\\\\\n&= \\sum_{k} \\sum_{l} a_{il}b_{lk}c_{kj}\\\\\n\\end{align}\n\\]\nIn the quadratic form \\(\\mathbf{y}^T\\mathbf{A}\\mathbf{y}\\), the row index \\(i\\) of \\(\\mathbf{y}^T\\), a row vector and the column index of \\(\\mathbf{y}\\), a column vector, are both restricted to 1. If we ommit them, the elements of the quadratic form are given by:\n\\[\\begin{align}\n(\\mathbf{y}^T\\mathbf{A}\\mathbf{y})_{ij} &= \\sum_{k} \\sum_{l} y_{l}a_{lk}y_{k}\\\\\n&= \\sum_{k} \\sum_{l} y_{l}y_{k}a_{lk}\\\\\n\\end{align}\\]\nRemembering the definition of covariance, and the assumption of independent observations, we can look at expectation value element-wise:\n\\[\\begin{align}\nE(\\mathbf{y}^T\\mathbf{A}\\mathbf{y})_{ij} &= E\\big(\\sum_{k}\\sum_{l} y_{l}y_{k}a_{lk}\\big)\\\\\n&= \\sum_{k}\\sum_{l} E(y_{l}y_{k})a_{lk}\\\\\n&= \\sum_{k}\\sum_{l} \\big(Cov(y_{l},y_{k}) + \\mu_l\\mu_k\\big)a_{lk}\\\\\n&= \\sum_{k}\\sum_{l} Cov(y_{l},y_{k})a_{lk} + \\sum_{k}\\sum_{l}\\mu_la_{lk}\\mu_k\\\\\n&= \\sum_{k}\\sum_{l} Var(y)_{lk}a_{lk} + \\sum_{k}\\sum_{l}\\mu_la_{lk}\\mu_k\\\\\n&= \\sum_{k}\\sum_{l} Var(y)_{kl}a_{lk} + \\sum_{k}\\sum_{l}\\mu_la_{lk}\\mu_k\\tag{symmetry of cov matrix}\\\\\n&= \\sum_{i} Var(y)_{ii}a_{ii} + \\sum_{k}\\sum_{l}\\mu_la_{lk}\\mu_k \\tag{independent observations/diagonal matrix}\\\\\n&= trace(\\Sigma \\mathbf{A}) + \\big(\\mathbf{\\mu}^T\\mathbf{A}\\mathbf{\\mu}\\big)_{lk}\\\\\n\\end{align}\\]\nThus, on the matrix level we have: \\[\\begin{align}\nE(\\mathbf{y}^T\\mathbf{A}\\mathbf{y}) &= trace(\\Sigma \\mathbf{A}) + \\mathbf{\\mu}^T\\mathbf{A}\\mathbf{\\mu}\\\\\n\\end{align}\\]"
  },
  {
    "objectID": "notes/MultivariateStatisticsNotes.html#multivariate-gaussian",
    "href": "notes/MultivariateStatisticsNotes.html#multivariate-gaussian",
    "title": "Multivariate Statistics",
    "section": "Multivariate Gaussian",
    "text": "Multivariate Gaussian\nThe multivariate Gaussian distribution appears often and has the probability density function:\n\\[f(x) = \\dfrac{1}{(2\\pi)^{p/2}|\\Sigma|^{1/2}} \\exp\\Big[-\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)\\Big]\\]\nwhere \\(x\\) is a \\(p\\)-dimensional random vector, \\(\\mu = E(x)\\), \\(\\Sigma\\) is the covariance matrix and \\(|\\Sigma|\\) is the determinant of \\(\\Sigma\\).\nUseful Theorem Results:\n\nIf \\(Z\\sim N(0,1)\\) and \\(x = \\mu + \\Sigma^{1/2}Z\\), then \\(x\\sim N(\\mu,\\Sigma)\\).\nIf \\(x \\sim N(\\mu,\\Sigma)\\), then \\(\\Sigma^{-1/2}(x-\\mu)\\sim N(0,1)\\)\nIf \\(x\\sim N(\\mu, \\Sigma)\\) and \\(a\\) is a vector of same length as \\(x\\), then \\(a^Tx \\sim N(a^T\\mu, a^T\\Sigma a)\\).\nLet $ V = (x-)T{-1}(x-)$. Then \\(V \\sim \\chi^2_p\\)\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nmean = (1, 2)\ndiag = .6\ncov = [[1, diag], [diag, 1]]\nx = np.random.multivariate_normal(mean=mean,cov=cov, size=800)\nx.shape\n\nplt.plot(x[:, 0], x[:, 1], '.', alpha=0.5)\nplt.axis('equal')\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\n\nx.shape\n\n(800, 2)\n\n\n\nKL Divergence for Guassian Distributions\nOne useful is the Kullback-Leibler divergence, \\(D_{KL}\\), a metric for comparing two probability distributions, between two Guassians, which we now show here.\nRecall that the KL is given by \\[D_{KL}(P||Q) = E_P \\Big[\\log \\frac{P}{Q}\\Big]\\]\nLet \\(P\\) and \\(Q\\) be two multivariate Gaussians of dimension \\(n\\) with means \\(\\mu_P, \\mu_Q\\) and variances \\(\\Sigma_P, \\Sigma_Q\\), respectively. We begin to find the \\(D_{KL}\\) between these two starting off by cancelling the common normalizing constants, flipping the inverse determinants, and turning the logarithm of a fraction into a difference of logarithms, cancelling out the exponentials in the process:\n\\[\\begin{align}\nD_{KL}(P||Q) &= E_P\\Bigg[\\log\\Big[\\frac{\\dfrac{1}{(2\\pi)^{n/2}|\\Sigma_P|^{1/2}} \\exp\\big[-\\frac{1}{2}(x-\\mu_P)^T\\Sigma_P^{-1}(x-\\mu_P)\\big]}{\\dfrac{1}{(2\\pi)^{n/2}|\\Sigma_Q|^{1/2}} \\exp\\big[-\\frac{1}{2}(x-\\mu_Q)^T\\Sigma_Q^{-1}(x-\\mu_Q)\\big]}\\Big]\\Bigg]\\\\\n            &= E_P\\Bigg[\\log\\Big[\\frac{|\\Sigma_Q|^{1/2} \\exp\\big[-\\frac{1}{2}(x-\\mu_P)^T\\Sigma_P^{-1}(x-\\mu_P)\\big]}{|\\Sigma_P|^{1/2} \\exp\\big[-\\frac{1}{2}(x-\\mu_Q)^T\\Sigma_Q^{-1}(x-\\mu_Q)\\big]}\\Big]\\Bigg]\\\\\n            &= E_P\\Big[\\frac{1}{2}\\log \\frac{|\\Sigma_Q|}{|\\Sigma_P|}  -\\frac{1}{2}(x-\\mu_P)^T\\Sigma_P^{-1}(x-\\mu_P) + \\frac{1}{2}(x-\\mu_Q)^T\\Sigma_Q^{-1}(x-\\mu_Q)\\Big] \\\\\n            &= \\frac{1}{2}\\log \\frac{|\\Sigma_Q|}{|\\Sigma_P|} + \\frac{1}{2}E_P\\Big[-(x-\\mu_P)^T\\Sigma_P^{-1}(x-\\mu_P) + (x-\\mu_Q)^T\\Sigma_Q^{-1}(x-\\mu_Q)\\Big]\\\\\n     &=\\frac{1}{2}\\log \\frac{|\\Sigma_Q|}{|\\Sigma_P|} + \\frac{1}{2}\\Big[-trace(\n     \\Sigma_P\\Sigma_P^{-1}) + trace(\\Sigma_Q\\Sigma_P^{-1}) + (\\mu_P-\\mu_Q)^T\\Sigma_Q^{-1}(\\mu_P-\\mu_Q)\\Big]\\\\\n     &=\\frac{1}{2}\\log \\frac{|\\Sigma_Q|}{|\\Sigma_P|} + \\frac{1}{2}\\Big[-n + trace(\\Sigma_Q\\Sigma_P^{-1}) + (\\mu_P-\\mu_Q)^T\\Sigma_Q^{-1}(\\mu_P-\\mu_Q)\\Big]\\\\\n     &= \\frac{1}{2}\\Bigg[\\log \\frac{|\\Sigma_Q|}{|\\Sigma_P|}  -n + trace(\\Sigma_Q\\Sigma_P^{-1}) + (\\mu_P-\\mu_Q)^T\\Sigma_Q^{-1}(\\mu_P-\\mu_Q)\\Bigg]\\\\\n\\end{align}     \n\\]\nIn our derivation, we used the expectation of quadratic forms results from before, \\(E(\\mathbf{y}^T\\mathbf{A}\\mathbf{y}) = trace(\\Sigma \\mathbf{A}) + \\mathbf{\\mu}^T\\mathbf{A}\\mathbf{\\mu}\\), with the vectors \\(y = x - \\mu_i\\), matrix \\(\\mathbf{A} =\\Sigma_i^{-1}\\) and expectations, \\(E_P(x) = \\mu_P\\), \\(E_P(x - \\mu_P) = 0\\) and \\(E_P(x-\\mu_Q)\\)."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Self Learning Journey\n\n\nReflections on my (ongoing) journey of self-study\n\n\n\n\n\n\nJun 4, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/library/index.html#physics",
    "href": "projects/library/index.html#physics",
    "title": "Self Learning Journey",
    "section": "Physics",
    "text": "Physics\n\nThe Feynman Lectures on Physics by Richard Feynman\n\nThese volumes have a wealth of insight and beautiful exposition straight from one of greatest teachers and legends of the subject. What other physics text has the Krebs Cycle?\n\nPrinciples of Quantum Mechanics by Ramamurti Shankar\n\nWith solved problems, fantastic organization and such a strong 1st chapter, this text helped me learn quantum mechanics more than any of the numerous classes I’ve taken."
  },
  {
    "objectID": "projects/library/index.html#mathematics",
    "href": "projects/library/index.html#mathematics",
    "title": "Self Learning Journey",
    "section": "Mathematics",
    "text": "Mathematics\n\nHow to Solve It by George Polya\n\nThis was an influential read that provides a general framework on solving problems not only in math, but beyond.\n\nHow to Prove It by Daniel J. Velleman\n\nI recall stumbling through proofs before working through the first few chapters of this helpful resource.\n\nAll of Statistics by Larry Wasserman\n\nThis book has such a high range of topics and working through through many of the problems yielded long-lasting insights into graduate statistics.\n\nStatistical Inference by George Casella and Roger Berger\n\nThis is a staple reference whenever I find myself requiring deeper insight into fundamentals."
  },
  {
    "objectID": "projects/library/index.html#machine-learning",
    "href": "projects/library/index.html#machine-learning",
    "title": "Self Learning Journey",
    "section": "Machine Learning",
    "text": "Machine Learning\n\nInformation Theory, Inference and Learning Algorithms by David MacKay\n\nAnother amazing resource for self-study, providing insightful expositions and exercises with Bayesian inference, core information theory like entropy and machine learning’s relation to compression.\n\nIntroduction to Statistical Learning by Hastie, Tibshirani, James and Witten\n\nThis book has some nice explanations of decision trees and is accessible.\n\nThe Elements of Statistical Learning by Hastie, Tibshirani and Friedman\n\nA well known resource and great reference for insights into machine learning.\n\nDeep Learning for Coders with fastai & PyTorch by Jeremy Howard and Sylvain Gugger\n\nThis book had such practical bits of knowledge and insights into Pytorch and applied machine learning, by one of the greatest contributors to the machine learning community.\n\nNatural Language Processing with Transformers by Tunstall, von Werra and Wolf\n\nAnother great practical book by contributors to the machine learning community, providing insight into the transformers architecture and library.\n\nEffective Xgboost by Matt Harrison\n\nA practical guide to XGBoost and integration into workflows.\n\nEffective Data Science Infrastructure by Ville Tuulos Written by the author of the library Metaflow, this provides great insights into productive workflows in data science and expressing that in code.\nMachine Learning with PyTorch and Scikit-Learn by Sebastian Raschka\n\nThis book has a nice blend of fundamentals with application, signature to the author’s insightful writings and work in general.\n\nDesigning Data-Intensive Applications by Martin Kleppmann\n\nA great reference for studying system design.\n\nThe Kaggle Book by Banachewicz and Massaron\n\nFilled with insights and tips in data science gleaned from Kaggle competitions.\n\nThe Kaggle Workbook by Banachewicz and Massaron\n\nThe practical counterpart to the previous book that helps build skill via code exercises.\n\nDesigning Machine Learning Systems by Chip Huyen\n\nThis was such a great book that helped me understand the bigger picture with machine learning, in the context of system design and MLOps.\n\nAI Engineering by Chip Huyen\n\nWhat the author’s first book did for ML systems, this book does for generative AI applications, providing a wide survey of important topics, in digestible bits, preserving key details and putting it all together in the final chapter."
  },
  {
    "objectID": "projects/library/index.html#computer-science",
    "href": "projects/library/index.html#computer-science",
    "title": "Self Learning Journey",
    "section": "Computer Science",
    "text": "Computer Science\n\nIntroduction to Algorithms by Cormen, Lieserson, Rivest, and Stein\n\nKnown as CLRS, it serves as a reference for classic algorithms, and is a good source of theory.\n\n\nThe Algorithm Design Manual by Steven Skiena\n\nThis is a text that is enjoyable to read beyond dry facts and has anecdotes from the author’s experiences in the wild along with C code implementations.\n\nAlgorithm Design by Jon Kleinberg\n\nSomewhat in between the previous two resources, this is worth taking a look at with its different perspectives and examples."
  },
  {
    "objectID": "projects/library/index.html#programming",
    "href": "projects/library/index.html#programming",
    "title": "Self Learning Journey",
    "section": "Programming",
    "text": "Programming\n\nElements of Programming Interviews in Python by Aziz, Lee, and Prakash\n\nMy main source of interview preparation, it’s filled with difficult problems and clever coding style.\n\nEffective Pandas by Matt Harrison\n\nA great resource that has helped me take pride at my Pandas code and learn even more about the staple library.\n\nArchitecture Patterns with Python by Percival and Gregory\n\nThis book provided great insight with low level design and the rationale behind abstractions, providing actual code for event-driven systems.\n\nMastering Regular Expressions by Jeffrey Friedl\n\nThis is the resource of choice to better understand regex past just the basic level to greater commands and optimizing expressions.\n\nSoftware Engineering at Google by Winters, Manshreck and Wright\n\nThis provides a new perspective on software engineering both in dimension and scale."
  },
  {
    "objectID": "projects/library/index.html#economics",
    "href": "projects/library/index.html#economics",
    "title": "Self Learning Journey",
    "section": "Economics",
    "text": "Economics\n\nBasic Economics by Thomas Sowell\n\nAs someone who has never taken an economics class, the insight I gained into monetary matters while reading this book was similar to what the Feynman lectures provided for me for physics.\n\nHealthcare Policy Issues: An Economic Perspective by Feldstein and Melnick\n\nThis book helped me better understand the US healthcare system, its players and potential reasons for its issues."
  },
  {
    "objectID": "projects/library/index.html#productivity",
    "href": "projects/library/index.html#productivity",
    "title": "Self Learning Journey",
    "section": "Productivity",
    "text": "Productivity\n\nThe 7 Habits of Highly Effective People by Stephen Covey\n\nA great read that provides a little guidance and organizing principles to living.\n\nAtomic Habits by James Clear\n\nThe earlier one reads and implements this, the better.\n\nSo Good They Can’t Ignore You by Cal Newport\n\nA fun read that provided inspiration to keep up with self-improvement and prioritize learning in career choices.\n\nDeep Work by Cal Newport\n\nThis book really drives home how concentration and attention are essential to modern success and work life.\n\n\nSlow Productivity by Cal Newport\n\nAnother book by Newport, this one reaffirms personal experiences I’ve had prioritizing consistency over bursts of productivity.\n\nRange: Why Generalists Triumph in a Specialized World by David Epstein\n\nI like being well-rounded and this book put generalists on the same pedestal we typically do for specialists.\n\nGrit by Angela Duckworth\n\nInspiring stories that emphasize the importance of perseverance.\n\nThe Startup of You by Reid Hoffman\n\nA useful read to gain a new, entrepenurial perspective on one’s self in the economy.\n\nZero to One by Peter Thiel\n\nThis book provided some insightful perspective on startups, innovation and monopolies."
  },
  {
    "objectID": "projects/library/index.html#career-guides",
    "href": "projects/library/index.html#career-guides",
    "title": "Self Learning Journey",
    "section": "Career Guides",
    "text": "Career Guides\n\nThe Effective Engineer by Edmond Lau\n\nAn interesting read on how to be a better engineer, framing it in the context of high leverage activities.\n\nEngineer’s Survival Guide by Merih Taze\n\nGood advice from the author’s personal experiences about navigating an SWE career.\n\nThe Staff Engineer’s Path by Tanya Reilly\n\nI appreciated the perspective of taking the individual contributor path to the highest levels and what that looks like.\n\nThe Software Engineer’s Guidebook by Gergely Orosz\n\nThis truly is a guidebook for the SWE career path and provides so much information on what to expect at different environments, meetings and managers and time in the career."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jonathan Sarker",
    "section": "",
    "text": "Welcome!\nHi, I’m Jonathan. I’m a life-long learner with interests spanning from biophysics, education and machine learning. This is my personal site to share projects, interesting ideas and personal musings. Happy browsing!"
  },
  {
    "objectID": "notes/Decorators.html",
    "href": "notes/Decorators.html",
    "title": "Decorator Notes",
    "section": "",
    "text": "These are notes based on Real Python’s primer.\n\n\n\n\n\ndef parent(num):\n    def first_child():\n        return \"Hi, I am Emma\"\n\n    def second_child():\n        return \"Call me Liam\"\n\n    if num == 1:\n        return first_child\n    else:\n        return second_child\n\n\nparent(2)\n\n&lt;function __main__.parent.&lt;locals&gt;.second_child()&gt;\n\n\n\n\n\n\ndef my_decorator(func):\n    def wrapper():\n        print(\"Something is happening before the function is called.\")\n        func()\n        print(\"Something is happening after the function is called.\")\n    return wrapper\n\ndef say_whee():\n    print(\"Whee!\")\n\nsay_whee = my_decorator(say_whee)\n\n\nsay_whee()\n\nSomething is happening before the function is called.\nWhee!\nSomething is happening after the function is called.\n\n\nNow with syntactic sugar:\n\ndef my_decorator(func):\n    def wrapper():\n        print(\"Something is happening before the function is called.\")\n        func()\n        print(\"Something is happening after the function is called.\")\n    return wrapper\n\n@my_decorator\ndef say_whee():\n    print(\"Whee!\")\n\n\nsay_whee()\n\nSomething is happening before the function is called.\nWhee!\nSomething is happening after the function is called.\n\n\n\n\n\nDecorating simple functions is easy enough, but what happens when decorated function takes in arguments? The second function greet() is incompatible with the decorator, do_twice since the inner function does not take any arguments.\n\ndef do_twice(func):\n    def wrapper_do_twice():\n        func()\n        func()\n    return wrapper_do_twice\n\n@do_twice\ndef say_whee():\n    print('Whee!')\n\n@do_twice\ndef greet(name: str):\n    print(f'Hello {name}!')\n\nsay_whee()\n\nWhee!\nWhee!\n\n\n\ngreet('Jonathan')\n\nTypeError: do_twice.&lt;locals&gt;.wrapper_do_twice() takes 0 positional arguments but 1 was given\n\n\nInstead, we can add arguments to our decorator to handle any potential parameters using *args, **kwargs**:\n\ndef do_twice(func):\n    def wrapper_do_twice(*args, **kwargs):\n        func(*args, **kwargs)\n        func(*args, **kwargs)\n    return wrapper_do_twice\n\n@do_twice\ndef say_whee():\n    print(\"Whee!\")\n    \n@do_twice\ndef greet(name):\n    print(f\"Hello {name}\")\n\n\nsay_whee()\n\nWhee!\nWhee!\n\n\n\ngreet('Jonathan')\n\nHello Jonathan\nHello Jonathan\n\n\n\n\n\nFor functions that return values, we must make sure the decorator’s inner function also returns a value. If not, like in the case of the do_twice decorator’s current form, the decorated function’s return value will get lost:\n\n@do_twice\ndef return_greeting(name: str):\n    print('Creating greeting')\n    return f'Hi {name}!'\n\nhi_jonathan = return_greeting('Jonathan')\n\nCreating greeting\nCreating greeting\n\n\n\nprint(hi_jonathan)\n\nNone\n\n\n\ndef do_twice(func):\n    def do_twice_wrapper(*args, **kwargs):\n       func(*args, **kwargs)\n       return func(*args, **kwargs)\n    return do_twice_wrapper\n\n@do_twice\ndef return_greeting(name: str):\n    print('Creating greeting')\n    return f'Hi {name}!'\n\nhi_jonathan = return_greeting('Jonathan')\n\nCreating greeting\nCreating greeting\n\n\n\nprint(hi_jonathan)\n\nHi Jonathan!\n\n\n\n\n\nPython functions have built-in documentation. Decorated ones simply show themselves as the inner function of the wrapper. To get the original, useful documenation of a decorated function, the decorator must have @functools.wraps(func) above its inner function definition:\n\nprint\n\n&lt;function print(*args, sep=' ', end='\\n', file=None, flush=False)&gt;\n\n\n\nprint.__name__\n\n'print'\n\n\n\nhelp(print)\n\nHelp on built-in function print in module builtins:\n\nprint(...)\n    print(value, ..., sep=' ', end='\\n', file=sys.stdout, flush=False)\n    \n    Prints the values to a stream, or to sys.stdout by default.\n    Optional keyword arguments:\n    file:  a file-like object (stream); defaults to the current sys.stdout.\n    sep:   string inserted between values, default a space.\n    end:   string appended after the last value, default a newline.\n    flush: whether to forcibly flush the stream.\n\n\n\n\nsay_whee\n\n&lt;function __main__.do_twice.&lt;locals&gt;.wrapper_do_twice(*args, **kwargs)&gt;\n\n\n\nsay_whee.__name__\n\n'wrapper_do_twice'\n\n\n\nhelp(say_whee)\n\nHelp on function wrapper_do_twice in module __main__:\n\nwrapper_do_twice(*args, **kwargs)\n\n\n\n\nfrom functools import wraps\n\ndef do_twice(func):\n    @wraps(func)\n    def wrapper_do_twice(*args, **kwargs):\n        func(*args, **kwargs)\n        return func(*args, **kwargs)\n    return wrapper_do_twice\n\n@do_twice\ndef say_whee():\n    print(\"Whee!\")\n\n\nsay_whee\n\n&lt;function __main__.say_whee()&gt;\n\n\n\nsay_whee.__name__\n\n'say_whee'\n\n\n\nhelp(say_whee)\n\nHelp on function say_whee in module __main__:\n\nsay_whee()\n\n\n\n\nhasattr()\n\n\n\n\n\nThe pattern for many decorators will take this form:\n\nimport functools\n\ndef decorator(func):\n    @functools.wrap(func)\n    def wrapper_decorator(*args, **kwargs):\n        # Do something before\n        value = func(*args, **kwargs)\n        # Do something after\n        return value\n    return wrapper_decorator\n\n\n\n\nimport functools\nimport time\n\ndef timer(func):\n    \"\"\"Print the runtime of the decorated function\"\"\"\n    @functools.wraps(func)\n    def wrapper_timer(*args, **kwargs):\n        start = time.perf_counter()\n        value = func(*args, **kwargs)\n        end = time.perf_counter()\n        runtime = end - start\n        print(f\"Finished {func.__name__}() in {runtime:.4f} secs\")\n        return value\n    return wrapper_timer\n\n\n@timer\ndef waste_some_time(num_times: int):\n    for _ in range(num_times):\n        sum([number**2 for number in range(10_000)])\n\nwaste_some_time(1)\n        \n\nFinished waste_some_time() in 0.0021 secs\n\n\n\nwaste_some_time(999)\n\nFinished waste_some_time() in 1.1273 secs\n\n\n\n\n\nWe can take advantage of the namespace access in the wrapped function to display parameters and return values for debugging:\n\nimport functools\n\ndef debug(func):\n    \"\"\"Print the function signature and return value\"\"\"\n    @functools.wraps(func)\n    def wrapper_debug(*args, **kwargs):\n        args_repr = [repr(a) for a in args]\n        kwargs_repr = [f\"{k}={repr(v)}\" for k, v in kwargs.items()]\n        signature = \", \".join(args_repr + kwargs_repr)\n        print(f\"Calling {func.__name__}({signature})\")\n        value = func(*args, **kwargs)\n        print(f\"{func.__name__}() returned {repr(value)}\")\n        return value\n    return wrapper_debug\n\n@debug\ndef make_greeting(name: str, age: int=None):\n    if age is None:\n        return f\"Howdy {name}!\"\n    else:\n        return f\"Whoa {name}! {age} already?! You're growing up!\"\n\nmake_greeting(\"Benjamin\")\n\nCalling make_greeting('Benjamin')\nmake_greeting() returned 'Howdy Benjamin!'\n\n\n'Howdy Benjamin!'\n\n\n\nmake_greeting(\"Juan\", age=114)\n\nCalling make_greeting('Juan', age=114)\nmake_greeting() returned \"Whoa Juan! 114 already?! You're growing up!\"\n\n\n\"Whoa Juan! 114 already?! You're growing up!\"\n\n\nThis is more impactful when used on small inconvenience functions not called directly:\n\nimport math\n\nmath.factorial = debug(math.factorial)\n\ndef approximate_e(terms=18):\n    return sum(1 / math.factorial(n) for n in range(terms))\n\napproximate_e(terms=5)\n\nCalling factorial(0)\nfactorial() returned 1\nCalling factorial(1)\nfactorial() returned 1\nCalling factorial(2)\nfactorial() returned 2\nCalling factorial(3)\nfactorial() returned 6\nCalling factorial(4)\nfactorial() returned 24\n\n\n2.7083333333333335\n\n\n\n\n\nSometimes, it’s useful to slow down code for the purposes of rate limiting, which can be achieved by a decorator:\n\nimport functools\nimport time\n\ndef slow_down(func):\n    @functools.wraps(func)\n    def wrapper_slow_down(*args, **kwargs):\n        time.sleep(1)\n        return func(*args, **kwargs)\n    return wrapper_slow_down\n\n@slow_down\ndef countdown(from_number):\n    if from_number &lt; 1:\n        print(\"Liftoff!\")\n    else:\n        print(from_number)\n        countdown(from_number - 1)\n\ncountdown(3)\n\n3\n2\n1\nLiftoff!\n\n\n\n\n\nHere we use a decorator to leave functions unmodified, but record them in a dictionary. This is a list separate from globals():\n\nPLUGINS = dict()\n\ndef register(func):\n    \"\"\"Register a funciton as a plug-in\"\"\"\n    PLUGINS[func.__name__] = func\n    return func\n\n@register\ndef say_hello(name: str):\n    return f\"Hello {name}\"\n\n@register\ndef be_awesome(name: str):\n    return f\"Yo {name}, together, we're awesome!\"\n\nPLUGINS\n\n{'say_hello': &lt;function __main__.say_hello(name: str)&gt;,\n 'be_awesome': &lt;function __main__.be_awesome(name: str)&gt;}\n\n\n\nbe_awesome('Jonathan')\n\n\"Yo Jonathan, together, we're awesome!\"\n\n\nHere we use the string representation of a Python object using the !r flag.\n\nimport random\n\ndef randomly_greet(name):\n    greeter, greeter_func = random.choice(list(PLUGINS.items()))\n    print(f\"Using {greeter!r}\")\n    return greeter_func(name)\n\nrandomly_greet(\"Alice\")\n\nUsing 'say_hello'\n\n\n'Hello Alice'\n\n\n\n\n\n\nimport functools\nfrom flask import Flask, g, request, redirect, url_for\n\napp = Flask(__name__)\n\ndef login_required(func):\n    \"\"\"Make sure user is logged in before proceeding\"\"\"\n    @functools.wraps(func)\n    def wrapper_login_required(*args, **kwargs):\n        if g.user is None:\n            return redirect(url_for(\"login\", next=request.url))\n        return func(*args, **kwargs)\n    return wrapper_login_required\n\n@app.route(\"/secret\")\n@login_required\ndef secret():\n    ...\n\n\n\n\n\n\n\nThis can be achieved on the individual methods or the entire class itself:\n\nclass TimeWaster:\n    @debug\n    def __init__(self, max_num: int):\n        self.max_num = max_num\n\n    @timer\n    def waste_time(self, num_times: int):\n        for _ in range(num_times):\n            sum([number**2 for number in range(self.max_num)])\n\ntw = TimeWaster(1000)\n\nCalling __init__(&lt;__main__.TimeWaster object at 0x000001D0871DACF0&gt;, 1000)\n__init__() returned None\n\n\n\ntw.waste_time(999)\n\nFinished waste_time() in 0.1318 secs\n\n\n\nfrom dataclasses import dataclass\n\n@dataclass\nclass PlayingCard:\n    rank: str\n    suit: str\n\nDecorating a class does not decorate its methods:\n\n@timer\nclass TimeWaster:\n    def __init__(self, max_num):\n        self.max_num = max_num\n\n    def waste_time(self, num_times):\n        for _ in range(num_times):\n            sum([i**2 for i in range(self.max_num)])\n\ntw = TimeWaster(1000)\n\nFinished TimeWaster() in 0.0000 secs\n\n\n\ntw.waste_time(999)\n\n\n\n\n\n@debug\n@do_twice\ndef greet(name):\n    print(f'Hello {name}!')\n\ngreet('Yadi')\n\nCalling wrapper_do_twice('Yadi')\nHello Yadi!\nHello Yadi!\nwrapper_do_twice() returned None\n\n\n\n@do_twice\n@debug\ndef greet(name):\n    print(f'Hello {name}!')\n\ngreet('Yadi')\n\nCalling greet('Yadi')\nHello Yadi!\ngreet() returned None\nCalling greet('Yadi')\nHello Yadi!\ngreet() returned None\n\n\n\n\n\nWe know how to define a decorator on general functions and how to add functionality to the decorated function. To add parameters to a decorator, we apply the same principle and decorate the decorator.\nThis adds another layer. The syntactic sugar saves more typing since another layer of nesting requires another function reassignment, from outer to inner.\n\nimport functools\n\ndef repeat(num_times: int):\n    def decorator_repeat(func):\n        functools.wraps(func)\n        def wrapper_decorator_repeat(*args, **kwargs):\n            for _ in range(num_times):\n                value = func(*args, **kwargs)\n            return value\n        return wrapper_decorator_repeat\n    return decorator_repeat\n\ndef greet(name: str):\n    print(f'Hello {name}!')\n\nrepeat4 = repeat(4)\ngreet = repeat4(greet)\ngreet('Jonathan')\n\nHello Jonathan!\nHello Jonathan!\nHello Jonathan!\nHello Jonathan!\n\n\n\n@repeat(4)\ndef greet(name: str):\n    print(f'Hello {name}!')\n\ngreet('Jonathan')\n\nHello Jonathan!\nHello Jonathan!\nHello Jonathan!\nHello Jonathan!\n\n\n\n\n\nTo modify our decorator to have optional arguments, we must modify the outermost layer in two ways: - to take in arguments with the * syntax - control flow of optional parameter values\nThis approach wraps the decorator with an outer layer that takes optional arguments, but also allows for a “skip” connection to layer below in case parameters are not specified. In this case, the synactic sugar will only take in the first argument, which is the function to be decorated.\n\ndef name(_func=None, *, key1=value1, key2=value2, ...):\n    def decorator_name(func):\n        ... # Create and return a wrapper function\n\n    if _func is None:\n        return decorator_name\n    else:\n        return decorator_name(_func)\n\n\nimport functools\n\ndef repeat(_func=None, *, num_times=2):\n    def decorator_repeat(func):\n        @functools.wraps(func)\n        def wrapper_repeat(*args, **kwargs):\n            for _ in range(num_times):\n                value = func(*args, **kwargs)\n            return value\n        return wrapper_repeat\n\n    if _func is None:\n        return decorator_repeat\n    else:\n        return decorator_repeat(_func)\n\n@repeat\ndef say_whee():\n    print('Whee!')\n\n@repeat(num_times=3)\ndef greet(name):\n    print(f'Hello {name}!')\n\nsay_whee()\n\nWhee!\nWhee!\n\n\n\ngreet('Jonathan')\n\nHello Jonathan!\nHello Jonathan!\nHello Jonathan!\n\n\n\n\n\nHere we an use function attributes to track a state variable such as a counter\n\nimport functools\n\ndef count_calls(func):\n    @functools.wraps(func)\n    def wrapper_count_calls(*args, **kwargs):\n        wrapper_count_calls.num_calls += 1\n        print(f'Call {wrapper_count_calls.num_calls} of {func.__name__}()')\n        return func(*args, **kwargs)\n    wrapper_count_calls.num_calls = 0\n    return wrapper_count_calls\n    \n\n\n\n\nIf we step back and think about how a closure decorator works, we have the following steps: - if there are decorator parameters, first initialize an “instance” with them set - next, rename the function to be modified with its wrapped self: func = decorator(func) - finally, call the function as normal with its usual inputs\nThese steps are similar to how a class instance works with a __init__() and __call__() method. The analogous steps are: - define an __init__() that sets the parameters of the decorator, acting as the outermost function in a nested closure - define a __call__() method with the structure of a closure - use functools.wraps() for appropriate introspection - have the __call__() method take in the function as an argument - define an inner function matching the decorated function’s inputs along with additional logic - return the inner function to finalize the __call__() method - for simpler, unparameterized decorating classes, use functools.update_wrapper() instead of functools.wraps() in the __init__() method for proper introspection\n\nclass Counter:\n    def __init__(self, start=0):\n        self.count = start\n    def __call__(self):\n        self.count += 1\n        print(f\"Current count is {self.count}\")\n\ncounter = Counter()\n\ncounter()\n    \n\nCurrent count is 1\n\n\n\ncounter()\n\nCurrent count is 2\n\n\nNow, with the modifications to make it a simple decorator without parameters:\n\nimport functools\nclass Counter:\n    def __init__(self, func):\n        functools.update_wrapper(self, func)\n        self.func = func\n        self.num_calls = 0\n\n    def __call__(self, *args, **kwargs):\n        self.num_calls += 1\n        print(f\"Call {self.num_calls} of {self.func.__name__}()\")\n        return self.func(*args, **kwargs)\n\n@Counter\ndef say_whee():\n    print('Whee!')\n\n\nsay_whee()\n\nCall 1 of say_whee()\nWhee!\n\n\n\nsay_whee()\n\nCall 2 of say_whee()\nWhee!\n\n\n\nsay_whee.num_calls\n\n2\n\n\nFinally, we have this example that was generated by Gemini where the __init__() sets parameters and the __call__() is implemented as a closure with general paramters:\n\nimport functools\nfrom datetime import datetime\n\nclass FunctionLogger:\n    \"\"\"\n    A class-based decorator to log function execution details.\n    It takes parameters to customize the log output.\n    \"\"\"\n    def __init__(self, log_level=\"INFO\", format_str=\"{timestamp} [{level}] - Entering '{func_name}'\"):\n        \"\"\"\n        The __init__ method is called when the decorator is instantiated.\n        It captures the decorator's arguments.\n        Example: @FunctionLogger(log_level=\"DEBUG\")\n        \"\"\"\n        print(f\"Decorator `FunctionLogger` is being initialized with level: {log_level}\")\n        self.log_level = log_level\n        self.format_str = format_str\n\n    def __call__(self, func):\n        \"\"\"\n        The __call__ method is invoked with the decorated function as its argument.\n        It must return a replacement (wrapper) function.\n        \"\"\"\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            \"\"\"\n            This is the wrapper function that gets executed instead of the original.\n            It contains the decorator's core logic.\n            \"\"\"\n            timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n            \n            # 1. Logic before the original function is called\n            log_message = self.format_str.format(\n                timestamp=timestamp,\n                level=self.log_level,\n                func_name=func.__name__\n            )\n            print(log_message)\n            \n            # 2. Call the original function\n            result = func(*args, **kwargs)\n            \n            # 3. Logic after the original function is called\n            print(f\"{timestamp} [{self.log_level}] - Exiting '{func.__name__}'\")\n            \n            return result\n            \n        # The __call__ method returns the wrapper function\n        return wrapper\n\n# --- Example Usage ---\n\n@FunctionLogger(log_level=\"DEBUG\", format_str=\"{timestamp} - {level} - Calling `{func_name}`...\")\ndef add(x, y):\n    \"\"\"This function adds two numbers together.\"\"\"\n    print(f\"  &gt; Inside add({x}, {y})\")\n    return x + y\n\n@FunctionLogger(log_level=\"WARNING\")\ndef greet(name):\n    \"\"\"This function greets a person.\"\"\"\n    print(f\"  &gt; Hello, {name}!\")\n\ngreet('Jonathan')\n\nDecorator `FunctionLogger` is being initialized with level: DEBUG\nDecorator `FunctionLogger` is being initialized with level: WARNING\n2025-09-25 12:44:26 [WARNING] - Entering 'greet'\n  &gt; Hello, Jonathan!\n2025-09-25 12:44:26 [WARNING] - Exiting 'greet'\n\n\n\ngreet('Jonathan')\n\n2025-09-25 12:45:14 [WARNING] - Entering 'greet'\n  &gt; Hello, Jonathan!\n2025-09-25 12:45:14 [WARNING] - Exiting 'greet'\n\n\n\n\n\n\n\n\n\nimport functools\nimport time\n\ndef slow_down(_func=None, *, rate=1):\n    \"\"\"Sleep given amount of seconds before calling the function\"\"\"\n    def decorator_slow_down(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            time.sleep(rate)\n            return func(*args, **kwargs)\n        return wrapper\n    if _func is None:\n        return decorator_slow_down\n    else:\n        return decorator_slow_down(_func)\n        \n\n@slow_down(rate=2)\ndef countdown(from_number: int):\n    if from_number &lt; 1:\n        print('Liftoff!')\n    else:\n        print(from_number)\n        countdown(from_number - 1)\n\ncountdown(3)\n\n3\n2\n1\nLiftoff!\n\n\n\n\n\nPython has a number of singletons, classes with only one instance: - True, False, None\nThis allows for use of is to check for equality, or rather, identity. We can use a decorator to store the first instance of a class as a function attribute, enforcing the singleton design pattern:\n\nimport functools\n\ndef singleton(cls):\n    \"\"\"Make a class a Singleton class\"\"\"\n    @functools.wraps(cls)\n    def wrapper_singleton(*args, **kwargs):\n        if wrapper_singleton.instance is None:\n            wrapper_singleton.instance = cls(*args, **kwargs)\n        return wrapper_singleton.instance\n    wrapper_singleton.instance = None\n    return wrapper_singleton\n\n@singleton\nclass TheOne:\n    pass\n\nfirst_one = TheOne()\n\nanother_one = TheOne()\nid(first_one)\n\n1995139107200\n\n\n\nid(another_one)\n\n1995139107200\n\n\n\nfirst_one is another_one\n\nTrue\n\n\n\n\n\n\nimport functools\n\ndef count_calls(func):\n    @functools.wraps(func)\n    def wrapper_count_calls(*args, **kwargs):\n        wrapper_count_calls.num_calls += 1\n        print(f'Call {wrapper_count_calls.num_calls} of {func.__name__}()')\n        return func(*args, **kwargs)\n    wrapper_count_calls.num_calls = 0\n    return wrapper_count_calls\n \n@count_calls\ndef fibonacci(num):\n    if num &lt; 2:\n        return num\n    return fibonacci(num - 2) + fibonacci(num - 1)\n\nfibonacci(10)\n\nCall 1 of fibonacci()\nCall 2 of fibonacci()\nCall 3 of fibonacci()\nCall 4 of fibonacci()\nCall 5 of fibonacci()\nCall 6 of fibonacci()\nCall 7 of fibonacci()\nCall 8 of fibonacci()\nCall 9 of fibonacci()\nCall 10 of fibonacci()\nCall 11 of fibonacci()\nCall 12 of fibonacci()\nCall 13 of fibonacci()\nCall 14 of fibonacci()\nCall 15 of fibonacci()\nCall 16 of fibonacci()\nCall 17 of fibonacci()\nCall 18 of fibonacci()\nCall 19 of fibonacci()\nCall 20 of fibonacci()\nCall 21 of fibonacci()\nCall 22 of fibonacci()\nCall 23 of fibonacci()\nCall 24 of fibonacci()\nCall 25 of fibonacci()\nCall 26 of fibonacci()\nCall 27 of fibonacci()\nCall 28 of fibonacci()\nCall 29 of fibonacci()\nCall 30 of fibonacci()\nCall 31 of fibonacci()\nCall 32 of fibonacci()\nCall 33 of fibonacci()\nCall 34 of fibonacci()\nCall 35 of fibonacci()\nCall 36 of fibonacci()\nCall 37 of fibonacci()\nCall 38 of fibonacci()\nCall 39 of fibonacci()\nCall 40 of fibonacci()\nCall 41 of fibonacci()\nCall 42 of fibonacci()\nCall 43 of fibonacci()\nCall 44 of fibonacci()\nCall 45 of fibonacci()\nCall 46 of fibonacci()\nCall 47 of fibonacci()\nCall 48 of fibonacci()\nCall 49 of fibonacci()\nCall 50 of fibonacci()\nCall 51 of fibonacci()\nCall 52 of fibonacci()\nCall 53 of fibonacci()\nCall 54 of fibonacci()\nCall 55 of fibonacci()\nCall 56 of fibonacci()\nCall 57 of fibonacci()\nCall 58 of fibonacci()\nCall 59 of fibonacci()\nCall 60 of fibonacci()\nCall 61 of fibonacci()\nCall 62 of fibonacci()\nCall 63 of fibonacci()\nCall 64 of fibonacci()\nCall 65 of fibonacci()\nCall 66 of fibonacci()\nCall 67 of fibonacci()\nCall 68 of fibonacci()\nCall 69 of fibonacci()\nCall 70 of fibonacci()\nCall 71 of fibonacci()\nCall 72 of fibonacci()\nCall 73 of fibonacci()\nCall 74 of fibonacci()\nCall 75 of fibonacci()\nCall 76 of fibonacci()\nCall 77 of fibonacci()\nCall 78 of fibonacci()\nCall 79 of fibonacci()\nCall 80 of fibonacci()\nCall 81 of fibonacci()\nCall 82 of fibonacci()\nCall 83 of fibonacci()\nCall 84 of fibonacci()\nCall 85 of fibonacci()\nCall 86 of fibonacci()\nCall 87 of fibonacci()\nCall 88 of fibonacci()\nCall 89 of fibonacci()\nCall 90 of fibonacci()\nCall 91 of fibonacci()\nCall 92 of fibonacci()\nCall 93 of fibonacci()\nCall 94 of fibonacci()\nCall 95 of fibonacci()\nCall 96 of fibonacci()\nCall 97 of fibonacci()\nCall 98 of fibonacci()\nCall 99 of fibonacci()\nCall 100 of fibonacci()\nCall 101 of fibonacci()\nCall 102 of fibonacci()\nCall 103 of fibonacci()\nCall 104 of fibonacci()\nCall 105 of fibonacci()\nCall 106 of fibonacci()\nCall 107 of fibonacci()\nCall 108 of fibonacci()\nCall 109 of fibonacci()\nCall 110 of fibonacci()\nCall 111 of fibonacci()\nCall 112 of fibonacci()\nCall 113 of fibonacci()\nCall 114 of fibonacci()\nCall 115 of fibonacci()\nCall 116 of fibonacci()\nCall 117 of fibonacci()\nCall 118 of fibonacci()\nCall 119 of fibonacci()\nCall 120 of fibonacci()\nCall 121 of fibonacci()\nCall 122 of fibonacci()\nCall 123 of fibonacci()\nCall 124 of fibonacci()\nCall 125 of fibonacci()\nCall 126 of fibonacci()\nCall 127 of fibonacci()\nCall 128 of fibonacci()\nCall 129 of fibonacci()\nCall 130 of fibonacci()\nCall 131 of fibonacci()\nCall 132 of fibonacci()\nCall 133 of fibonacci()\nCall 134 of fibonacci()\nCall 135 of fibonacci()\nCall 136 of fibonacci()\nCall 137 of fibonacci()\nCall 138 of fibonacci()\nCall 139 of fibonacci()\nCall 140 of fibonacci()\nCall 141 of fibonacci()\nCall 142 of fibonacci()\nCall 143 of fibonacci()\nCall 144 of fibonacci()\nCall 145 of fibonacci()\nCall 146 of fibonacci()\nCall 147 of fibonacci()\nCall 148 of fibonacci()\nCall 149 of fibonacci()\nCall 150 of fibonacci()\nCall 151 of fibonacci()\nCall 152 of fibonacci()\nCall 153 of fibonacci()\nCall 154 of fibonacci()\nCall 155 of fibonacci()\nCall 156 of fibonacci()\nCall 157 of fibonacci()\nCall 158 of fibonacci()\nCall 159 of fibonacci()\nCall 160 of fibonacci()\nCall 161 of fibonacci()\nCall 162 of fibonacci()\nCall 163 of fibonacci()\nCall 164 of fibonacci()\nCall 165 of fibonacci()\nCall 166 of fibonacci()\nCall 167 of fibonacci()\nCall 168 of fibonacci()\nCall 169 of fibonacci()\nCall 170 of fibonacci()\nCall 171 of fibonacci()\nCall 172 of fibonacci()\nCall 173 of fibonacci()\nCall 174 of fibonacci()\nCall 175 of fibonacci()\nCall 176 of fibonacci()\nCall 177 of fibonacci()\n\n\n55\n\n\n\ndef cache(func):\n    \"\"\"Keep a cache of previous function calls\"\"\"\n    @functools.wraps(func)\n    def wrapper_cache(*args, **kwargs):\n        cache_key = args + tuple(kwargs.items())\n        if cache_key not in wrapper_cache.cache:\n            wrapper_cache.cache[cache_key] = func(*args, **kwargs)\n        return wrapper_cache.cache[cache_key]\n    wrapper_cache.cache = {}\n    return wrapper_cache\n\n@cache\n@count_calls\ndef fibonacci(num):\n    if num &lt; 2:\n        return num\n    return fibonacci(num - 1) + fibonacci(num - 2)\n\nfibonacci(10)\n\nCall 1 of fibonacci()\nCall 2 of fibonacci()\nCall 3 of fibonacci()\nCall 4 of fibonacci()\nCall 5 of fibonacci()\nCall 6 of fibonacci()\nCall 7 of fibonacci()\nCall 8 of fibonacci()\nCall 9 of fibonacci()\nCall 10 of fibonacci()\nCall 11 of fibonacci()\n\n\n55\n\n\n\nfibonacci(8)\n\n21\n\n\n\n@functools.lru_cache(maxsize=4)\ndef fibonacci(num: int):\n    if num &lt; 2:\n        value = num\n    else:\n        value = fibonacci(num - 1) + fibonacci(num - 2)\n    print(f\"Calculated fibonacci({num}) = {value}\")\n    return value\n\n\nfibonacci(10)\n\nCalculated fibonacci(1) = 1\nCalculated fibonacci(0) = 0\nCalculated fibonacci(2) = 1\nCalculated fibonacci(3) = 2\nCalculated fibonacci(4) = 3\nCalculated fibonacci(5) = 5\nCalculated fibonacci(6) = 8\nCalculated fibonacci(7) = 13\nCalculated fibonacci(8) = 21\nCalculated fibonacci(9) = 34\nCalculated fibonacci(10) = 55\n\n\n55\n\n\n\nfibonacci(8)\n\n21\n\n\n\nfibonacci(5)\n\nCalculated fibonacci(1) = 1\nCalculated fibonacci(0) = 0\nCalculated fibonacci(2) = 1\nCalculated fibonacci(3) = 2\nCalculated fibonacci(4) = 3\nCalculated fibonacci(5) = 5\n\n\n5\n\n\n\nfibonacci(8)\n\nCalculated fibonacci(6) = 8\nCalculated fibonacci(7) = 13\nCalculated fibonacci(8) = 21\n\n\n21\n\n\n\nfibonacci(5)\n\n5\n\n\n\nfibonacci.cache_info()\n\nCacheInfo(hits=17, misses=20, maxsize=4, currsize=4)\n\n\n\n\n\n\ndef set_unit(unit):\n    def set_unit_wrapper(func):\n        func.unit = unit\n        return func\n    return set_unit_wrapper\n\n\n \n\n\n\n\n\n\nclass Label:\n    def __init__(self, text, font):\n        self._text = text\n        self._font = font\n\n    def get_text(self):\n        return self._text\n\n    def set_text(self, value):\n        self._text = value\n\n    def get_font(self):\n        return self._font\n\n    def set_font(self, value):\n        self._font = value\n\n\nlabel = Label(\"Fruits\", \"JetBrains Mono NL\")\n\n\nlabel._text = 'hello'\n\n\nlabel._text\n\n'hello'\n\n\n\n\n\nThese notes are derived from this Real Python article. Attributes can be manipulated in two ways: - directly - via getter and setter methods (requires non-public API)\nRather than define our own getter/setter methods manually, we can use the property decorator.\nThe following example sets name and birth_date as properties, and assigns them setter methods\n\nfrom datetime import date\n\nclass Employee:\n    def __init__(self, name, birth_date):\n        self.name = name\n        self.birth_date = birth_date\n\n    @property\n    def name(self):\n        return self._name\n\n    @name.setter\n    def name(self, value):\n        self._name = value.upper()\n\n    @property\n    def birth_date(self):\n        return self._birth_date\n\n    @birth_date.setter\n    def birth_date(self, value):\n        self._birth_date = date.fromisoformat(value)\n\n\n\n\nWe can add to the previous example a new attribute, start_date and associated getter/setter methods:\n\nfrom datetime import date\n\nclass Employee:\n    def __init__(self, name, birth_date, start_date):\n        self.name = name\n        self.birth_date = birth_date\n        self.start_date = start_date\n\n    @property\n    def name(self):\n        return self._name\n\n    @name.setter\n    def name(self, value):\n        self._name = value.upper()\n\n    @property\n    def birth_date(self):\n        return self._birth_date\n\n    @birth_date.setter\n    def birth_date(self, value):\n        self._birth_date = date.fromisoformat(value)\n\n    @property\n    def start_date(self):\n        return self._start_date\n\n    @start_date.setter\n    def start_date(self, value):\n        self._start_date = date.fromisoformat(value)\n\nThis seems repetitive so we can refactor the code and set up a Date() object since our attributes are setup the same way:\n\nfrom datetime import date\n\nclass Date:\n    def __set_name__(self, owner, name):\n        self._name = name\n\n    def __get__(self, instance, owner):\n        return instance.__dict__[self._name]\n\n    def __set__(self, instance, value):\n        instance.__dict__[self._name] = date.fromisoformat(value)\n\nclass Employee:\n    birth_date = Date()\n    start_date = Date()\n\n    def __init__(self, name, birth_date, start_date):\n        self.name = name\n        self.birth_date = birth_date\n        self.start_date = start_date\n\n    @property\n    def name(self):\n        return self._name\n\n    @name.setter\n    def name(self, value):\n        self._name = value.upper()"
  },
  {
    "objectID": "notes/Decorators.html#simple-decorators",
    "href": "notes/Decorators.html#simple-decorators",
    "title": "Decorator Notes",
    "section": "",
    "text": "def parent(num):\n    def first_child():\n        return \"Hi, I am Emma\"\n\n    def second_child():\n        return \"Call me Liam\"\n\n    if num == 1:\n        return first_child\n    else:\n        return second_child\n\n\nparent(2)\n\n&lt;function __main__.parent.&lt;locals&gt;.second_child()&gt;\n\n\n\n\n\n\ndef my_decorator(func):\n    def wrapper():\n        print(\"Something is happening before the function is called.\")\n        func()\n        print(\"Something is happening after the function is called.\")\n    return wrapper\n\ndef say_whee():\n    print(\"Whee!\")\n\nsay_whee = my_decorator(say_whee)\n\n\nsay_whee()\n\nSomething is happening before the function is called.\nWhee!\nSomething is happening after the function is called.\n\n\nNow with syntactic sugar:\n\ndef my_decorator(func):\n    def wrapper():\n        print(\"Something is happening before the function is called.\")\n        func()\n        print(\"Something is happening after the function is called.\")\n    return wrapper\n\n@my_decorator\ndef say_whee():\n    print(\"Whee!\")\n\n\nsay_whee()\n\nSomething is happening before the function is called.\nWhee!\nSomething is happening after the function is called.\n\n\n\n\n\nDecorating simple functions is easy enough, but what happens when decorated function takes in arguments? The second function greet() is incompatible with the decorator, do_twice since the inner function does not take any arguments.\n\ndef do_twice(func):\n    def wrapper_do_twice():\n        func()\n        func()\n    return wrapper_do_twice\n\n@do_twice\ndef say_whee():\n    print('Whee!')\n\n@do_twice\ndef greet(name: str):\n    print(f'Hello {name}!')\n\nsay_whee()\n\nWhee!\nWhee!\n\n\n\ngreet('Jonathan')\n\nTypeError: do_twice.&lt;locals&gt;.wrapper_do_twice() takes 0 positional arguments but 1 was given\n\n\nInstead, we can add arguments to our decorator to handle any potential parameters using *args, **kwargs**:\n\ndef do_twice(func):\n    def wrapper_do_twice(*args, **kwargs):\n        func(*args, **kwargs)\n        func(*args, **kwargs)\n    return wrapper_do_twice\n\n@do_twice\ndef say_whee():\n    print(\"Whee!\")\n    \n@do_twice\ndef greet(name):\n    print(f\"Hello {name}\")\n\n\nsay_whee()\n\nWhee!\nWhee!\n\n\n\ngreet('Jonathan')\n\nHello Jonathan\nHello Jonathan\n\n\n\n\n\nFor functions that return values, we must make sure the decorator’s inner function also returns a value. If not, like in the case of the do_twice decorator’s current form, the decorated function’s return value will get lost:\n\n@do_twice\ndef return_greeting(name: str):\n    print('Creating greeting')\n    return f'Hi {name}!'\n\nhi_jonathan = return_greeting('Jonathan')\n\nCreating greeting\nCreating greeting\n\n\n\nprint(hi_jonathan)\n\nNone\n\n\n\ndef do_twice(func):\n    def do_twice_wrapper(*args, **kwargs):\n       func(*args, **kwargs)\n       return func(*args, **kwargs)\n    return do_twice_wrapper\n\n@do_twice\ndef return_greeting(name: str):\n    print('Creating greeting')\n    return f'Hi {name}!'\n\nhi_jonathan = return_greeting('Jonathan')\n\nCreating greeting\nCreating greeting\n\n\n\nprint(hi_jonathan)\n\nHi Jonathan!\n\n\n\n\n\nPython functions have built-in documentation. Decorated ones simply show themselves as the inner function of the wrapper. To get the original, useful documenation of a decorated function, the decorator must have @functools.wraps(func) above its inner function definition:\n\nprint\n\n&lt;function print(*args, sep=' ', end='\\n', file=None, flush=False)&gt;\n\n\n\nprint.__name__\n\n'print'\n\n\n\nhelp(print)\n\nHelp on built-in function print in module builtins:\n\nprint(...)\n    print(value, ..., sep=' ', end='\\n', file=sys.stdout, flush=False)\n    \n    Prints the values to a stream, or to sys.stdout by default.\n    Optional keyword arguments:\n    file:  a file-like object (stream); defaults to the current sys.stdout.\n    sep:   string inserted between values, default a space.\n    end:   string appended after the last value, default a newline.\n    flush: whether to forcibly flush the stream.\n\n\n\n\nsay_whee\n\n&lt;function __main__.do_twice.&lt;locals&gt;.wrapper_do_twice(*args, **kwargs)&gt;\n\n\n\nsay_whee.__name__\n\n'wrapper_do_twice'\n\n\n\nhelp(say_whee)\n\nHelp on function wrapper_do_twice in module __main__:\n\nwrapper_do_twice(*args, **kwargs)\n\n\n\n\nfrom functools import wraps\n\ndef do_twice(func):\n    @wraps(func)\n    def wrapper_do_twice(*args, **kwargs):\n        func(*args, **kwargs)\n        return func(*args, **kwargs)\n    return wrapper_do_twice\n\n@do_twice\ndef say_whee():\n    print(\"Whee!\")\n\n\nsay_whee\n\n&lt;function __main__.say_whee()&gt;\n\n\n\nsay_whee.__name__\n\n'say_whee'\n\n\n\nhelp(say_whee)\n\nHelp on function say_whee in module __main__:\n\nsay_whee()\n\n\n\n\nhasattr()"
  },
  {
    "objectID": "notes/Decorators.html#decorator-real-world-examples",
    "href": "notes/Decorators.html#decorator-real-world-examples",
    "title": "Decorator Notes",
    "section": "",
    "text": "The pattern for many decorators will take this form:\n\nimport functools\n\ndef decorator(func):\n    @functools.wrap(func)\n    def wrapper_decorator(*args, **kwargs):\n        # Do something before\n        value = func(*args, **kwargs)\n        # Do something after\n        return value\n    return wrapper_decorator\n\n\n\n\nimport functools\nimport time\n\ndef timer(func):\n    \"\"\"Print the runtime of the decorated function\"\"\"\n    @functools.wraps(func)\n    def wrapper_timer(*args, **kwargs):\n        start = time.perf_counter()\n        value = func(*args, **kwargs)\n        end = time.perf_counter()\n        runtime = end - start\n        print(f\"Finished {func.__name__}() in {runtime:.4f} secs\")\n        return value\n    return wrapper_timer\n\n\n@timer\ndef waste_some_time(num_times: int):\n    for _ in range(num_times):\n        sum([number**2 for number in range(10_000)])\n\nwaste_some_time(1)\n        \n\nFinished waste_some_time() in 0.0021 secs\n\n\n\nwaste_some_time(999)\n\nFinished waste_some_time() in 1.1273 secs\n\n\n\n\n\nWe can take advantage of the namespace access in the wrapped function to display parameters and return values for debugging:\n\nimport functools\n\ndef debug(func):\n    \"\"\"Print the function signature and return value\"\"\"\n    @functools.wraps(func)\n    def wrapper_debug(*args, **kwargs):\n        args_repr = [repr(a) for a in args]\n        kwargs_repr = [f\"{k}={repr(v)}\" for k, v in kwargs.items()]\n        signature = \", \".join(args_repr + kwargs_repr)\n        print(f\"Calling {func.__name__}({signature})\")\n        value = func(*args, **kwargs)\n        print(f\"{func.__name__}() returned {repr(value)}\")\n        return value\n    return wrapper_debug\n\n@debug\ndef make_greeting(name: str, age: int=None):\n    if age is None:\n        return f\"Howdy {name}!\"\n    else:\n        return f\"Whoa {name}! {age} already?! You're growing up!\"\n\nmake_greeting(\"Benjamin\")\n\nCalling make_greeting('Benjamin')\nmake_greeting() returned 'Howdy Benjamin!'\n\n\n'Howdy Benjamin!'\n\n\n\nmake_greeting(\"Juan\", age=114)\n\nCalling make_greeting('Juan', age=114)\nmake_greeting() returned \"Whoa Juan! 114 already?! You're growing up!\"\n\n\n\"Whoa Juan! 114 already?! You're growing up!\"\n\n\nThis is more impactful when used on small inconvenience functions not called directly:\n\nimport math\n\nmath.factorial = debug(math.factorial)\n\ndef approximate_e(terms=18):\n    return sum(1 / math.factorial(n) for n in range(terms))\n\napproximate_e(terms=5)\n\nCalling factorial(0)\nfactorial() returned 1\nCalling factorial(1)\nfactorial() returned 1\nCalling factorial(2)\nfactorial() returned 2\nCalling factorial(3)\nfactorial() returned 6\nCalling factorial(4)\nfactorial() returned 24\n\n\n2.7083333333333335\n\n\n\n\n\nSometimes, it’s useful to slow down code for the purposes of rate limiting, which can be achieved by a decorator:\n\nimport functools\nimport time\n\ndef slow_down(func):\n    @functools.wraps(func)\n    def wrapper_slow_down(*args, **kwargs):\n        time.sleep(1)\n        return func(*args, **kwargs)\n    return wrapper_slow_down\n\n@slow_down\ndef countdown(from_number):\n    if from_number &lt; 1:\n        print(\"Liftoff!\")\n    else:\n        print(from_number)\n        countdown(from_number - 1)\n\ncountdown(3)\n\n3\n2\n1\nLiftoff!\n\n\n\n\n\nHere we use a decorator to leave functions unmodified, but record them in a dictionary. This is a list separate from globals():\n\nPLUGINS = dict()\n\ndef register(func):\n    \"\"\"Register a funciton as a plug-in\"\"\"\n    PLUGINS[func.__name__] = func\n    return func\n\n@register\ndef say_hello(name: str):\n    return f\"Hello {name}\"\n\n@register\ndef be_awesome(name: str):\n    return f\"Yo {name}, together, we're awesome!\"\n\nPLUGINS\n\n{'say_hello': &lt;function __main__.say_hello(name: str)&gt;,\n 'be_awesome': &lt;function __main__.be_awesome(name: str)&gt;}\n\n\n\nbe_awesome('Jonathan')\n\n\"Yo Jonathan, together, we're awesome!\"\n\n\nHere we use the string representation of a Python object using the !r flag.\n\nimport random\n\ndef randomly_greet(name):\n    greeter, greeter_func = random.choice(list(PLUGINS.items()))\n    print(f\"Using {greeter!r}\")\n    return greeter_func(name)\n\nrandomly_greet(\"Alice\")\n\nUsing 'say_hello'\n\n\n'Hello Alice'\n\n\n\n\n\n\nimport functools\nfrom flask import Flask, g, request, redirect, url_for\n\napp = Flask(__name__)\n\ndef login_required(func):\n    \"\"\"Make sure user is logged in before proceeding\"\"\"\n    @functools.wraps(func)\n    def wrapper_login_required(*args, **kwargs):\n        if g.user is None:\n            return redirect(url_for(\"login\", next=request.url))\n        return func(*args, **kwargs)\n    return wrapper_login_required\n\n@app.route(\"/secret\")\n@login_required\ndef secret():\n    ..."
  },
  {
    "objectID": "notes/Decorators.html#fancy-decorators",
    "href": "notes/Decorators.html#fancy-decorators",
    "title": "Decorator Notes",
    "section": "",
    "text": "This can be achieved on the individual methods or the entire class itself:\n\nclass TimeWaster:\n    @debug\n    def __init__(self, max_num: int):\n        self.max_num = max_num\n\n    @timer\n    def waste_time(self, num_times: int):\n        for _ in range(num_times):\n            sum([number**2 for number in range(self.max_num)])\n\ntw = TimeWaster(1000)\n\nCalling __init__(&lt;__main__.TimeWaster object at 0x000001D0871DACF0&gt;, 1000)\n__init__() returned None\n\n\n\ntw.waste_time(999)\n\nFinished waste_time() in 0.1318 secs\n\n\n\nfrom dataclasses import dataclass\n\n@dataclass\nclass PlayingCard:\n    rank: str\n    suit: str\n\nDecorating a class does not decorate its methods:\n\n@timer\nclass TimeWaster:\n    def __init__(self, max_num):\n        self.max_num = max_num\n\n    def waste_time(self, num_times):\n        for _ in range(num_times):\n            sum([i**2 for i in range(self.max_num)])\n\ntw = TimeWaster(1000)\n\nFinished TimeWaster() in 0.0000 secs\n\n\n\ntw.waste_time(999)\n\n\n\n\n\n@debug\n@do_twice\ndef greet(name):\n    print(f'Hello {name}!')\n\ngreet('Yadi')\n\nCalling wrapper_do_twice('Yadi')\nHello Yadi!\nHello Yadi!\nwrapper_do_twice() returned None\n\n\n\n@do_twice\n@debug\ndef greet(name):\n    print(f'Hello {name}!')\n\ngreet('Yadi')\n\nCalling greet('Yadi')\nHello Yadi!\ngreet() returned None\nCalling greet('Yadi')\nHello Yadi!\ngreet() returned None\n\n\n\n\n\nWe know how to define a decorator on general functions and how to add functionality to the decorated function. To add parameters to a decorator, we apply the same principle and decorate the decorator.\nThis adds another layer. The syntactic sugar saves more typing since another layer of nesting requires another function reassignment, from outer to inner.\n\nimport functools\n\ndef repeat(num_times: int):\n    def decorator_repeat(func):\n        functools.wraps(func)\n        def wrapper_decorator_repeat(*args, **kwargs):\n            for _ in range(num_times):\n                value = func(*args, **kwargs)\n            return value\n        return wrapper_decorator_repeat\n    return decorator_repeat\n\ndef greet(name: str):\n    print(f'Hello {name}!')\n\nrepeat4 = repeat(4)\ngreet = repeat4(greet)\ngreet('Jonathan')\n\nHello Jonathan!\nHello Jonathan!\nHello Jonathan!\nHello Jonathan!\n\n\n\n@repeat(4)\ndef greet(name: str):\n    print(f'Hello {name}!')\n\ngreet('Jonathan')\n\nHello Jonathan!\nHello Jonathan!\nHello Jonathan!\nHello Jonathan!\n\n\n\n\n\nTo modify our decorator to have optional arguments, we must modify the outermost layer in two ways: - to take in arguments with the * syntax - control flow of optional parameter values\nThis approach wraps the decorator with an outer layer that takes optional arguments, but also allows for a “skip” connection to layer below in case parameters are not specified. In this case, the synactic sugar will only take in the first argument, which is the function to be decorated.\n\ndef name(_func=None, *, key1=value1, key2=value2, ...):\n    def decorator_name(func):\n        ... # Create and return a wrapper function\n\n    if _func is None:\n        return decorator_name\n    else:\n        return decorator_name(_func)\n\n\nimport functools\n\ndef repeat(_func=None, *, num_times=2):\n    def decorator_repeat(func):\n        @functools.wraps(func)\n        def wrapper_repeat(*args, **kwargs):\n            for _ in range(num_times):\n                value = func(*args, **kwargs)\n            return value\n        return wrapper_repeat\n\n    if _func is None:\n        return decorator_repeat\n    else:\n        return decorator_repeat(_func)\n\n@repeat\ndef say_whee():\n    print('Whee!')\n\n@repeat(num_times=3)\ndef greet(name):\n    print(f'Hello {name}!')\n\nsay_whee()\n\nWhee!\nWhee!\n\n\n\ngreet('Jonathan')\n\nHello Jonathan!\nHello Jonathan!\nHello Jonathan!\n\n\n\n\n\nHere we an use function attributes to track a state variable such as a counter\n\nimport functools\n\ndef count_calls(func):\n    @functools.wraps(func)\n    def wrapper_count_calls(*args, **kwargs):\n        wrapper_count_calls.num_calls += 1\n        print(f'Call {wrapper_count_calls.num_calls} of {func.__name__}()')\n        return func(*args, **kwargs)\n    wrapper_count_calls.num_calls = 0\n    return wrapper_count_calls\n    \n\n\n\n\nIf we step back and think about how a closure decorator works, we have the following steps: - if there are decorator parameters, first initialize an “instance” with them set - next, rename the function to be modified with its wrapped self: func = decorator(func) - finally, call the function as normal with its usual inputs\nThese steps are similar to how a class instance works with a __init__() and __call__() method. The analogous steps are: - define an __init__() that sets the parameters of the decorator, acting as the outermost function in a nested closure - define a __call__() method with the structure of a closure - use functools.wraps() for appropriate introspection - have the __call__() method take in the function as an argument - define an inner function matching the decorated function’s inputs along with additional logic - return the inner function to finalize the __call__() method - for simpler, unparameterized decorating classes, use functools.update_wrapper() instead of functools.wraps() in the __init__() method for proper introspection\n\nclass Counter:\n    def __init__(self, start=0):\n        self.count = start\n    def __call__(self):\n        self.count += 1\n        print(f\"Current count is {self.count}\")\n\ncounter = Counter()\n\ncounter()\n    \n\nCurrent count is 1\n\n\n\ncounter()\n\nCurrent count is 2\n\n\nNow, with the modifications to make it a simple decorator without parameters:\n\nimport functools\nclass Counter:\n    def __init__(self, func):\n        functools.update_wrapper(self, func)\n        self.func = func\n        self.num_calls = 0\n\n    def __call__(self, *args, **kwargs):\n        self.num_calls += 1\n        print(f\"Call {self.num_calls} of {self.func.__name__}()\")\n        return self.func(*args, **kwargs)\n\n@Counter\ndef say_whee():\n    print('Whee!')\n\n\nsay_whee()\n\nCall 1 of say_whee()\nWhee!\n\n\n\nsay_whee()\n\nCall 2 of say_whee()\nWhee!\n\n\n\nsay_whee.num_calls\n\n2\n\n\nFinally, we have this example that was generated by Gemini where the __init__() sets parameters and the __call__() is implemented as a closure with general paramters:\n\nimport functools\nfrom datetime import datetime\n\nclass FunctionLogger:\n    \"\"\"\n    A class-based decorator to log function execution details.\n    It takes parameters to customize the log output.\n    \"\"\"\n    def __init__(self, log_level=\"INFO\", format_str=\"{timestamp} [{level}] - Entering '{func_name}'\"):\n        \"\"\"\n        The __init__ method is called when the decorator is instantiated.\n        It captures the decorator's arguments.\n        Example: @FunctionLogger(log_level=\"DEBUG\")\n        \"\"\"\n        print(f\"Decorator `FunctionLogger` is being initialized with level: {log_level}\")\n        self.log_level = log_level\n        self.format_str = format_str\n\n    def __call__(self, func):\n        \"\"\"\n        The __call__ method is invoked with the decorated function as its argument.\n        It must return a replacement (wrapper) function.\n        \"\"\"\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            \"\"\"\n            This is the wrapper function that gets executed instead of the original.\n            It contains the decorator's core logic.\n            \"\"\"\n            timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n            \n            # 1. Logic before the original function is called\n            log_message = self.format_str.format(\n                timestamp=timestamp,\n                level=self.log_level,\n                func_name=func.__name__\n            )\n            print(log_message)\n            \n            # 2. Call the original function\n            result = func(*args, **kwargs)\n            \n            # 3. Logic after the original function is called\n            print(f\"{timestamp} [{self.log_level}] - Exiting '{func.__name__}'\")\n            \n            return result\n            \n        # The __call__ method returns the wrapper function\n        return wrapper\n\n# --- Example Usage ---\n\n@FunctionLogger(log_level=\"DEBUG\", format_str=\"{timestamp} - {level} - Calling `{func_name}`...\")\ndef add(x, y):\n    \"\"\"This function adds two numbers together.\"\"\"\n    print(f\"  &gt; Inside add({x}, {y})\")\n    return x + y\n\n@FunctionLogger(log_level=\"WARNING\")\ndef greet(name):\n    \"\"\"This function greets a person.\"\"\"\n    print(f\"  &gt; Hello, {name}!\")\n\ngreet('Jonathan')\n\nDecorator `FunctionLogger` is being initialized with level: DEBUG\nDecorator `FunctionLogger` is being initialized with level: WARNING\n2025-09-25 12:44:26 [WARNING] - Entering 'greet'\n  &gt; Hello, Jonathan!\n2025-09-25 12:44:26 [WARNING] - Exiting 'greet'\n\n\n\ngreet('Jonathan')\n\n2025-09-25 12:45:14 [WARNING] - Entering 'greet'\n  &gt; Hello, Jonathan!\n2025-09-25 12:45:14 [WARNING] - Exiting 'greet'"
  },
  {
    "objectID": "notes/Decorators.html#more-real-world-examples",
    "href": "notes/Decorators.html#more-real-world-examples",
    "title": "Decorator Notes",
    "section": "",
    "text": "import functools\nimport time\n\ndef slow_down(_func=None, *, rate=1):\n    \"\"\"Sleep given amount of seconds before calling the function\"\"\"\n    def decorator_slow_down(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            time.sleep(rate)\n            return func(*args, **kwargs)\n        return wrapper\n    if _func is None:\n        return decorator_slow_down\n    else:\n        return decorator_slow_down(_func)\n        \n\n@slow_down(rate=2)\ndef countdown(from_number: int):\n    if from_number &lt; 1:\n        print('Liftoff!')\n    else:\n        print(from_number)\n        countdown(from_number - 1)\n\ncountdown(3)\n\n3\n2\n1\nLiftoff!\n\n\n\n\n\nPython has a number of singletons, classes with only one instance: - True, False, None\nThis allows for use of is to check for equality, or rather, identity. We can use a decorator to store the first instance of a class as a function attribute, enforcing the singleton design pattern:\n\nimport functools\n\ndef singleton(cls):\n    \"\"\"Make a class a Singleton class\"\"\"\n    @functools.wraps(cls)\n    def wrapper_singleton(*args, **kwargs):\n        if wrapper_singleton.instance is None:\n            wrapper_singleton.instance = cls(*args, **kwargs)\n        return wrapper_singleton.instance\n    wrapper_singleton.instance = None\n    return wrapper_singleton\n\n@singleton\nclass TheOne:\n    pass\n\nfirst_one = TheOne()\n\nanother_one = TheOne()\nid(first_one)\n\n1995139107200\n\n\n\nid(another_one)\n\n1995139107200\n\n\n\nfirst_one is another_one\n\nTrue\n\n\n\n\n\n\nimport functools\n\ndef count_calls(func):\n    @functools.wraps(func)\n    def wrapper_count_calls(*args, **kwargs):\n        wrapper_count_calls.num_calls += 1\n        print(f'Call {wrapper_count_calls.num_calls} of {func.__name__}()')\n        return func(*args, **kwargs)\n    wrapper_count_calls.num_calls = 0\n    return wrapper_count_calls\n \n@count_calls\ndef fibonacci(num):\n    if num &lt; 2:\n        return num\n    return fibonacci(num - 2) + fibonacci(num - 1)\n\nfibonacci(10)\n\nCall 1 of fibonacci()\nCall 2 of fibonacci()\nCall 3 of fibonacci()\nCall 4 of fibonacci()\nCall 5 of fibonacci()\nCall 6 of fibonacci()\nCall 7 of fibonacci()\nCall 8 of fibonacci()\nCall 9 of fibonacci()\nCall 10 of fibonacci()\nCall 11 of fibonacci()\nCall 12 of fibonacci()\nCall 13 of fibonacci()\nCall 14 of fibonacci()\nCall 15 of fibonacci()\nCall 16 of fibonacci()\nCall 17 of fibonacci()\nCall 18 of fibonacci()\nCall 19 of fibonacci()\nCall 20 of fibonacci()\nCall 21 of fibonacci()\nCall 22 of fibonacci()\nCall 23 of fibonacci()\nCall 24 of fibonacci()\nCall 25 of fibonacci()\nCall 26 of fibonacci()\nCall 27 of fibonacci()\nCall 28 of fibonacci()\nCall 29 of fibonacci()\nCall 30 of fibonacci()\nCall 31 of fibonacci()\nCall 32 of fibonacci()\nCall 33 of fibonacci()\nCall 34 of fibonacci()\nCall 35 of fibonacci()\nCall 36 of fibonacci()\nCall 37 of fibonacci()\nCall 38 of fibonacci()\nCall 39 of fibonacci()\nCall 40 of fibonacci()\nCall 41 of fibonacci()\nCall 42 of fibonacci()\nCall 43 of fibonacci()\nCall 44 of fibonacci()\nCall 45 of fibonacci()\nCall 46 of fibonacci()\nCall 47 of fibonacci()\nCall 48 of fibonacci()\nCall 49 of fibonacci()\nCall 50 of fibonacci()\nCall 51 of fibonacci()\nCall 52 of fibonacci()\nCall 53 of fibonacci()\nCall 54 of fibonacci()\nCall 55 of fibonacci()\nCall 56 of fibonacci()\nCall 57 of fibonacci()\nCall 58 of fibonacci()\nCall 59 of fibonacci()\nCall 60 of fibonacci()\nCall 61 of fibonacci()\nCall 62 of fibonacci()\nCall 63 of fibonacci()\nCall 64 of fibonacci()\nCall 65 of fibonacci()\nCall 66 of fibonacci()\nCall 67 of fibonacci()\nCall 68 of fibonacci()\nCall 69 of fibonacci()\nCall 70 of fibonacci()\nCall 71 of fibonacci()\nCall 72 of fibonacci()\nCall 73 of fibonacci()\nCall 74 of fibonacci()\nCall 75 of fibonacci()\nCall 76 of fibonacci()\nCall 77 of fibonacci()\nCall 78 of fibonacci()\nCall 79 of fibonacci()\nCall 80 of fibonacci()\nCall 81 of fibonacci()\nCall 82 of fibonacci()\nCall 83 of fibonacci()\nCall 84 of fibonacci()\nCall 85 of fibonacci()\nCall 86 of fibonacci()\nCall 87 of fibonacci()\nCall 88 of fibonacci()\nCall 89 of fibonacci()\nCall 90 of fibonacci()\nCall 91 of fibonacci()\nCall 92 of fibonacci()\nCall 93 of fibonacci()\nCall 94 of fibonacci()\nCall 95 of fibonacci()\nCall 96 of fibonacci()\nCall 97 of fibonacci()\nCall 98 of fibonacci()\nCall 99 of fibonacci()\nCall 100 of fibonacci()\nCall 101 of fibonacci()\nCall 102 of fibonacci()\nCall 103 of fibonacci()\nCall 104 of fibonacci()\nCall 105 of fibonacci()\nCall 106 of fibonacci()\nCall 107 of fibonacci()\nCall 108 of fibonacci()\nCall 109 of fibonacci()\nCall 110 of fibonacci()\nCall 111 of fibonacci()\nCall 112 of fibonacci()\nCall 113 of fibonacci()\nCall 114 of fibonacci()\nCall 115 of fibonacci()\nCall 116 of fibonacci()\nCall 117 of fibonacci()\nCall 118 of fibonacci()\nCall 119 of fibonacci()\nCall 120 of fibonacci()\nCall 121 of fibonacci()\nCall 122 of fibonacci()\nCall 123 of fibonacci()\nCall 124 of fibonacci()\nCall 125 of fibonacci()\nCall 126 of fibonacci()\nCall 127 of fibonacci()\nCall 128 of fibonacci()\nCall 129 of fibonacci()\nCall 130 of fibonacci()\nCall 131 of fibonacci()\nCall 132 of fibonacci()\nCall 133 of fibonacci()\nCall 134 of fibonacci()\nCall 135 of fibonacci()\nCall 136 of fibonacci()\nCall 137 of fibonacci()\nCall 138 of fibonacci()\nCall 139 of fibonacci()\nCall 140 of fibonacci()\nCall 141 of fibonacci()\nCall 142 of fibonacci()\nCall 143 of fibonacci()\nCall 144 of fibonacci()\nCall 145 of fibonacci()\nCall 146 of fibonacci()\nCall 147 of fibonacci()\nCall 148 of fibonacci()\nCall 149 of fibonacci()\nCall 150 of fibonacci()\nCall 151 of fibonacci()\nCall 152 of fibonacci()\nCall 153 of fibonacci()\nCall 154 of fibonacci()\nCall 155 of fibonacci()\nCall 156 of fibonacci()\nCall 157 of fibonacci()\nCall 158 of fibonacci()\nCall 159 of fibonacci()\nCall 160 of fibonacci()\nCall 161 of fibonacci()\nCall 162 of fibonacci()\nCall 163 of fibonacci()\nCall 164 of fibonacci()\nCall 165 of fibonacci()\nCall 166 of fibonacci()\nCall 167 of fibonacci()\nCall 168 of fibonacci()\nCall 169 of fibonacci()\nCall 170 of fibonacci()\nCall 171 of fibonacci()\nCall 172 of fibonacci()\nCall 173 of fibonacci()\nCall 174 of fibonacci()\nCall 175 of fibonacci()\nCall 176 of fibonacci()\nCall 177 of fibonacci()\n\n\n55\n\n\n\ndef cache(func):\n    \"\"\"Keep a cache of previous function calls\"\"\"\n    @functools.wraps(func)\n    def wrapper_cache(*args, **kwargs):\n        cache_key = args + tuple(kwargs.items())\n        if cache_key not in wrapper_cache.cache:\n            wrapper_cache.cache[cache_key] = func(*args, **kwargs)\n        return wrapper_cache.cache[cache_key]\n    wrapper_cache.cache = {}\n    return wrapper_cache\n\n@cache\n@count_calls\ndef fibonacci(num):\n    if num &lt; 2:\n        return num\n    return fibonacci(num - 1) + fibonacci(num - 2)\n\nfibonacci(10)\n\nCall 1 of fibonacci()\nCall 2 of fibonacci()\nCall 3 of fibonacci()\nCall 4 of fibonacci()\nCall 5 of fibonacci()\nCall 6 of fibonacci()\nCall 7 of fibonacci()\nCall 8 of fibonacci()\nCall 9 of fibonacci()\nCall 10 of fibonacci()\nCall 11 of fibonacci()\n\n\n55\n\n\n\nfibonacci(8)\n\n21\n\n\n\n@functools.lru_cache(maxsize=4)\ndef fibonacci(num: int):\n    if num &lt; 2:\n        value = num\n    else:\n        value = fibonacci(num - 1) + fibonacci(num - 2)\n    print(f\"Calculated fibonacci({num}) = {value}\")\n    return value\n\n\nfibonacci(10)\n\nCalculated fibonacci(1) = 1\nCalculated fibonacci(0) = 0\nCalculated fibonacci(2) = 1\nCalculated fibonacci(3) = 2\nCalculated fibonacci(4) = 3\nCalculated fibonacci(5) = 5\nCalculated fibonacci(6) = 8\nCalculated fibonacci(7) = 13\nCalculated fibonacci(8) = 21\nCalculated fibonacci(9) = 34\nCalculated fibonacci(10) = 55\n\n\n55\n\n\n\nfibonacci(8)\n\n21\n\n\n\nfibonacci(5)\n\nCalculated fibonacci(1) = 1\nCalculated fibonacci(0) = 0\nCalculated fibonacci(2) = 1\nCalculated fibonacci(3) = 2\nCalculated fibonacci(4) = 3\nCalculated fibonacci(5) = 5\n\n\n5\n\n\n\nfibonacci(8)\n\nCalculated fibonacci(6) = 8\nCalculated fibonacci(7) = 13\nCalculated fibonacci(8) = 21\n\n\n21\n\n\n\nfibonacci(5)\n\n5\n\n\n\nfibonacci.cache_info()\n\nCacheInfo(hits=17, misses=20, maxsize=4, currsize=4)\n\n\n\n\n\n\ndef set_unit(unit):\n    def set_unit_wrapper(func):\n        func.unit = unit\n        return func\n    return set_unit_wrapper"
  },
  {
    "objectID": "notes/Decorators.html#getter-and-setters",
    "href": "notes/Decorators.html#getter-and-setters",
    "title": "Decorator Notes",
    "section": "",
    "text": "class Label:\n    def __init__(self, text, font):\n        self._text = text\n        self._font = font\n\n    def get_text(self):\n        return self._text\n\n    def set_text(self, value):\n        self._text = value\n\n    def get_font(self):\n        return self._font\n\n    def set_font(self, value):\n        self._font = value\n\n\nlabel = Label(\"Fruits\", \"JetBrains Mono NL\")\n\n\nlabel._text = 'hello'\n\n\nlabel._text\n\n'hello'"
  },
  {
    "objectID": "notes/Decorators.html#properties-introducing-functionality-to-attributes",
    "href": "notes/Decorators.html#properties-introducing-functionality-to-attributes",
    "title": "Decorator Notes",
    "section": "",
    "text": "These notes are derived from this Real Python article. Attributes can be manipulated in two ways: - directly - via getter and setter methods (requires non-public API)\nRather than define our own getter/setter methods manually, we can use the property decorator.\nThe following example sets name and birth_date as properties, and assigns them setter methods\n\nfrom datetime import date\n\nclass Employee:\n    def __init__(self, name, birth_date):\n        self.name = name\n        self.birth_date = birth_date\n\n    @property\n    def name(self):\n        return self._name\n\n    @name.setter\n    def name(self, value):\n        self._name = value.upper()\n\n    @property\n    def birth_date(self):\n        return self._birth_date\n\n    @birth_date.setter\n    def birth_date(self, value):\n        self._birth_date = date.fromisoformat(value)"
  },
  {
    "objectID": "notes/Decorators.html#descriptors-attributes-with-attached-behaviors",
    "href": "notes/Decorators.html#descriptors-attributes-with-attached-behaviors",
    "title": "Decorator Notes",
    "section": "",
    "text": "We can add to the previous example a new attribute, start_date and associated getter/setter methods:\n\nfrom datetime import date\n\nclass Employee:\n    def __init__(self, name, birth_date, start_date):\n        self.name = name\n        self.birth_date = birth_date\n        self.start_date = start_date\n\n    @property\n    def name(self):\n        return self._name\n\n    @name.setter\n    def name(self, value):\n        self._name = value.upper()\n\n    @property\n    def birth_date(self):\n        return self._birth_date\n\n    @birth_date.setter\n    def birth_date(self, value):\n        self._birth_date = date.fromisoformat(value)\n\n    @property\n    def start_date(self):\n        return self._start_date\n\n    @start_date.setter\n    def start_date(self, value):\n        self._start_date = date.fromisoformat(value)\n\nThis seems repetitive so we can refactor the code and set up a Date() object since our attributes are setup the same way:\n\nfrom datetime import date\n\nclass Date:\n    def __set_name__(self, owner, name):\n        self._name = name\n\n    def __get__(self, instance, owner):\n        return instance.__dict__[self._name]\n\n    def __set__(self, instance, value):\n        instance.__dict__[self._name] = date.fromisoformat(value)\n\nclass Employee:\n    birth_date = Date()\n    start_date = Date()\n\n    def __init__(self, name, birth_date, start_date):\n        self.name = name\n        self.birth_date = birth_date\n        self.start_date = start_date\n\n    @property\n    def name(self):\n        return self._name\n\n    @name.setter\n    def name(self, value):\n        self._name = value.upper()"
  },
  {
    "objectID": "notes/ClusteringNotes.html",
    "href": "notes/ClusteringNotes.html",
    "title": "Clustering Notes",
    "section": "",
    "text": "The following notes follow along this blog post. Clustering methods all use a central idea of a distance or metric defined by a function \\(d(i,j)\\). It is often the Euclidean distance.\n\n\n\n\nConsider a set of points in some space to be clustered. Let \\(X = \\{x_1, ..., x_n\\}\\), a set of \\(n\\) objects, \\(\\epsilon\\) be a parameter specifying a radius of a neighborhood with respect to some point. Points in DBSCAN fall into 3 categories: - a point \\(p\\) is a core point if there are at least a minimum number of points within \\(\\epsilon\\) of it, including itself - a point \\(q\\) is directly reachable from point \\(p\\) if \\(q\\) is within distance \\(\\epsilon\\), but only if \\(p\\) is a core point - a point \\(q\\) is reachable form \\(p\\) if there is a path \\(p_1, ..., p_n\\) with \\(p_1 = p\\) and \\(p_n = q\\) where each \\(p_{i+1}\\) is directly reachable from \\(p_i\\). This implies that the initial point and all points on the path must be core points, with the potential exception of \\(q\\) - all points not reachable from any other point are outliers or noise points\nReachability is not a symmetric relation, so another concept of connection is required for the extent of clusters: - two points are density-connected if there is a point \\(o\\) such that both \\(p\\) and \\(q\\) are reachable from \\(o\\)\nClusters satisfy two properties: - all points within a cluster are mutually density-connected - if a point is density-reachable from some point of the cluster, it is part of the cluster as well\n\n\n\n\nChoose minimum points \\(m_{pts}\\) threshold and \\(\\epsilon\\)\nFind the points in the \\(\\epsilon\\) neighborhood of every point, and identify the core points with more than \\(m_{pts}\\) neighbors\nFind the connected components of core points on the neighbor graph, ignoring all non-core points\nAssign each non-core point to a nearby cluster if the cluster is an \\(\\epsilon\\) neighbor, otherwise assign it to noise\n\n\n\n\nFor any possible clustering \\(C = \\{C_1, ..., C_t\\}\\) out of the set of all clusterings \\(\\mathcal{C}\\), DBSCAN minimizes the number of clusters such that every pair of points, \\(p, q\\) in a cluster is density-reachable.\n\\[\\underset{C \\subset\\mathcal{C}, d_{db}(p, q) \\le \\epsilon \\forall p, q \\in C_i \\forall C_i \\in C}\\min |C|\\]\n\n\n\n\nThis is a hierarchical extension of DBSCAN. For a chosen \\(m_{pts}\\)\n\n\n\n\napplies to object \\(x_p \\in X\\), with respect to \\(m_{pts}\\), is defined as the distance from \\(x_p\\) to its \\(m_{pts}\\)-nearest neighbor, including itself\n\n\n\nAny object \\(x_p \\in X\\) is where the following holds: \\[d_{core}(x_p) \\le \\epsilon\\]\n\n\n\nis defined as \\[d_{mreach}(x_p, x_q) \\equiv\\max\\{d_{core}(x_p), d_{core}(x_q), d(x_p, x_q) \\}\\]\n\n\n\nThe complete graph in which the objects of \\(X\\) are vertices and the weight of each edge is the mutual reachability distance (wrt \\(m_{pts}\\)) between the respective pair of objects.\n\n\n\nIf we take a subgraph \\(G_{m_{pts}, \\epsilon}\\) obtained by removing all edges from \\(G_{m_{pts}}\\) with weights greater than \\(\\epsilon\\), then the connected components are the \\(\\epsilon\\)-core objects in the DBSCAN clusters, and remaining objects noise. Thus, DBSCAN partitions for \\(\\epsilon \\in [0, \\infty)\\) can be produced in a hierarchical way: at 0 we have the finest clusters, or leaves of a dendrogram, while at higher values, we approach the root of the dendrogram.\n\n\n\n\n\n\n\nCompute core distance wrt \\(m_{pts}\\) for all data objects in \\(X\\)\nCompute minimum spanning tree (MST) of \\(G_{m_{pts}}\\), the mutual reachability graph\nExtend the MST to obtain \\(MST_{ext}\\) by adding for each vertex, a “self edge” with the core distance of the corresponding object as weight\nExtract the HDBSCAN hierarchy as a dendrogram from \\(MST_{ext}\\)\n\n4.1 For the root of the tree, assign all objects the same label\n4.2 Iteratively remove all edges from the \\(MST_{ext}\\) in decreasing order of weight, with ties being removed simultaenously\n\n4.2.1 Before each removal, set the dendrogram scale value of the current hierarchical level as the weight of the edge(s) to be removed\n4.2.2 After each removal, assign labels to connected component(s) that contain the end vertices of removed edges, a new cluster label to a component if it still has at least one edge, else noise\n\n\n\n\n\n\nWhen reaching a lower \\(\\epsilon\\) level in the tree, we must consider what happens to the clusters. We can define a separate minimum cluster size which can be applied to the following: - if all cluster’s subcomponents are spurious, it disappears - if only one of cluster’s subcomponents is not spurious, keep original label, ie cluster shrinkage - if two or more of cluster’s subcomponents are not spurious, “true” cluster split\n\n\n\n\n\n\nWhen increasing the density threshold, we can see prominent clusters remain and shrink or split while others disappear. A cluster can be seen as a set of points that whose density \\(f(x)\\), exceed a threshold, ie. \\(\\lambda = 1/{\\epsilon}\\). For a density contour cluster \\(C_i\\) that appears at density level \\(\\lambda_{min}(C_i)\\), we can formalize its stability by defining its excess of mass as:\n\\[E(C_i) = \\int_{x \\in C_{i}}\\Big( f(x) - \\lambda_{min}(C_i)\\Big)dx\\]\nThis exhibits monotonic behavior along branches of the dendrogram, so it cannot be used to compare stabilities of nested clusters. Instead, we can define the relative excess of mass:\n\\[E_R(C_i) = \\int_{x \\in C_{i}}\\Big( \\lambda_{max}(x, C_i) - \\lambda_{min}(C_i)\\Big)dx\\]\nwhere \\(\\lambda_{max}(C_i)\\) is the density level at which \\(C_i\\) is split or disappears, and \\(\\lambda_{max}(x, C_i) = \\min\\{f(x), \\lambda_{max}(C_i)\\}\\)\nFor an HDBSCAN hierarchy, where we have finite data set \\(X\\), cluster labels, and desnity thresholds associated with each hierarchical level, we can adapt the previous expression to define the stability of a cluster \\(C_i\\) as:\n\\[ S(C_i) = \\sum_{x_j \\in C_i} \\Big( \\lambda_{max}(x, C_i) - \\lambda_{min}(C_i)\\Big) = \\sum_{x_j \\in C_i} \\Big(\\frac{1}{\\epsilon_{min}(x_j, C_i)} - \\frac{1}{\\epsilon_{max}(C_i)}\\Big)\\]\nwhere \\(\\lambda_{min}(C_i)\\) is the minimum density level at which \\(C_i\\) exists, \\(\\lambda_{max}(x_j, C_i)\\) is the density level beyond which object \\(x_j\\) no longer belongs to cluster \\(C_i\\) and \\(\\epsilon_{max}(C_i)\\) and \\(\\epsilon_{min}(x_j, C_i)\\) are the corresponding values for the threshold \\(\\epsilon\\).\n\n\n\nWith the notion of cluster stability developed, we can have extraction of the most prominent clusters formulated as an optimization problem of maximizing cluster stabilities subject to constraints:\n\\[\\underset{\\delta_2, ..., \\delta_{\\kappa}}\\max J = \\sum^\\kappa_{i=2}\\delta_iS(C_i)\\]\n\\[\\text{subject to} \\begin{cases}\n                    \\delta_i \\in \\{0,1\\}, i=2,...,\\kappa \\\\                    \n                    \\sum_{j \\in I_h} \\delta_j = 1, \\forall h \\in L\\\\\n                    \\end{cases}\\]\nwhere \\(\\delta_i\\) indicates if cluster \\(i\\) is in the flat solution, \\(L = \\{h | C_h \\text{is a leaf cluster}\\}\\) is the set of indices of leaf clusters, and \\(I_h = \\{j | j \\ne 1 \\text{ and }C_j \\text{ is an ascendant of } C_h\\}\\), the set of indices of all clusters on the path from \\(C_h\\) (included) to the root (excluded). The constraints prevent nested clusters on the same path from being selected at the same time.\nSelecting the clusters requires bottom up processing of nodes excluding the root, starting with the leaves, deciding whether to keep node \\(C_i\\) or a best so far selection of clusters in its subtrees. In the process, the total stability, \\(\\hat{S}(C_i)\\) is updated:\n\\[\\hat{S}(C_i) = \\begin{cases}\n                S(C_i), &\\text{if } C_i \\text{ is a leaf node}\\\\\n                \\max\\{S(C_i), \\hat{S}(C_{i_l}) + \\hat{S}(C_{i_r}) &\\text{if } C_i \\text{ is an internal node}\\\\\n                \\end{cases}\n                \\]\n\n\n\nInitialize \\(\\delta_2 = ... \\delta_\\kappa = 1\\) and for all leaf nodes, set \\(\\hat{S}(C_h) = S(C_h)\\)\nStarting from the deepest levels, do bottom-up (excluding root):\n\n2.1 If \\(S(C_i) &lt; \\hat{S}(C_{i_r}) + \\hat{S}(C_{i_l})\\), set \\(\\hat{S}(C_i) = \\hat{S}(C_{i_l}) + \\hat{S}(C_{i_r})\\) and set \\(\\delta_i=0\\)\n2.2 Else, set \\(\\hat{S}(C_i) = S(C_i)\\) and set \\(\\delta_{(.)} = 0\\) for all clusters in \\(C_i\\)’s subtrees.\n\n\n\n\n\n\n\nHierarchical clustering can typically be slow but thanks to parallelization, can be sped up via GPUs. Nvidia’s blog entry, Faster HDBSCAN Soft Clustering with RAPIDS cuML, shows comparisons with multiple methods on datasets of varying sizes.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn.datasets as data\n%matplotlib inline\nsns.set_context('poster')\nsns.set_style('white')\nsns.set_color_codes()\nplot_kwds = {'alpha' : 0.5, 's' : 80, 'linewidths':0}\n\n\nmoons, _ = data.make_moons(n_samples=50, noise=0.05)\nblobs, _ = data.make_blobs(n_samples=50, centers=[(-0.75,2.25), (1.0, 2.0)], cluster_std=0.25)\ntest_data = np.vstack([moons, blobs])\nplt.scatter(test_data.T[0], test_data.T[1], color='b', **plot_kwds)\n\n\n\n\n\n\n\n\n\nimport hdbscan\n\nclusterer = hdbscan.HDBSCAN(min_cluster_size=5, gen_min_span_tree=True)\nclusterer.fit(test_data)\n\nHDBSCAN(gen_min_span_tree=True)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.HDBSCANHDBSCAN(gen_min_span_tree=True)\n\n\n\nclusterer.minimum_spanning_tree_.plot(edge_cmap='viridis',\n                                      edge_alpha=0.6,\n                                      node_size=80,\n                                      edge_linewidth=2)\n\n\n\n\n\n\n\n\n\nclusterer.single_linkage_tree_.plot(cmap='viridis', colorbar=True)\n\n\n\n\n\n\n\n\n\nclusterer.condensed_tree_.plot()\n\n\n\n\n\n\n\n\n\nclusterer.condensed_tree_.plot(select_clusters=True, selection_palette=sns.color_palette())\n\n\n\n\n\n\n\n\n\npalette = sns.color_palette()\ncluster_colors = [sns.desaturate(palette[col], sat)\n                  if col &gt;= 0 else (0.5, 0.5, 0.5) for col, sat in\n                  zip(clusterer.labels_, clusterer.probabilities_)]\nplt.scatter(test_data.T[0], test_data.T[1], c=cluster_colors, **plot_kwds)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHard clustering assigns a single cluster label or noise\nSoft clustering assigns a vector of probabilities\n\n\n\n\n\nimport hdbscan\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport matplotlib as mpl\n\nfrom scipy.spatial.distance import cdist\n\n%matplotlib inline\nsns.set_context('poster')\nsns.set_style('white')\nsns.set_color_codes()\n\nplot_kwds={'alpha':0.25, 's':60, 'linewidths':0}\npalette = sns.color_palette('deep', 12)\n\n\ndata = np.load('clusterable_data.npy')\nfig = plt.figure()\nax = fig.add_subplot(111)\nplt.scatter(data.T[0], data.T[1], **plot_kwds)\nax.set_xticks([])\nax.set_yticks([]);\n\n\n\n\n\n\n\n\n\nlen(data)\n\n2309\n\n\n\n# build a clustering object to fit the data\nclusterer = hdbscan.HDBSCAN(min_cluster_size=15).fit(data)\nclusterer.labels_\n\narray([ 5,  5,  5, ..., -1, -1,  5], dtype=int64)\n\n\n\nlen(clusterer.labels_)\n\n2309\n\n\n\nclusterer.probabilities_\n\narray([1.        , 0.85883269, 0.90828071, ..., 0.        , 0.        ,\n       1.        ])\n\n\n\nclusterer.probabilities_\n\narray([1.        , 0.85883269, 0.90828071, ..., 0.        , 0.        ,\n       1.        ])\n\n\n\n# visualize clustering using scores as saturation\npal = sns.color_palette('deep', 8)\ncolors = [sns.desaturate(pal[col], sat) for col, sat in zip(clusterer.labels_,\n                                                            clusterer.probabilities_)]\nplt.scatter(data.T[0], data.T[1], c=colors, **plot_kwds)\n\n\n\n\n\n\n\n\n\n\nThis takes advantage of features to examine clustering further, as pointed out in these docs. In particular, we use the CondensedTree object, obtained via `clusterer. which is a dendrogram, or family tree of various clusters.\n\n_raw_tree is the tree in the form of a numpy recarray allowing lookup based on field names where each row represents an edge between a parent and child clusters:\n\nparent: id of parent cluster\nchild: id of child cluster\nlambda_val: inverse distance, aka density at which edge forms\nchild_size: number of points in the child cluster\n\n\n\ndef exemplars(cluster_id, condensed_tree):\n    raw_tree = condensed_tree._raw_tree\n    # Just the cluster elements of the tree, excluding singleton points\n    clusterer_tree = raw_tree[raw_tree['child_size'] &gt; 1]\n    # Get the leaf cluster nodes under the cluster we are considering\n    leaves = hdbscan.plots._recurse_leaf_dfs(clusterer_tree, cluster_id)\n    # Now collect up the last remaining points of each leaf cluster (the heart of the leaf)\n    result = np.array([])\n    for leaf in leaves:\n        max_lambda = raw_tree['lambda_val'][raw_tree['parent'] == leaf].max()\n        points = raw_tree['child'][(raw_tree['parent'] == leaf) &\n                                   (raw_tree['lambda_val'] == max_lambda)]\n        result = np.hstack((result, points))\n    return result.astype('int32')\n\ntree = clusterer.condensed_tree_\nplt.scatter(data.T[0], data.T[1], c='grey', **plot_kwds)\nfor i, c in enumerate(tree._select_clusters()):\n    c_exemplars = exemplars(c, tree)\n    plt.scatter(data.T[0][c_exemplars], data.T[1][c_exemplars], c=palette[i], **plot_kwds)\n\nC:\\Users\\Jonathan\\AppData\\Local\\Temp\\ipykernel_16588\\4108267181.py:20: UserWarning: *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n  plt.scatter(data.T[0][c_exemplars], data.T[1][c_exemplars], c=palette[i], **plot_kwds)\n\n\n\n\n\n\n\n\n\n\ndef min_dist_to_exemplar(point, cluster_exemplars, data):\n    dists = cdist([data[point]], data[cluster_exemplars.astype(np.int32)])\n    return dists.min()\n\ndef dist_vector(point, exemplar_dict, data):\n    result = {}\n    for cluster in exemplar_dict:\n        result[cluster] = min_dist_to_exemplar(point, exemplar_dict[cluster], data)\n    return np.array(list(result.values()))\n\ndef dist_membership_vector(point, exemplar_dict, data, softmax=False):\n    if softmax:\n        result = np.exp(1./dist_vector(point, exemplar_dict, data))\n        result[~np.isfinite(result)] = np.finfo(np.double).max\n    else:\n        result = 1./dist_vector(point, exemplar_dict, data)\n        result[~np.isfinite(result)] = np.finfo(np.double).max\n    result /= result.sum()\n    return result\n\n\nexemplar_dict = {c:exemplars(c, tree) for c in tree._select_clusters()}\ncolors = np.empty((data.shape[0], 3))\nfor x in range(data.shape[0]):\n    membership_vector = dist_membership_vector(x, exemplar_dict, data)\n    color = np.argmax(membership_vector)\n    saturation = membership_vector[color]\n    colors[x] = sns.desaturate(pal[color], saturation)\nplt.scatter(data.T[0], data.T[1], c=colors, **plot_kwds);\n\nC:\\Users\\Jonathan\\AppData\\Local\\Temp\\ipykernel_16588\\1075745609.py:16: RuntimeWarning: divide by zero encountered in divide\n  result = 1./dist_vector(point, exemplar_dict, data)\n\n\n\n\n\n\n\n\n\n\n\n\n\nthese are density-based memberships using cluster persistence as a baseline of comparison\nGLOSH algorithm compares cluster persistence with how long a point stayed in the cluster\nwe can compare cluster persistence with the merge height of the point with a fixed cluster in the dendrogram\n\nmerge height represents the disimilarity between two clusters\nwe can do that with multiple clusters to form a vector which can be normalized into memership scores\n\n\n\ndef max_lambda_val(cluster, tree):\n    cluster_tree = tree[tree['child_size'] &gt; 1]\n    leaves = hdbscan.plots._recurse_leaf_dfs(cluster_tree, cluster)\n    max_lambda = 0.0\n    for leaf in leaves:\n        max_lambda = max(max_lambda,\n                         tree['lambda_val'][tree['parent'] == leaf].max())\n    return max_lambda\n\ndef points_in_cluster(cluster, tree):\n    leaves = hdbscan.plots._recurse_leaf_dfs(tree, cluster)\n    return leaves\n\ndef merge_height(point, cluster, tree, point_dict):\n    cluster_row = tree[tree['child'] == cluster]\n    cluster_height = cluster_row['lambda_val'][0]\n    if point in point_dict[cluster]:\n        merge_row = tree[tree['child'] == float(point)][0]\n        return merge_row['lambda_val']\n        \n    else:\n        while point not in point_dict[cluster]:\n            parent_row = tree[tree['child'] == cluster]\n            cluster = parent_row['parent'].astype(np.float64)[0]\n        for row in tree[tree['parent'] == cluster]:\n            child_cluster = float(row['child'])\n            if child_cluster == point:\n                return row['lambda_val']\n            if child_cluster in point_dict and point in point_dict[child_cluster]:\n                return row['lambda_val']\n            \ndef per_cluster_scores(point, cluster_ids, tree, max_lambda_dict, point_dict):\n    result = {}\n    point_row = tree[tree['child'] == point]\n    point_cluster = float(point_row[0]['parent'])\n    max_lambda = max_lambda_dict[point_cluster] + 1e-8 # avoid zero lambda vals in odd cases\n    \n    for c in cluster_ids:\n        height = merge_height(point, c, tree, point_dict)\n        result[c] = (max_lambda / (max_lambda - height))\n    return result\n\ndef outlier_membership_vector(point, cluster_ids, tree,\n                              max_lambda_dict, point_dict, softmax=True):\n    if softmax:\n        result = np.exp(np.array(list(per_cluster_scores(point,\n                                                         cluster_ids,\n                                                         tree,\n                                                         max_lambda_dict,\n                                                         point_dict\n                                                         ).values())))\n        result[~np.isfinite(result)] = np.finfo(np.double).max\n    else:\n        result = np.array(list(per_cluster_scores(point,\n                                                  cluster_ids,\n                                                  tree,\n                                                  max_lambda_dict,\n                                                  point_dict\n                                                 ).values()))\n        result /= result.sum()\n    return result\n\ncluster_ids = tree._select_clusters()\nraw_tree = tree._raw_tree\nall_possible_clusters = np.arange(data.shape[0], raw_tree['parent'].max() + 1).astype(np.float64)\nmax_lambda_dict = {c:max_lambda_val(c, raw_tree) for c in all_possible_clusters}\npoint_dict = {c:set(points_in_cluster(c, raw_tree)) for c in all_possible_clusters}\ncolors = np.empty((data.shape[0], 3))\nfor x in range(data.shape[0]):\n    membership_vector = outlier_membership_vector(x, cluster_ids, raw_tree,\n                                                  max_lambda_dict, point_dict, False)\n    color = np.argmax(membership_vector)\n    saturation = membership_vector[color]\n    colors[x] = sns.desaturate(pal[color], saturation)\nplt.scatter(data.T[0], data.T[1], c=colors, **plot_kwds);\n\n\n\n\n\n\n\n\n\n\n\nTo combine distance and membership approaches we can: - view membership vectors as probability distributions - combining them can be achieved via Bayes’ rule\n\ndef combined_membership_vector(point, data, tree, exemplar_dict, cluster_ids,\n                    max_lambda_dict, point_dict, softmax=False):\n    raw_tree = tree._raw_tree\n    dist_vec = dist_membership_vector(point, exemplar_dict, data, softmax)\n    outl_vec = outlier_membership_vector(point, cluster_ids, raw_tree,\n                                         max_lambda_dict, point_dict, softmax)\n    result = dist_vec * outl_vec\n    result /= result.sum()\n    return result\n\ncolors = np.empty((data.shape[0], 3))\nfor x in range(data.shape[0]):\n    membership_vector = combined_membership_vector(x, data, tree, exemplar_dict, cluster_ids,\n                                                   max_lambda_dict, point_dict, False)\n    color = np.argmax(membership_vector)\n    saturation = membership_vector[color]\n    colors[x] = sns.desaturate(pal[color], saturation)\nplt.scatter(data.T[0], data.T[1], c=colors, **plot_kwds);\n\nC:\\Users\\Jonathan\\AppData\\Local\\Temp\\ipykernel_16588\\1075745609.py:16: RuntimeWarning: divide by zero encountered in divide\n  result = 1./dist_vector(point, exemplar_dict, data)\n\n\n\n\n\n\n\n\n\n\n\n\nThe previous computation was the probability vector that a point is in each cluster, given that the point is in a cluster. To convert the conditional probability to a joint one, we need to: - multiply it by the probability that that there is a cluster to which the point belongs - this can be estimated from the merge height and comparing it with the max density - this should result in a number between 0 and 1\n\nplotting can show the $ $ cluster and the corresponding color\n\n\ndef prob_in_some_cluster(point, tree, cluster_ids, point_dict, max_lambda_dict):\n    heights = []\n    for cluster in cluster_ids:\n        heights.append(merge_height(point, cluster, tree._raw_tree, point_dict))\n    height = max(heights)\n    nearest_cluster = cluster_ids[np.argmax(heights)]\n    max_lambda = max_lambda_dict[nearest_cluster]\n    return height / max_lambda\n\ncolors = np.empty((data.shape[0], 3))\nfor x in range(data.shape[0]):\n    membership_vector = combined_membership_vector(x, data, tree, exemplar_dict, cluster_ids,\n                                                   max_lambda_dict, point_dict, False)\n    membership_vector *= prob_in_some_cluster(x, tree, cluster_ids, point_dict, max_lambda_dict)\n    color = np.argmax(membership_vector)\n    saturation = membership_vector[color]\n    colors[x] = sns.desaturate(pal[color], saturation)\nplt.scatter(data.T[0], data.T[1], c=colors, **plot_kwds);\n\nC:\\Users\\Jonathan\\AppData\\Local\\Temp\\ipykernel_16588\\1075745609.py:16: RuntimeWarning: divide by zero encountered in divide\n  result = 1./dist_vector(point, exemplar_dict, data)\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat are the general steps involved in performing a soft clustering analysis?\n\ndimensionality reduction as a prerequisite to 2D visualization:\n\nPCA\nTSNE\nUMAP\n\n\nvisualization of raw data\nsoft clustering via HDBSCAN\nvisualization of cluster membership\n\nhard clustering color coding\nsoft clustering color saturation\n\n\nquantitative analysis of cluster membership probabilities\n\n\nfrom sklearn import datasets\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport hdbscan\n\n# loading raw data, 8x8 gray scale of handwritten digits, plotting low dimensional projection\ndigits = datasets.load_digits()\ndata = digits.data\nprojection = TSNE().fit_transform(data)\nplt.scatter(*projection.T, **plot_kwds)\n\n\n# setting up a clustering object, fitting and clustering the data \nclusterer = hdbscan.HDBSCAN(min_cluster_size=10, prediction_data=True).fit(data)\ncolor_palette = sns.color_palette('Paired', 12)\ncluster_colors = [color_palette[x] if x &gt;= 0\n                  else (0.5, 0.5, 0.5)\n                  for x in clusterer.labels_]\ncluster_member_colors = [sns.desaturate(x, p) for x, p in\n                         zip(cluster_colors, clusterer.probabilities_)]\nplt.scatter(*projection.T, s=50, linewidth=0, c=cluster_member_colors, alpha=0.25)\n\n\n\n\n\n\n\n\nSome of the data is noisy, so we can examine the noisy ones more through soft clustering.\n\nsoft_clusters = hdbscan.all_points_membership_vectors(clusterer)\ncolor_palette = sns.color_palette('Paired', 12)\ncluster_colors = [color_palette[np.argmax(x)]\n                  for x in soft_clusters]\nplt.scatter(*projection.T, s=50, linewidth=0, c=cluster_colors, alpha=0.25)\n\n\n\n\n\n\n\n\nWe can show uncertainty by coupling probability with desaturation of color. The higher the probability, the more saturated or pure the color. The lower the probability, the more gray it will be. Here we can see that desaturation is a harsh treatment with lots of gray, and that a lower limit may be more visually meaningful.\n\ncolor_palette = sns.color_palette('Paired', 12)\ncluster_colors = [sns.desaturate(color_palette[np.argmax(x)], np.max(x))\n                  for x in soft_clusters]\nplt.scatter(*projection.T, s=50, linewidth=0, c=cluster_colors, alpha=0.25)\n\n\n\n\n\n\n\n\nOne question to investigate is what points have high likelihoods for two clusters and low for the others. It’s worthwhile to note that the probabilities are joint ones, and that points have a probability of not being in a cluster.\n\ndef top_two_probs_diff(probs):\n    sorted_probs = np.sort(probs)\n    return sorted_probs[-1] - sorted_probs[-2]\n\n# Compute the differences between the top two probabilities\ndiffs = np.array([top_two_probs_diff(x) for x in soft_clusters])\n# Select out the indices that have a small difference, and a larger total probability, extract from tuple form\nmixed_points = np.where((diffs &lt; 0.001)  & (np.sum(soft_clusters, axis=1) &gt; 0.5))[0]\n\ncolors = [(0.75, 0.1, 0.1) if x in mixed_points\n          else (0.5, 0.5, 0.5) for x in range(data.shape[0])]\nplt.scatter(*projection.T, s=50, linewidth=0, c=colors, alpha=0.5)\n\n\n\n\n\n\n\n\n\nfig = plt.figure()\nfor i, image in enumerate(digits.images[mixed_points][:16]):\n    ax = fig.add_subplot(4,4,i+1)\n    ax.imshow(image)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nplt.hist(np.sum(soft_clusters, axis=1))\n\n(array([ 36.,  47.,  92., 171., 332., 229., 181., 297., 153., 259.]),\n array([0.58730357, 0.62857321, 0.66984285, 0.7111125 , 0.75238214,\n        0.79365178, 0.83492143, 0.87619107, 0.91746071, 0.95873036,\n        1.        ]),\n &lt;BarContainer object of 10 artists&gt;)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGiven a set of observations \\((x_1, x_2, ..., x_n)\\) in a \\(d\\)-dimensional vector space, k-means clustering partitions the \\(n\\) observations into \\(k\\) sets, \\(\\textbf{S} = \\{S_1, S_2, ... S_k\\}\\). The goal is to minimize the variance within each cluster:\n\\[\\underset{\\textbf{S}}{\\arg\\min} \\sum^k_{i=1} \\sum_{x\\in S_i} ||x - \\mu_i||^2 = \\underset{\\textbf{S}}{\\arg\\min}\\sum^k_{i=1}|S_i|\\text{Var}S_i\\]\nwhere \\(\\mu_i\\) is the mean or centroid of the points in \\(S_i\\).\n\n\n\n\n\nFirst choose \\(k\\) and initialize randomly chosen \\(k\\) centroids at \\(t = 0\\), the initial step.\n\n\n\nAssign each observation to the cluster with the nearest mean, in terms of Euclidean distance. Each set is thus:\n\\[S^{(t)}_i = \\big\\{x_p : ||x_p - m_i^{(t)}||^2 \\le ||x_p - m_j^{(t)}||^2 \\forall j, 1 \\le j \\le k \\big\\}\\]\n\n\n\nAfter assignment, update to find the new means for the next iteration, \\(t + 1\\):\n\\[ m_i^{t + 1} = \\frac{1}{|S_i^{(t)}|}\\sum_{x_j \\in S_i^{(t)}}x_j\\]\nThe number of computations scales with the number of samples \\(n\\), the number of centers \\(k\\), the number of dimensions \\(d\\) and the number of iterations \\(i\\), for \\(O(nkdi)\\)\n\n\n\n\n\n# import statements\nfrom sklearn.datasets import make_blobs\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set('notebook')\n# Generate and plot data with predetermined model and classes\nX, y = make_blobs(n_samples=200, n_features=2, centers=4,\n                  cluster_std=1.6, random_state=50)\n\nplt.scatter(X[:,0], X[:,1], c=y, cmap='viridis')\nplt.xlim(-15,15)\nplt.ylim(-15,15)\nplt.show()\n\n\n\n\n\n\n\n\n\nY = np.zeros((200, 2))\nY[:,0] = y\nY\n\narray([[2., 0.],\n       [0., 0.],\n       [3., 0.],\n       [3., 0.],\n       [1., 0.],\n       [0., 0.],\n       [2., 0.],\n       [0., 0.],\n       [0., 0.],\n       [1., 0.],\n       [1., 0.],\n       [2., 0.],\n       [1., 0.],\n       [1., 0.],\n       [1., 0.],\n       [3., 0.],\n       [2., 0.],\n       [2., 0.],\n       [1., 0.],\n       [1., 0.],\n       [1., 0.],\n       [3., 0.],\n       [2., 0.],\n       [2., 0.],\n       [2., 0.],\n       [3., 0.],\n       [2., 0.],\n       [1., 0.],\n       [3., 0.],\n       [2., 0.],\n       [2., 0.],\n       [1., 0.],\n       [1., 0.],\n       [0., 0.],\n       [0., 0.],\n       [1., 0.],\n       [1., 0.],\n       [1., 0.],\n       [3., 0.],\n       [3., 0.],\n       [2., 0.],\n       [1., 0.],\n       [3., 0.],\n       [3., 0.],\n       [0., 0.],\n       [0., 0.],\n       [2., 0.],\n       [3., 0.],\n       [3., 0.],\n       [2., 0.],\n       [0., 0.],\n       [1., 0.],\n       [3., 0.],\n       [2., 0.],\n       [3., 0.],\n       [3., 0.],\n       [0., 0.],\n       [1., 0.],\n       [1., 0.],\n       [3., 0.],\n       [0., 0.],\n       [0., 0.],\n       [2., 0.],\n       [0., 0.],\n       [0., 0.],\n       [2., 0.],\n       [0., 0.],\n       [3., 0.],\n       [2., 0.],\n       [2., 0.],\n       [0., 0.],\n       [1., 0.],\n       [2., 0.],\n       [0., 0.],\n       [3., 0.],\n       [3., 0.],\n       [1., 0.],\n       [3., 0.],\n       [3., 0.],\n       [0., 0.],\n       [1., 0.],\n       [3., 0.],\n       [1., 0.],\n       [1., 0.],\n       [1., 0.],\n       [1., 0.],\n       [2., 0.],\n       [0., 0.],\n       [2., 0.],\n       [1., 0.],\n       [0., 0.],\n       [2., 0.],\n       [0., 0.],\n       [3., 0.],\n       [3., 0.],\n       [1., 0.],\n       [0., 0.],\n       [3., 0.],\n       [0., 0.],\n       [3., 0.],\n       [1., 0.],\n       [0., 0.],\n       [3., 0.],\n       [2., 0.],\n       [3., 0.],\n       [0., 0.],\n       [3., 0.],\n       [1., 0.],\n       [1., 0.],\n       [2., 0.],\n       [0., 0.],\n       [1., 0.],\n       [2., 0.],\n       [2., 0.],\n       [1., 0.],\n       [3., 0.],\n       [2., 0.],\n       [2., 0.],\n       [3., 0.],\n       [0., 0.],\n       [2., 0.],\n       [2., 0.],\n       [2., 0.],\n       [2., 0.],\n       [0., 0.],\n       [2., 0.],\n       [2., 0.],\n       [3., 0.],\n       [2., 0.],\n       [0., 0.],\n       [1., 0.],\n       [0., 0.],\n       [0., 0.],\n       [3., 0.],\n       [2., 0.],\n       [3., 0.],\n       [0., 0.],\n       [2., 0.],\n       [2., 0.],\n       [1., 0.],\n       [2., 0.],\n       [3., 0.],\n       [2., 0.],\n       [1., 0.],\n       [1., 0.],\n       [2., 0.],\n       [0., 0.],\n       [3., 0.],\n       [0., 0.],\n       [1., 0.],\n       [2., 0.],\n       [1., 0.],\n       [3., 0.],\n       [0., 0.],\n       [0., 0.],\n       [3., 0.],\n       [2., 0.],\n       [0., 0.],\n       [0., 0.],\n       [0., 0.],\n       [3., 0.],\n       [3., 0.],\n       [1., 0.],\n       [0., 0.],\n       [1., 0.],\n       [1., 0.],\n       [1., 0.],\n       [3., 0.],\n       [0., 0.],\n       [0., 0.],\n       [0., 0.],\n       [0., 0.],\n       [1., 0.],\n       [3., 0.],\n       [0., 0.],\n       [2., 0.],\n       [1., 0.],\n       [0., 0.],\n       [2., 0.],\n       [3., 0.],\n       [0., 0.],\n       [3., 0.],\n       [3., 0.],\n       [2., 0.],\n       [1., 0.],\n       [0., 0.],\n       [1., 0.],\n       [2., 0.],\n       [3., 0.],\n       [0., 0.],\n       [3., 0.],\n       [3., 0.],\n       [1., 0.],\n       [3., 0.],\n       [0., 0.],\n       [2., 0.],\n       [1., 0.],\n       [3., 0.],\n       [1., 0.],\n       [2., 0.]])\n\n\n\nfrom sklearn.cluster import KMeans\n\n\nn_clusters=4\n\n# Fit and predict clusters\nkmeans = KMeans(n_clusters=n_clusters)\nkmeans.fit(X)\nprint(kmeans.cluster_centers_)\ny_km = kmeans.fit_predict(X)\n\n# Plot predictions, one class at a time\nfor i, c in enumerate(['red', 'black', 'blue', 'cyan']):\n    plt.scatter(X[y_km ==i,0], X[y_km == i,1], s=100, c=c)\nplt.show()\n\n[[-5.56465793 -2.34988939]\n [-1.92101646  5.21673484]\n [ 0.05161133 -5.35489826]\n [-2.40167949 10.17352695]]\n\n\n\n\n\n\n\n\n\n\n\n\n\nHierarchical clustering involves starting with treating each observation as a set, and then at the following step, creating a new cluster from the two “nearest” clusters, according to the defined distance metric. There are multiple ways the idea of a distance metric can extend to clusters. Some common choices include the average, closest members, or farthest nembers.\n\n# import hierarchical clustering libraries\nfrom sklearn.cluster import AgglomerativeClustering\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom scipy.cluster.hierarchy import dendrogram, linkage, to_tree\n%load_ext autoreload\n%autoreload 2\nsns.set()\ndata = [[i] for i in [-30, 4, 1, 2, 5, 6, 10, 50, 75, 100]]\n\n\ndef llf(id):\n    return str(id)\n\n\nZ = linkage(data, 'ward')\nfig = plt.figure(figsize=(20, 10))\ndn_truncated = dendrogram(Z, orientation='right', p=4, truncate_mode='lastp', leaf_label_func=llf)\n\n\n\n\n\n\n\n\n\ndn = dendrogram(Z, orientation='right', p=4, truncate_mode=None, leaf_label_func=llf)\n\n\n\n\n\n\n\n\n\ndn_truncated\n\n{'icoord': [[5.0, 5.0, 15.0, 15.0],\n  [25.0, 25.0, 35.0, 35.0],\n  [10.0, 10.0, 30.0, 30.0]],\n 'dcoord': [[0.0, 43.30127018922193, 43.30127018922193, 0.0],\n  [0.0, 45.389321169086415, 45.389321169086415, 0.0],\n  [43.30127018922193,\n   154.2898015331631,\n   154.2898015331631,\n   45.389321169086415]],\n 'ivl': ['9', '15', '0', '14'],\n 'leaves': [9, 15, 0, 14],\n 'color_list': ['C1', 'C2', 'C0'],\n 'leaves_color_list': ['C1', 'C1', 'C2', 'C2'],\n 'traversal': [18, 16, 9, 15, 17, 0, 14]}\n\n\n\ndn['icoord']\n\n[[15.0, 15.0, 25.0, 25.0],\n [5.0, 5.0, 20.0, 20.0],\n [55.0, 55.0, 65.0, 65.0],\n [85.0, 85.0, 95.0, 95.0],\n [75.0, 75.0, 90.0, 90.0],\n [60.0, 60.0, 82.5, 82.5],\n [45.0, 45.0, 71.25, 71.25],\n [35.0, 35.0, 58.125, 58.125],\n [12.5, 12.5, 46.5625, 46.5625]]\n\n\n\ndn['dcoord']\n\n[[0.0, 25.0, 25.0, 0.0],\n [0.0, 43.30127018922193, 43.30127018922193, 25.0],\n [0.0, 1.0, 1.0, 0.0],\n [0.0, 1.0, 1.0, 0.0],\n [0.0, 1.7320508075688772, 1.7320508075688772, 1.0],\n [1.0, 5.422176684690383, 5.422176684690383, 1.7320508075688772],\n [0.0, 8.262364471909155, 8.262364471909155, 5.422176684690383],\n [0.0, 45.389321169086415, 45.389321169086415, 8.262364471909155],\n [43.30127018922193, 154.2898015331631, 154.2898015331631, 45.389321169086415]]\n\n\n\ndn_truncated['icoord']\n\n[[5.0, 5.0, 15.0, 15.0], [25.0, 25.0, 35.0, 35.0], [10.0, 10.0, 30.0, 30.0]]\n\n\n\ndn_truncated['dcoord']\n\n[[0.0, 43.30127018922193, 43.30127018922193, 0.0],\n [0.0, 45.389321169086415, 45.389321169086415, 0.0],\n [43.30127018922193, 154.2898015331631, 154.2898015331631, 45.389321169086415]]\n\n\n\nk = 4\nfor i in zip(dn['icoord'][k], dn['dcoord'][k]):\n    print(i)\n\n(75.0, 0.0)\n(75.0, 1.7320508075688772)\n(90.0, 1.7320508075688772)\n(90.0, 1.0)\n\n\n\n\nFull documentation for Scipy’s implementation is found here. The original observations are treated as \\(n\\) clusters. There are \\(n-1\\) new clusters created beyond these total, with indices ranging from \\(n+1\\) to \\(2n-1\\), ordered by the inter-cluster distance. Each of thse new clusters is made from two other clusters. The \\(n-1 \\times 4\\) linkage matrix \\(Z\\) encodes information about the new clusters in the following manner: Each row \\(i\\) (keeping in mind zero indexing) describes the \\((i+1)\\)th cluster by listing the indices of the two source clusters, the inter-cluster distance between these two clusters, and finally, the number of members of the new cluster.\n\nZ\n\narray([[  2.        ,   3.        ,   1.        ,   2.        ],\n       [  1.        ,   4.        ,   1.        ,   2.        ],\n       [  5.        ,  11.        ,   1.73205081,   3.        ],\n       [ 10.        ,  12.        ,   5.42217668,   5.        ],\n       [  6.        ,  13.        ,   8.26236447,   6.        ],\n       [  7.        ,   8.        ,  25.        ,   2.        ],\n       [  9.        ,  15.        ,  43.30127019,   3.        ],\n       [  0.        ,  14.        ,  45.38932117,   7.        ],\n       [ 16.        ,  17.        , 154.28980153,  10.        ]])\n\n\n\nplt.show()\n\n\n\n\n\nto_tree(Z)\nrootnode, nodelist = to_tree(Z, rd=True)\nrootnode\n\n&lt;__main__.ClusterNode at 0x29427b86370&gt;\n\n\n\nrootnode.get_right().pre_order()\n\n[0, 6, 2, 3, 5, 1, 4]\n\n\n\nlen(nodelist)\n\n19\n\n\n\nnodelist[4].pre_order()\n\n[4]\n\n\n\n\n\n\np =6\n\n\nnp.array(data)\n\narray([[-30],\n       [  4],\n       [  1],\n       [  2],\n       [  5],\n       [  6],\n       [ 10],\n       [ 50],\n       [ 75],\n       [100]])\n\n\n\nn = len(data)\n\n\nlen(range(2*n-p + 1, 2*n))\n\n5\n\n\n\ntraversal = dn_truncated['traversal']\n\n\ntraversal.reverse()\n\n\nlen(traversal)\n\n7\n\n\n\ntraversal[p:]\n\n[14]\n\n\n\n# import plotly.figure_factory as ff\nimport numpy as np\nnp.random.seed(1)\nn = len(data)\np = 4\nfig = create_dendrogram(np.array(data), hovertext=traversal[p:],\n                        labels=data, orientation='left', p=p, truncate_mode='lastp', leaf_label_func=llf)\nfig.update_layout(width=800, height=500)\nfig.show()\n\n                                                \n\n\n\nnodelist[13].pre_order()\n\n[2, 3, 5, 1, 4]\n\n\n\ndn_truncated\n\n{'icoord': [[15.0, 15.0, 25.0, 25.0],\n  [5.0, 5.0, 20.0, 20.0],\n  [45.0, 45.0, 55.0, 55.0],\n  [35.0, 35.0, 50.0, 50.0],\n  [12.5, 12.5, 42.5, 42.5]],\n 'dcoord': [[0.0, 25.0, 25.0, 0.0],\n  [0.0, 43.30127018922193, 43.30127018922193, 25.0],\n  [0.0, 8.262364471909155, 8.262364471909155, 0.0],\n  [0.0, 45.389321169086415, 45.389321169086415, 8.262364471909155],\n  [43.30127018922193,\n   154.2898015331631,\n   154.2898015331631,\n   45.389321169086415]],\n 'ivl': ['9', '7', '8', '0', '6', '13'],\n 'leaves': [9, 7, 8, 0, 6, 13],\n 'color_list': ['C1', 'C1', 'C2', 'C2', 'C0'],\n 'leaves_color_list': ['C1', 'C1', 'C1', 'C2', 'C2', 'C2']}\n\n\n\nZ\n\narray([[  2.        ,   3.        ,   1.        ,   2.        ],\n       [  1.        ,   4.        ,   1.        ,   2.        ],\n       [  5.        ,  11.        ,   1.73205081,   3.        ],\n       [ 10.        ,  12.        ,   5.42217668,   5.        ],\n       [  6.        ,  13.        ,   8.26236447,   6.        ],\n       [  7.        ,   8.        ,  25.        ,   2.        ],\n       [  9.        ,  15.        ,  43.30127019,   3.        ],\n       [  0.        ,  14.        ,  45.38932117,   7.        ],\n       [ 16.        ,  17.        , 154.28980153,  10.        ]])\n\n\n\ndn\n\n{'icoord': [[15.0, 15.0, 25.0, 25.0],\n  [5.0, 5.0, 20.0, 20.0],\n  [55.0, 55.0, 65.0, 65.0],\n  [85.0, 85.0, 95.0, 95.0],\n  [75.0, 75.0, 90.0, 90.0],\n  [60.0, 60.0, 82.5, 82.5],\n  [45.0, 45.0, 71.25, 71.25],\n  [35.0, 35.0, 58.125, 58.125],\n  [12.5, 12.5, 46.5625, 46.5625]],\n 'dcoord': [[0.0, 25.0, 25.0, 0.0],\n  [0.0, 43.30127018922193, 43.30127018922193, 25.0],\n  [0.0, 1.0, 1.0, 0.0],\n  [0.0, 1.0, 1.0, 0.0],\n  [0.0, 1.7320508075688772, 1.7320508075688772, 1.0],\n  [1.0, 5.422176684690383, 5.422176684690383, 1.7320508075688772],\n  [0.0, 8.262364471909155, 8.262364471909155, 5.422176684690383],\n  [0.0, 45.389321169086415, 45.389321169086415, 8.262364471909155],\n  [43.30127018922193,\n   154.2898015331631,\n   154.2898015331631,\n   45.389321169086415]],\n 'ivl': ['9', '7', '8', '0', '6', '2', '3', '5', '1', '4'],\n 'leaves': [9, 7, 8, 0, 6, 2, 3, 5, 1, 4],\n 'color_list': ['C1', 'C1', 'C2', 'C2', 'C2', 'C2', 'C2', 'C2', 'C0'],\n 'leaves_color_list': ['C1',\n  'C1',\n  'C1',\n  'C2',\n  'C2',\n  'C2',\n  'C2',\n  'C2',\n  'C2',\n  'C2'],\n 'traversal': [18,\n  16,\n  9,\n  15,\n  7,\n  8,\n  17,\n  0,\n  14,\n  6,\n  13,\n  10,\n  2,\n  3,\n  12,\n  5,\n  11,\n  1,\n  4]}\n\n\n\ndn_truncated\n\n{'icoord': [[15.0, 15.0, 25.0, 25.0],\n  [5.0, 5.0, 20.0, 20.0],\n  [45.0, 45.0, 55.0, 55.0],\n  [35.0, 35.0, 50.0, 50.0],\n  [12.5, 12.5, 42.5, 42.5]],\n 'dcoord': [[0.0, 25.0, 25.0, 0.0],\n  [0.0, 43.30127018922193, 43.30127018922193, 25.0],\n  [0.0, 8.262364471909155, 8.262364471909155, 0.0],\n  [0.0, 45.389321169086415, 45.389321169086415, 8.262364471909155],\n  [43.30127018922193,\n   154.2898015331631,\n   154.2898015331631,\n   45.389321169086415]],\n 'ivl': ['9', '7', '8', '0', '6', '(5)'],\n 'leaves': [9, 7, 8, 0, 6, 13],\n 'color_list': ['C1', 'C1', 'C2', 'C2', 'C0'],\n 'leaves_color_list': ['C1', 'C1', 'C1', 'C2', 'C2', 'C2']}\n\n\n\ni = 0\nprint(dn_truncated['dcoord'][i])\nprint(dn_truncated['icoord'][i])\n\n[0.0, 25.0, 25.0, 0.0]\n[15.0, 15.0, 25.0, 25.0]\n\n\n\ndn_truncated['dcoord']\n\n[[0.0, 25.0, 25.0, 0.0],\n [0.0, 43.30127018922193, 43.30127018922193, 25.0],\n [0.0, 8.262364471909155, 8.262364471909155, 0.0],\n [0.0, 45.389321169086415, 45.389321169086415, 8.262364471909155],\n [43.30127018922193, 154.2898015331631, 154.2898015331631, 45.389321169086415]]\n\n\n\ndn_truncated['icoord']\n\n[[15.0, 15.0, 25.0, 25.0],\n [5.0, 5.0, 20.0, 20.0],\n [45.0, 45.0, 55.0, 55.0],\n [35.0, 35.0, 50.0, 50.0],\n [12.5, 12.5, 42.5, 42.5]]\n\n\n\nnodelist[].pre_order()\n\n[4]\n\n\n\nnodelist[9].id\n\n\ndata[2]\n\n[1]\n\n\n\n#from plotly.figure_factory import create_dendrogram\nimport numpy as np\nimport pandas as pd\nIndex= ['A','B','C','D','E','F','G','H','I','J']\ndf = pd.DataFrame(abs(np.random.randn(10, 10)), index=Index)\nfig = create_dendrogram(df, labels=Index, orientation='left', hovertext=list(range(19)))\nfig.show()\n\n\n\n\n\n\n\n\n_dendrogram_calculate_info now has a list to track the cluster indices during construction of the dendrogram\n\n\n\"\"\"\nHierarchical clustering (:mod:`scipy.cluster.hierarchy`)\n========================================================\n.. currentmodule:: scipy.cluster.hierarchy\nThese functions cut hierarchical clusterings into flat clusterings\nor find the roots of the forest formed by a cut by providing the flat\ncluster ids of each observation.\n.. autosummary::\n   :toctree: generated/\n   fcluster\n   fclusterdata\n   leaders\nThese are routines for agglomerative clustering.\n.. autosummary::\n   :toctree: generated/\n   linkage\n   single\n   complete\n   average\n   weighted\n   centroid\n   median\n   ward\nThese routines compute statistics on hierarchies.\n.. autosummary::\n   :toctree: generated/\n   cophenet\n   from_mlab_linkage\n   inconsistent\n   maxinconsts\n   maxdists\n   maxRstat\n   to_mlab_linkage\nRoutines for visualizing flat clusters.\n.. autosummary::\n   :toctree: generated/\n   dendrogram\nThese are data structures and routines for representing hierarchies as\ntree objects.\n.. autosummary::\n   :toctree: generated/\n   ClusterNode\n   leaves_list\n   to_tree\n   cut_tree\n   optimal_leaf_ordering\nThese are predicates for checking the validity of linkage and\ninconsistency matrices as well as for checking isomorphism of two\nflat cluster assignments.\n.. autosummary::\n   :toctree: generated/\n   is_valid_im\n   is_valid_linkage\n   is_isomorphic\n   is_monotonic\n   correspond\n   num_obs_linkage\nUtility routines for plotting:\n.. autosummary::\n   :toctree: generated/\n   set_link_color_palette\nUtility classes:\n.. autosummary::\n   :toctree: generated/\n   DisjointSet -- data structure for incremental connectivity queries\n\"\"\"\n# Copyright (C) Damian Eads, 2007-2008. New BSD License.\n\n# hierarchy.py (derived from cluster.py, http://scipy-cluster.googlecode.com)\n#\n# Author: Damian Eads\n# Date:   September 22, 2007\n#\n# Copyright (c) 2007, 2008, Damian Eads\n#\n# All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#   - Redistributions of source code must retain the above\n#     copyright notice, this list of conditions and the\n#     following disclaimer.\n#   - Redistributions in binary form must reproduce the above copyright\n#     notice, this list of conditions and the following disclaimer\n#     in the documentation and/or other materials provided with the\n#     distribution.\n#   - Neither the name of the author nor the names of its\n#     contributors may be used to endorse or promote products derived\n#     from this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n# \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n# A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n# OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n# SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n# LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n# DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n# THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nimport warnings\nimport bisect\nfrom collections import deque\n\nimport numpy as np\nfrom scipy.cluster import _hierarchy, _optimal_leaf_ordering\nimport scipy.spatial.distance as distance\nfrom scipy._lib._disjoint_set import DisjointSet\n\n\n_LINKAGE_METHODS = {'single': 0, 'complete': 1, 'average': 2, 'centroid': 3,\n                    'median': 4, 'ward': 5, 'weighted': 6}\n_EUCLIDEAN_METHODS = ('centroid', 'median', 'ward')\n\n__all__ = ['ClusterNode', 'DisjointSet', 'average', 'centroid', 'complete',\n           'cophenet', 'correspond', 'cut_tree', 'dendrogram', 'fcluster',\n           'fclusterdata', 'from_mlab_linkage', 'inconsistent',\n           'is_isomorphic', 'is_monotonic', 'is_valid_im', 'is_valid_linkage',\n           'leaders', 'leaves_list', 'linkage', 'maxRstat', 'maxdists',\n           'maxinconsts', 'median', 'num_obs_linkage', 'optimal_leaf_ordering',\n           'set_link_color_palette', 'single', 'to_mlab_linkage', 'to_tree',\n           'ward', 'weighted', 'distance']\n\n\nclass ClusterWarning(UserWarning):\n    pass\n\n\ndef _warning(s):\n    warnings.warn('scipy.cluster: %s' % s, ClusterWarning, stacklevel=3)\n\n\ndef _copy_array_if_base_present(a):\n    \"\"\"\n    Copy the array if its base points to a parent array.\n    \"\"\"\n    if a.base is not None:\n        return a.copy()\n    elif np.issubsctype(a, np.float32):\n        return np.array(a, dtype=np.double)\n    else:\n        return a\n\n\ndef _copy_arrays_if_base_present(T):\n    \"\"\"\n    Accept a tuple of arrays T. Copies the array T[i] if its base array\n    points to an actual array. Otherwise, the reference is just copied.\n    This is useful if the arrays are being passed to a C function that\n    does not do proper striding.\n    \"\"\"\n    l = [_copy_array_if_base_present(a) for a in T]\n    return l\n\n\ndef _randdm(pnts):\n    \"\"\"\n    Generate a random distance matrix stored in condensed form.\n    Parameters\n    ----------\n    pnts : int\n        The number of points in the distance matrix. Has to be at least 2.\n    Returns\n    -------\n    D : ndarray\n        A ``pnts * (pnts - 1) / 2`` sized vector is returned.\n    \"\"\"\n    if pnts &gt;= 2:\n        D = np.random.rand(pnts * (pnts - 1) / 2)\n    else:\n        raise ValueError(\"The number of points in the distance matrix \"\n                         \"must be at least 2.\")\n    return D\n\n\ndef single(y):\n    \"\"\"\n    Perform single/min/nearest linkage on the condensed distance matrix ``y``.\n    Parameters\n    ----------\n    y : ndarray\n        The upper triangular of the distance matrix. The result of\n        ``pdist`` is returned in this form.\n    Returns\n    -------\n    Z : ndarray\n        The linkage matrix.\n    See Also\n    --------\n    linkage: for advanced creation of hierarchical clusterings.\n    scipy.spatial.distance.pdist : pairwise distance metrics\n    Examples\n    --------\n    &gt;&gt;&gt; from scipy.cluster.hierarchy import single, fcluster\n    &gt;&gt;&gt; from scipy.spatial.distance import pdist\n    First, we need a toy dataset to play with::\n        x x    x x\n        x        x\n        x        x\n        x x    x x\n    &gt;&gt;&gt; X = [[0, 0], [0, 1], [1, 0],\n    ...      [0, 4], [0, 3], [1, 4],\n    ...      [4, 0], [3, 0], [4, 1],\n    ...      [4, 4], [3, 4], [4, 3]]\n    Then, we get a condensed distance matrix from this dataset:\n    &gt;&gt;&gt; y = pdist(X)\n    Finally, we can perform the clustering:\n    &gt;&gt;&gt; Z = single(y)\n    &gt;&gt;&gt; Z\n    array([[ 0.,  1.,  1.,  2.],\n           [ 2., 12.,  1.,  3.],\n           [ 3.,  4.,  1.,  2.],\n           [ 5., 14.,  1.,  3.],\n           [ 6.,  7.,  1.,  2.],\n           [ 8., 16.,  1.,  3.],\n           [ 9., 10.,  1.,  2.],\n           [11., 18.,  1.,  3.],\n           [13., 15.,  2.,  6.],\n           [17., 20.,  2.,  9.],\n           [19., 21.,  2., 12.]])\n    The linkage matrix ``Z`` represents a dendrogram - see\n    `scipy.cluster.hierarchy.linkage` for a detailed explanation of its\n    contents.\n    We can use `scipy.cluster.hierarchy.fcluster` to see to which cluster\n    each initial point would belong given a distance threshold:\n    &gt;&gt;&gt; fcluster(Z, 0.9, criterion='distance')\n    array([ 7,  8,  9, 10, 11, 12,  4,  5,  6,  1,  2,  3], dtype=int32)\n    &gt;&gt;&gt; fcluster(Z, 1, criterion='distance')\n    array([3, 3, 3, 4, 4, 4, 2, 2, 2, 1, 1, 1], dtype=int32)\n    &gt;&gt;&gt; fcluster(Z, 2, criterion='distance')\n    array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)\n    Also, `scipy.cluster.hierarchy.dendrogram` can be used to generate a\n    plot of the dendrogram.\n    \"\"\"\n    return linkage(y, method='single', metric='euclidean')\n\n\ndef complete(y):\n    \"\"\"\n    Perform complete/max/farthest point linkage on a condensed distance matrix.\n    Parameters\n    ----------\n    y : ndarray\n        The upper triangular of the distance matrix. The result of\n        ``pdist`` is returned in this form.\n    Returns\n    -------\n    Z : ndarray\n        A linkage matrix containing the hierarchical clustering. See\n        the `linkage` function documentation for more information\n        on its structure.\n    See Also\n    --------\n    linkage: for advanced creation of hierarchical clusterings.\n    scipy.spatial.distance.pdist : pairwise distance metrics\n    Examples\n    --------\n    &gt;&gt;&gt; from scipy.cluster.hierarchy import complete, fcluster\n    &gt;&gt;&gt; from scipy.spatial.distance import pdist\n    First, we need a toy dataset to play with::\n        x x    x x\n        x        x\n        x        x\n        x x    x x\n    &gt;&gt;&gt; X = [[0, 0], [0, 1], [1, 0],\n    ...      [0, 4], [0, 3], [1, 4],\n    ...      [4, 0], [3, 0], [4, 1],\n    ...      [4, 4], [3, 4], [4, 3]]\n    Then, we get a condensed distance matrix from this dataset:\n    &gt;&gt;&gt; y = pdist(X)\n    Finally, we can perform the clustering:\n    &gt;&gt;&gt; Z = complete(y)\n    &gt;&gt;&gt; Z\n    array([[ 0.        ,  1.        ,  1.        ,  2.        ],\n           [ 3.        ,  4.        ,  1.        ,  2.        ],\n           [ 6.        ,  7.        ,  1.        ,  2.        ],\n           [ 9.        , 10.        ,  1.        ,  2.        ],\n           [ 2.        , 12.        ,  1.41421356,  3.        ],\n           [ 5.        , 13.        ,  1.41421356,  3.        ],\n           [ 8.        , 14.        ,  1.41421356,  3.        ],\n           [11.        , 15.        ,  1.41421356,  3.        ],\n           [16.        , 17.        ,  4.12310563,  6.        ],\n           [18.        , 19.        ,  4.12310563,  6.        ],\n           [20.        , 21.        ,  5.65685425, 12.        ]])\n    The linkage matrix ``Z`` represents a dendrogram - see\n    `scipy.cluster.hierarchy.linkage` for a detailed explanation of its\n    contents.\n    We can use `scipy.cluster.hierarchy.fcluster` to see to which cluster\n    each initial point would belong given a distance threshold:\n    &gt;&gt;&gt; fcluster(Z, 0.9, criterion='distance')\n    array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12], dtype=int32)\n    &gt;&gt;&gt; fcluster(Z, 1.5, criterion='distance')\n    array([1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4], dtype=int32)\n    &gt;&gt;&gt; fcluster(Z, 4.5, criterion='distance')\n    array([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2], dtype=int32)\n    &gt;&gt;&gt; fcluster(Z, 6, criterion='distance')\n    array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)\n    Also, `scipy.cluster.hierarchy.dendrogram` can be used to generate a\n    plot of the dendrogram.\n    \"\"\"\n    return linkage(y, method='complete', metric='euclidean')\n\n\ndef average(y):\n    \"\"\"\n    Perform average/UPGMA linkage on a condensed distance matrix.\n    Parameters\n    ----------\n    y : ndarray\n        The upper triangular of the distance matrix. The result of\n        ``pdist`` is returned in this form.\n    Returns\n    -------\n    Z : ndarray\n        A linkage matrix containing the hierarchical clustering. See\n        `linkage` for more information on its structure.\n    See Also\n    --------\n    linkage: for advanced creation of hierarchical clusterings.\n    scipy.spatial.distance.pdist : pairwise distance metrics\n    Examples\n    --------\n    &gt;&gt;&gt; from scipy.cluster.hierarchy import average, fcluster\n    &gt;&gt;&gt; from scipy.spatial.distance import pdist\n    First, we need a toy dataset to play with::\n        x x    x x\n        x        x\n        x        x\n        x x    x x\n    &gt;&gt;&gt; X = [[0, 0], [0, 1], [1, 0],\n    ...      [0, 4], [0, 3], [1, 4],\n    ...      [4, 0], [3, 0], [4, 1],\n    ...      [4, 4], [3, 4], [4, 3]]\n    Then, we get a condensed distance matrix from this dataset:\n    &gt;&gt;&gt; y = pdist(X)\n    Finally, we can perform the clustering:\n    &gt;&gt;&gt; Z = average(y)\n    &gt;&gt;&gt; Z\n    array([[ 0.        ,  1.        ,  1.        ,  2.        ],\n           [ 3.        ,  4.        ,  1.        ,  2.        ],\n           [ 6.        ,  7.        ,  1.        ,  2.        ],\n           [ 9.        , 10.        ,  1.        ,  2.        ],\n           [ 2.        , 12.        ,  1.20710678,  3.        ],\n           [ 5.        , 13.        ,  1.20710678,  3.        ],\n           [ 8.        , 14.        ,  1.20710678,  3.        ],\n           [11.        , 15.        ,  1.20710678,  3.        ],\n           [16.        , 17.        ,  3.39675184,  6.        ],\n           [18.        , 19.        ,  3.39675184,  6.        ],\n           [20.        , 21.        ,  4.09206523, 12.        ]])\n    The linkage matrix ``Z`` represents a dendrogram - see\n    `scipy.cluster.hierarchy.linkage` for a detailed explanation of its\n    contents.\n    We can use `scipy.cluster.hierarchy.fcluster` to see to which cluster\n    each initial point would belong given a distance threshold:\n    &gt;&gt;&gt; fcluster(Z, 0.9, criterion='distance')\n    array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12], dtype=int32)\n    &gt;&gt;&gt; fcluster(Z, 1.5, criterion='distance')\n    array([1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4], dtype=int32)\n    &gt;&gt;&gt; fcluster(Z, 4, criterion='distance')\n    array([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2], dtype=int32)\n    &gt;&gt;&gt; fcluster(Z, 6, criterion='distance')\n    array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)\n    Also, `scipy.cluster.hierarchy.dendrogram` can be used to generate a\n    plot of the dendrogram.\n    \"\"\"\n    return linkage(y, method='average', metric='euclidean')\n\n\ndef weighted(y):\n    \"\"\"\n    Perform weighted/WPGMA linkage on the condensed distance matrix.\n    See `linkage` for more information on the return\n    structure and algorithm.\n    Parameters\n    ----------\n    y : ndarray\n        The upper triangular of the distance matrix. The result of\n        ``pdist`` is returned in this form.\n    Returns\n    -------\n    Z : ndarray\n        A linkage matrix containing the hierarchical clustering. See\n        `linkage` for more information on its structure.\n    See Also\n    --------\n    linkage : for advanced creation of hierarchical clusterings.\n    scipy.spatial.distance.pdist : pairwise distance metrics\n    Examples\n    --------\n    &gt;&gt;&gt; from scipy.cluster.hierarchy import weighted, fcluster\n    &gt;&gt;&gt; from scipy.spatial.distance import pdist\n    First, we need a toy dataset to play with::\n        x x    x x\n        x        x\n        x        x\n        x x    x x\n    &gt;&gt;&gt; X = [[0, 0], [0, 1], [1, 0],\n    ...      [0, 4], [0, 3], [1, 4],\n    ...      [4, 0], [3, 0], [4, 1],\n    ...      [4, 4], [3, 4], [4, 3]]\n    Then, we get a condensed distance matrix from this dataset:\n    &gt;&gt;&gt; y = pdist(X)\n    Finally, we can perform the clustering:\n    &gt;&gt;&gt; Z = weighted(y)\n    &gt;&gt;&gt; Z\n    array([[ 0.        ,  1.        ,  1.        ,  2.        ],\n           [ 6.        ,  7.        ,  1.        ,  2.        ],\n           [ 3.        ,  4.        ,  1.        ,  2.        ],\n           [ 9.        , 11.        ,  1.        ,  2.        ],\n           [ 2.        , 12.        ,  1.20710678,  3.        ],\n           [ 8.        , 13.        ,  1.20710678,  3.        ],\n           [ 5.        , 14.        ,  1.20710678,  3.        ],\n           [10.        , 15.        ,  1.20710678,  3.        ],\n           [18.        , 19.        ,  3.05595762,  6.        ],\n           [16.        , 17.        ,  3.32379407,  6.        ],\n           [20.        , 21.        ,  4.06357713, 12.        ]])\n    The linkage matrix ``Z`` represents a dendrogram - see\n    `scipy.cluster.hierarchy.linkage` for a detailed explanation of its\n    contents.\n    We can use `scipy.cluster.hierarchy.fcluster` to see to which cluster\n    each initial point would belong given a distance threshold:\n    &gt;&gt;&gt; fcluster(Z, 0.9, criterion='distance')\n    array([ 7,  8,  9,  1,  2,  3, 10, 11, 12,  4,  6,  5], dtype=int32)\n    &gt;&gt;&gt; fcluster(Z, 1.5, criterion='distance')\n    array([3, 3, 3, 1, 1, 1, 4, 4, 4, 2, 2, 2], dtype=int32)\n    &gt;&gt;&gt; fcluster(Z, 4, criterion='distance')\n    array([2, 2, 2, 1, 1, 1, 2, 2, 2, 1, 1, 1], dtype=int32)\n    &gt;&gt;&gt; fcluster(Z, 6, criterion='distance')\n    array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)\n    Also, `scipy.cluster.hierarchy.dendrogram` can be used to generate a\n    plot of the dendrogram.\n    \"\"\"\n    return linkage(y, method='weighted', metric='euclidean')\n\n\ndef centroid(y):\n    \"\"\"\n    Perform centroid/UPGMC linkage.\n    See `linkage` for more information on the input matrix,\n    return structure, and algorithm.\n    The following are common calling conventions:\n    1. ``Z = centroid(y)``\n       Performs centroid/UPGMC linkage on the condensed distance\n       matrix ``y``.\n    2. ``Z = centroid(X)``\n       Performs centroid/UPGMC linkage on the observation matrix ``X``\n       using Euclidean distance as the distance metric.\n    Parameters\n    ----------\n    y : ndarray\n        A condensed distance matrix. A condensed\n        distance matrix is a flat array containing the upper\n        triangular of the distance matrix. This is the form that\n        ``pdist`` returns. Alternatively, a collection of\n        m observation vectors in n dimensions may be passed as\n        an m by n array.\n    Returns\n    -------\n    Z : ndarray\n        A linkage matrix containing the hierarchical clustering. See\n        the `linkage` function documentation for more information\n        on its structure.\n    See Also\n    --------\n    linkage: for advanced creation of hierarchical clusterings.\n    scipy.spatial.distance.pdist : pairwise distance metrics\n    Examples\n    --------\n    &gt;&gt;&gt; from scipy.cluster.hierarchy import centroid, fcluster\n    &gt;&gt;&gt; from scipy.spatial.distance import pdist\n    First, we need a toy dataset to play with::\n        x x    x x\n        x        x\n        x        x\n        x x    x x\n    &gt;&gt;&gt; X = [[0, 0], [0, 1], [1, 0],\n    ...      [0, 4], [0, 3], [1, 4],\n    ...      [4, 0], [3, 0], [4, 1],\n    ...      [4, 4], [3, 4], [4, 3]]\n    Then, we get a condensed distance matrix from this dataset:\n    &gt;&gt;&gt; y = pdist(X)\n    Finally, we can perform the clustering:\n    &gt;&gt;&gt; Z = centroid(y)\n    &gt;&gt;&gt; Z\n    array([[ 0.        ,  1.        ,  1.        ,  2.        ],\n           [ 3.        ,  4.        ,  1.        ,  2.        ],\n           [ 9.        , 10.        ,  1.        ,  2.        ],\n           [ 6.        ,  7.        ,  1.        ,  2.        ],\n           [ 2.        , 12.        ,  1.11803399,  3.        ],\n           [ 5.        , 13.        ,  1.11803399,  3.        ],\n           [ 8.        , 15.        ,  1.11803399,  3.        ],\n           [11.        , 14.        ,  1.11803399,  3.        ],\n           [18.        , 19.        ,  3.33333333,  6.        ],\n           [16.        , 17.        ,  3.33333333,  6.        ],\n           [20.        , 21.        ,  3.33333333, 12.        ]])\n    The linkage matrix ``Z`` represents a dendrogram - see\n    `scipy.cluster.hierarchy.linkage` for a detailed explanation of its\n    contents.\n    We can use `scipy.cluster.hierarchy.fcluster` to see to which cluster\n    each initial point would belong given a distance threshold:\n    &gt;&gt;&gt; fcluster(Z, 0.9, criterion='distance')\n    array([ 7,  8,  9, 10, 11, 12,  1,  2,  3,  4,  5,  6], dtype=int32)\n    &gt;&gt;&gt; fcluster(Z, 1.1, criterion='distance')\n    array([5, 5, 6, 7, 7, 8, 1, 1, 2, 3, 3, 4], dtype=int32)\n    &gt;&gt;&gt; fcluster(Z, 2, criterion='distance')\n    array([3, 3, 3, 4, 4, 4, 1, 1, 1, 2, 2, 2], dtype=int32)\n    &gt;&gt;&gt; fcluster(Z, 4, criterion='distance')\n    array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)\n    Also, `scipy.cluster.hierarchy.dendrogram` can be used to generate a\n    plot of the dendrogram.\n    \"\"\"\n    return linkage(y, method='centroid', metric='euclidean')\n\n\ndef median(y):\n    \"\"\"\n    Perform median/WPGMC linkage.\n    See `linkage` for more information on the return structure\n    and algorithm.\n     The following are common calling conventions:\n     1. ``Z = median(y)``\n        Performs median/WPGMC linkage on the condensed distance matrix\n        ``y``.  See ``linkage`` for more information on the return\n        structure and algorithm.\n     2. ``Z = median(X)``\n        Performs median/WPGMC linkage on the observation matrix ``X``\n        using Euclidean distance as the distance metric. See `linkage`\n        for more information on the return structure and algorithm.\n    Parameters\n    ----------\n    y : ndarray\n        A condensed distance matrix. A condensed\n        distance matrix is a flat array containing the upper\n        triangular of the distance matrix. This is the form that\n        ``pdist`` returns.  Alternatively, a collection of\n        m observation vectors in n dimensions may be passed as\n        an m by n array.\n    Returns\n    -------\n    Z : ndarray\n        The hierarchical clustering encoded as a linkage matrix.\n    See Also\n    --------\n    linkage: for advanced creation of hierarchical clusterings.\n    scipy.spatial.distance.pdist : pairwise distance metrics\n    Examples\n    --------\n    &gt;&gt;&gt; from scipy.cluster.hierarchy import median, fcluster\n    &gt;&gt;&gt; from scipy.spatial.distance import pdist\n    First, we need a toy dataset to play with::\n        x x    x x\n        x        x\n        x        x\n        x x    x x\n    &gt;&gt;&gt; X = [[0, 0], [0, 1], [1, 0],\n    ...      [0, 4], [0, 3], [1, 4],\n    ...      [4, 0], [3, 0], [4, 1],\n    ...      [4, 4], [3, 4], [4, 3]]\n    Then, we get a condensed distance matrix from this dataset:\n    &gt;&gt;&gt; y = pdist(X)\n    Finally, we can perform the clustering:\n    &gt;&gt;&gt; Z = median(y)\n    &gt;&gt;&gt; Z\n    array([[ 0.        ,  1.        ,  1.        ,  2.        ],\n           [ 3.        ,  4.        ,  1.        ,  2.        ],\n           [ 9.        , 10.        ,  1.        ,  2.        ],\n           [ 6.        ,  7.        ,  1.        ,  2.        ],\n           [ 2.        , 12.        ,  1.11803399,  3.        ],\n           [ 5.        , 13.        ,  1.11803399,  3.        ],\n           [ 8.        , 15.        ,  1.11803399,  3.        ],\n           [11.        , 14.        ,  1.11803399,  3.        ],\n           [18.        , 19.        ,  3.        ,  6.        ],\n           [16.        , 17.        ,  3.5       ,  6.        ],\n           [20.        , 21.        ,  3.25      , 12.        ]])\n    The linkage matrix ``Z`` represents a dendrogram - see\n    `scipy.cluster.hierarchy.linkage` for a detailed explanation of its\n    contents.\n    We can use `scipy.cluster.hierarchy.fcluster` to see to which cluster\n    each initial point would belong given a distance threshold:\n    &gt;&gt;&gt; fcluster(Z, 0.9, criterion='distance')\n    array([ 7,  8,  9, 10, 11, 12,  1,  2,  3,  4,  5,  6], dtype=int32)\n    &gt;&gt;&gt; fcluster(Z, 1.1, criterion='distance')\n    array([5, 5, 6, 7, 7, 8, 1, 1, 2, 3, 3, 4], dtype=int32)\n    &gt;&gt;&gt; fcluster(Z, 2, criterion='distance')\n    array([3, 3, 3, 4, 4, 4, 1, 1, 1, 2, 2, 2], dtype=int32)\n    &gt;&gt;&gt; fcluster(Z, 4, criterion='distance')\n    array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)\n    Also, `scipy.cluster.hierarchy.dendrogram` can be used to generate a\n    plot of the dendrogram.\n    \"\"\"\n    return linkage(y, method='median', metric='euclidean')\n\n\ndef ward(y):\n    \"\"\"\n    Perform Ward's linkage on a condensed distance matrix.\n    See `linkage` for more information on the return structure\n    and algorithm.\n    The following are common calling conventions:\n    1. ``Z = ward(y)``\n       Performs Ward's linkage on the condensed distance matrix ``y``.\n    2. ``Z = ward(X)``\n       Performs Ward's linkage on the observation matrix ``X`` using\n       Euclidean distance as the distance metric.\n    Parameters\n    ----------\n    y : ndarray\n        A condensed distance matrix. A condensed\n        distance matrix is a flat array containing the upper\n        triangular of the distance matrix. This is the form that\n        ``pdist`` returns.  Alternatively, a collection of\n        m observation vectors in n dimensions may be passed as\n        an m by n array.\n    Returns\n    -------\n    Z : ndarray\n        The hierarchical clustering encoded as a linkage matrix. See\n        `linkage` for more information on the return structure and\n        algorithm.\n    See Also\n    --------\n    linkage: for advanced creation of hierarchical clusterings.\n    scipy.spatial.distance.pdist : pairwise distance metrics\n    Examples\n    --------\n    &gt;&gt;&gt; from scipy.cluster.hierarchy import ward, fcluster\n    &gt;&gt;&gt; from scipy.spatial.distance import pdist\n    First, we need a toy dataset to play with::\n        x x    x x\n        x        x\n        x        x\n        x x    x x\n    &gt;&gt;&gt; X = [[0, 0], [0, 1], [1, 0],\n    ...      [0, 4], [0, 3], [1, 4],\n    ...      [4, 0], [3, 0], [4, 1],\n    ...      [4, 4], [3, 4], [4, 3]]\n    Then, we get a condensed distance matrix from this dataset:\n    &gt;&gt;&gt; y = pdist(X)\n    Finally, we can perform the clustering:\n    &gt;&gt;&gt; Z = ward(y)\n    &gt;&gt;&gt; Z\n    array([[ 0.        ,  1.        ,  1.        ,  2.        ],\n           [ 3.        ,  4.        ,  1.        ,  2.        ],\n           [ 6.        ,  7.        ,  1.        ,  2.        ],\n           [ 9.        , 10.        ,  1.        ,  2.        ],\n           [ 2.        , 12.        ,  1.29099445,  3.        ],\n           [ 5.        , 13.        ,  1.29099445,  3.        ],\n           [ 8.        , 14.        ,  1.29099445,  3.        ],\n           [11.        , 15.        ,  1.29099445,  3.        ],\n           [16.        , 17.        ,  5.77350269,  6.        ],\n           [18.        , 19.        ,  5.77350269,  6.        ],\n           [20.        , 21.        ,  8.16496581, 12.        ]])\n    The linkage matrix ``Z`` represents a dendrogram - see\n    `scipy.cluster.hierarchy.linkage` for a detailed explanation of its\n    contents.\n    We can use `scipy.cluster.hierarchy.fcluster` to see to which cluster\n    each initial point would belong given a distance threshold:\n    &gt;&gt;&gt; fcluster(Z, 0.9, criterion='distance')\n    array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12], dtype=int32)\n    &gt;&gt;&gt; fcluster(Z, 1.1, criterion='distance')\n    array([1, 1, 2, 3, 3, 4, 5, 5, 6, 7, 7, 8], dtype=int32)\n    &gt;&gt;&gt; fcluster(Z, 3, criterion='distance')\n    array([1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4], dtype=int32)\n    &gt;&gt;&gt; fcluster(Z, 9, criterion='distance')\n    array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)\n    Also, `scipy.cluster.hierarchy.dendrogram` can be used to generate a\n    plot of the dendrogram.\n    \"\"\"\n    return linkage(y, method='ward', metric='euclidean')\n\n\ndef linkage(y, method='single', metric='euclidean', optimal_ordering=False):\n    \"\"\"\n    Perform hierarchical/agglomerative clustering.\n    The input y may be either a 1-D condensed distance matrix\n    or a 2-D array of observation vectors.\n    If y is a 1-D condensed distance matrix,\n    then y must be a :math:`\\\\binom{n}{2}` sized\n    vector, where n is the number of original observations paired\n    in the distance matrix. The behavior of this function is very\n    similar to the MATLAB linkage function.\n    A :math:`(n-1)` by 4 matrix ``Z`` is returned. At the\n    :math:`i`-th iteration, clusters with indices ``Z[i, 0]`` and\n    ``Z[i, 1]`` are combined to form cluster :math:`n + i`. A\n    cluster with an index less than :math:`n` corresponds to one of\n    the :math:`n` original observations. The distance between\n    clusters ``Z[i, 0]`` and ``Z[i, 1]`` is given by ``Z[i, 2]``. The\n    fourth value ``Z[i, 3]`` represents the number of original\n    observations in the newly formed cluster.\n    The following linkage methods are used to compute the distance\n    :math:`d(s, t)` between two clusters :math:`s` and\n    :math:`t`. The algorithm begins with a forest of clusters that\n    have yet to be used in the hierarchy being formed. When two\n    clusters :math:`s` and :math:`t` from this forest are combined\n    into a single cluster :math:`u`, :math:`s` and :math:`t` are\n    removed from the forest, and :math:`u` is added to the\n    forest. When only one cluster remains in the forest, the algorithm\n    stops, and this cluster becomes the root.\n    A distance matrix is maintained at each iteration. The ``d[i,j]``\n    entry corresponds to the distance between cluster :math:`i` and\n    :math:`j` in the original forest.\n    At each iteration, the algorithm must update the distance matrix\n    to reflect the distance of the newly formed cluster u with the\n    remaining clusters in the forest.\n    Suppose there are :math:`|u|` original observations\n    :math:`u[0], \\\\ldots, u[|u|-1]` in cluster :math:`u` and\n    :math:`|v|` original objects :math:`v[0], \\\\ldots, v[|v|-1]` in\n    cluster :math:`v`. Recall, :math:`s` and :math:`t` are\n    combined to form cluster :math:`u`. Let :math:`v` be any\n    remaining cluster in the forest that is not :math:`u`.\n    The following are methods for calculating the distance between the\n    newly formed cluster :math:`u` and each :math:`v`.\n      * method='single' assigns\n        .. math::\n           d(u,v) = \\\\min(dist(u[i],v[j]))\n        for all points :math:`i` in cluster :math:`u` and\n        :math:`j` in cluster :math:`v`. This is also known as the\n        Nearest Point Algorithm.\n      * method='complete' assigns\n        .. math::\n           d(u, v) = \\\\max(dist(u[i],v[j]))\n        for all points :math:`i` in cluster u and :math:`j` in\n        cluster :math:`v`. This is also known by the Farthest Point\n        Algorithm or Voor Hees Algorithm.\n      * method='average' assigns\n        .. math::\n           d(u,v) = \\\\sum_{ij} \\\\frac{d(u[i], v[j])}\n                                   {(|u|*|v|)}\n        for all points :math:`i` and :math:`j` where :math:`|u|`\n        and :math:`|v|` are the cardinalities of clusters :math:`u`\n        and :math:`v`, respectively. This is also called the UPGMA\n        algorithm.\n      * method='weighted' assigns\n        .. math::\n           d(u,v) = (dist(s,v) + dist(t,v))/2\n        where cluster u was formed with cluster s and t and v\n        is a remaining cluster in the forest (also called WPGMA).\n      * method='centroid' assigns\n        .. math::\n           dist(s,t) = ||c_s-c_t||_2\n        where :math:`c_s` and :math:`c_t` are the centroids of\n        clusters :math:`s` and :math:`t`, respectively. When two\n        clusters :math:`s` and :math:`t` are combined into a new\n        cluster :math:`u`, the new centroid is computed over all the\n        original objects in clusters :math:`s` and :math:`t`. The\n        distance then becomes the Euclidean distance between the\n        centroid of :math:`u` and the centroid of a remaining cluster\n        :math:`v` in the forest. This is also known as the UPGMC\n        algorithm.\n      * method='median' assigns :math:`d(s,t)` like the ``centroid``\n        method. When two clusters :math:`s` and :math:`t` are combined\n        into a new cluster :math:`u`, the average of centroids s and t\n        give the new centroid :math:`u`. This is also known as the\n        WPGMC algorithm.\n      * method='ward' uses the Ward variance minimization algorithm.\n        The new entry :math:`d(u,v)` is computed as follows,\n        .. math::\n           d(u,v) = \\\\sqrt{\\\\frac{|v|+|s|}\n                               {T}d(v,s)^2\n                        + \\\\frac{|v|+|t|}\n                               {T}d(v,t)^2\n                        - \\\\frac{|v|}\n                               {T}d(s,t)^2}\n        where :math:`u` is the newly joined cluster consisting of\n        clusters :math:`s` and :math:`t`, :math:`v` is an unused\n        cluster in the forest, :math:`T=|v|+|s|+|t|`, and\n        :math:`|*|` is the cardinality of its argument. This is also\n        known as the incremental algorithm.\n    Warning: When the minimum distance pair in the forest is chosen, there\n    may be two or more pairs with the same minimum distance. This\n    implementation may choose a different minimum than the MATLAB\n    version.\n    Parameters\n    ----------\n    y : ndarray\n        A condensed distance matrix. A condensed distance matrix\n        is a flat array containing the upper triangular of the distance matrix.\n        This is the form that ``pdist`` returns. Alternatively, a collection of\n        :math:`m` observation vectors in :math:`n` dimensions may be passed as\n        an :math:`m` by :math:`n` array. All elements of the condensed distance\n        matrix must be finite, i.e., no NaNs or infs.\n    method : str, optional\n        The linkage algorithm to use. See the ``Linkage Methods`` section below\n        for full descriptions.\n    metric : str or function, optional\n        The distance metric to use in the case that y is a collection of\n        observation vectors; ignored otherwise. See the ``pdist``\n        function for a list of valid distance metrics. A custom distance\n        function can also be used.\n    optimal_ordering : bool, optional\n        If True, the linkage matrix will be reordered so that the distance\n        between successive leaves is minimal. This results in a more intuitive\n        tree structure when the data are visualized. defaults to False, because\n        this algorithm can be slow, particularly on large datasets [2]_. See\n        also the `optimal_leaf_ordering` function.\n        .. versionadded:: 1.0.0\n    Returns\n    -------\n    Z : ndarray\n        The hierarchical clustering encoded as a linkage matrix.\n    Notes\n    -----\n    1. For method 'single', an optimized algorithm based on minimum spanning\n       tree is implemented. It has time complexity :math:`O(n^2)`.\n       For methods 'complete', 'average', 'weighted' and 'ward', an algorithm\n       called nearest-neighbors chain is implemented. It also has time\n       complexity :math:`O(n^2)`.\n       For other methods, a naive algorithm is implemented with :math:`O(n^3)`\n       time complexity.\n       All algorithms use :math:`O(n^2)` memory.\n       Refer to [1]_ for details about the algorithms.\n    2. Methods 'centroid', 'median', and 'ward' are correctly defined only if\n       Euclidean pairwise metric is used. If `y` is passed as precomputed\n       pairwise distances, then it is the user's responsibility to assure that\n       these distances are in fact Euclidean, otherwise the produced result\n       will be incorrect.\n    See Also\n    --------\n    scipy.spatial.distance.pdist : pairwise distance metrics\n    References\n    ----------\n    .. [1] Daniel Mullner, \"Modern hierarchical, agglomerative clustering\n           algorithms\", :arXiv:`1109.2378v1`.\n    .. [2] Ziv Bar-Joseph, David K. Gifford, Tommi S. Jaakkola, \"Fast optimal\n           leaf ordering for hierarchical clustering\", 2001. Bioinformatics\n           :doi:`10.1093/bioinformatics/17.suppl_1.S22`\n    Examples\n    --------\n    &gt;&gt;&gt; from scipy.cluster.hierarchy import dendrogram, linkage\n    &gt;&gt;&gt; from matplotlib import pyplot as plt\n    &gt;&gt;&gt; X = [[i] for i in [2, 8, 0, 4, 1, 9, 9, 0]]\n    &gt;&gt;&gt; Z = linkage(X, 'ward')\n    &gt;&gt;&gt; fig = plt.figure(figsize=(25, 10))\n    &gt;&gt;&gt; dn = dendrogram(Z)\n    &gt;&gt;&gt; Z = linkage(X, 'single')\n    &gt;&gt;&gt; fig = plt.figure(figsize=(25, 10))\n    &gt;&gt;&gt; dn = dendrogram(Z)\n    &gt;&gt;&gt; plt.show()\n    \"\"\"\n    if method not in _LINKAGE_METHODS:\n        raise ValueError(\"Invalid method: {0}\".format(method))\n\n    y = _convert_to_double(np.asarray(y, order='c'))\n\n    if y.ndim == 1:\n        distance.is_valid_y(y, throw=True, name='y')\n        [y] = _copy_arrays_if_base_present([y])\n    elif y.ndim == 2:\n        if method in _EUCLIDEAN_METHODS and metric != 'euclidean':\n            raise ValueError(\"Method '{0}' requires the distance metric \"\n                             \"to be Euclidean\".format(method))\n        if y.shape[0] == y.shape[1] and np.allclose(np.diag(y), 0):\n            if np.all(y &gt;= 0) and np.allclose(y, y.T):\n                _warning('The symmetric non-negative hollow observation '\n                         'matrix looks suspiciously like an uncondensed '\n                         'distance matrix')\n        y = distance.pdist(y, metric)\n    else:\n        raise ValueError(\"`y` must be 1 or 2 dimensional.\")\n\n    if not np.all(np.isfinite(y)):\n        raise ValueError(\"The condensed distance matrix must contain only \"\n                         \"finite values.\")\n\n    n = int(distance.num_obs_y(y))\n    method_code = _LINKAGE_METHODS[method]\n\n    if method == 'single':\n        result = _hierarchy.mst_single_linkage(y, n)\n    elif method in ['complete', 'average', 'weighted', 'ward']:\n        result = _hierarchy.nn_chain(y, n, method_code)\n    else:\n        result = _hierarchy.fast_linkage(y, n, method_code)\n\n    if optimal_ordering:\n        return optimal_leaf_ordering(result, y)\n    else:\n        return result\n\n\nclass ClusterNode(object):\n    \"\"\"\n    A tree node class for representing a cluster.\n    Leaf nodes correspond to original observations, while non-leaf nodes\n    correspond to non-singleton clusters.\n    The `to_tree` function converts a matrix returned by the linkage\n    function into an easy-to-use tree representation.\n    All parameter names are also attributes.\n    Parameters\n    ----------\n    id : int\n        The node id.\n    left : ClusterNode instance, optional\n        The left child tree node.\n    right : ClusterNode instance, optional\n        The right child tree node.\n    dist : float, optional\n        Distance for this cluster in the linkage matrix.\n    count : int, optional\n        The number of samples in this cluster.\n    See Also\n    --------\n    to_tree : for converting a linkage matrix ``Z`` into a tree object.\n    \"\"\"\n\n    def __init__(self, id, left=None, right=None, dist=0, count=1):\n        if id &lt; 0:\n            raise ValueError('The id must be non-negative.')\n        if dist &lt; 0:\n            raise ValueError('The distance must be non-negative.')\n        if (left is None and right is not None) or \\\n                (left is not None and right is None):\n            raise ValueError('Only full or proper binary trees are permitted.'\n                             '  This node has one child.')\n        if count &lt; 1:\n            raise ValueError('A cluster must contain at least one original '\n                             'observation.')\n        self.id = id\n        self.left = left\n        self.right = right\n        self.dist = dist\n        if self.left is None:\n            self.count = count\n        else:\n            self.count = left.count + right.count\n\n    def __lt__(self, node):\n        if not isinstance(node, ClusterNode):\n            raise ValueError(\"Can't compare ClusterNode \"\n                             \"to type {}\".format(type(node)))\n        return self.dist &lt; node.dist\n\n    def __gt__(self, node):\n        if not isinstance(node, ClusterNode):\n            raise ValueError(\"Can't compare ClusterNode \"\n                             \"to type {}\".format(type(node)))\n        return self.dist &gt; node.dist\n\n    def __eq__(self, node):\n        if not isinstance(node, ClusterNode):\n            raise ValueError(\"Can't compare ClusterNode \"\n                             \"to type {}\".format(type(node)))\n        return self.dist == node.dist\n\n    def get_id(self):\n        \"\"\"\n        The identifier of the target node.\n        For ``0 &lt;= i &lt; n``, `i` corresponds to original observation i.\n        For ``n &lt;= i &lt; 2n-1``, `i` corresponds to non-singleton cluster formed\n        at iteration ``i-n``.\n        Returns\n        -------\n        id : int\n            The identifier of the target node.\n        \"\"\"\n        return self.id\n\n    def get_count(self):\n        \"\"\"\n        The number of leaf nodes (original observations) belonging to\n        the cluster node nd. If the target node is a leaf, 1 is\n        returned.\n        Returns\n        -------\n        get_count : int\n            The number of leaf nodes below the target node.\n        \"\"\"\n        return self.count\n\n    def get_left(self):\n        \"\"\"\n        Return a reference to the left child tree object.\n        Returns\n        -------\n        left : ClusterNode\n            The left child of the target node. If the node is a leaf,\n            None is returned.\n        \"\"\"\n        return self.left\n\n    def get_right(self):\n        \"\"\"\n        Return a reference to the right child tree object.\n        Returns\n        -------\n        right : ClusterNode\n            The left child of the target node. If the node is a leaf,\n            None is returned.\n        \"\"\"\n        return self.right\n\n    def is_leaf(self):\n        \"\"\"\n        Return True if the target node is a leaf.\n        Returns\n        -------\n        leafness : bool\n            True if the target node is a leaf node.\n        \"\"\"\n        return self.left is None\n\n    def pre_order(self, func=(lambda x: x.id)):\n        \"\"\"\n        Perform pre-order traversal without recursive function calls.\n        When a leaf node is first encountered, ``func`` is called with\n        the leaf node as its argument, and its result is appended to\n        the list.\n        For example, the statement::\n           ids = root.pre_order(lambda x: x.id)\n        returns a list of the node ids corresponding to the leaf nodes\n        of the tree as they appear from left to right.\n        Parameters\n        ----------\n        func : function\n            Applied to each leaf ClusterNode object in the pre-order traversal.\n            Given the ``i``-th leaf node in the pre-order traversal ``n[i]``,\n            the result of ``func(n[i])`` is stored in ``L[i]``. If not\n            provided, the index of the original observation to which the node\n            corresponds is used.\n        Returns\n        -------\n        L : list\n            The pre-order traversal.\n        \"\"\"\n        # Do a preorder traversal, caching the result. To avoid having to do\n        # recursion, we'll store the previous index we've visited in a vector.\n        n = self.count\n\n        curNode = [None] * (2 * n)\n        lvisited = set()\n        rvisited = set()\n        curNode[0] = self\n        k = 0\n        preorder = []\n        while k &gt;= 0:\n            nd = curNode[k]\n            ndid = nd.id\n            if nd.is_leaf():\n                preorder.append(func(nd))\n                k = k - 1\n            else:\n                if ndid not in lvisited:\n                    curNode[k + 1] = nd.left\n                    lvisited.add(ndid)\n                    k = k + 1\n                elif ndid not in rvisited:\n                    curNode[k + 1] = nd.right\n                    rvisited.add(ndid)\n                    k = k + 1\n                # If we've visited the left and right of this non-leaf\n                # node already, go up in the tree.\n                else:\n                    k = k - 1\n\n        return preorder\n\n\n_cnode_bare = ClusterNode(0)\n_cnode_type = type(ClusterNode)\n\n\ndef _order_cluster_tree(Z):\n    \"\"\"\n    Return clustering nodes in bottom-up order by distance.\n    Parameters\n    ----------\n    Z : scipy.cluster.linkage array\n        The linkage matrix.\n    Returns\n    -------\n    nodes : list\n        A list of ClusterNode objects.\n    \"\"\"\n    q = deque()\n    tree = to_tree(Z)\n    q.append(tree)\n    nodes = []\n\n    while q:\n        node = q.popleft()\n        if not node.is_leaf():\n            bisect.insort_left(nodes, node)\n            q.append(node.get_right())\n            q.append(node.get_left())\n    return nodes\n\n\ndef cut_tree(Z, n_clusters=None, height=None):\n    \"\"\"\n    Given a linkage matrix Z, return the cut tree.\n    Parameters\n    ----------\n    Z : scipy.cluster.linkage array\n        The linkage matrix.\n    n_clusters : array_like, optional\n        Number of clusters in the tree at the cut point.\n    height : array_like, optional\n        The height at which to cut the tree. Only possible for ultrametric\n        trees.\n    Returns\n    -------\n    cutree : array\n        An array indicating group membership at each agglomeration step. I.e.,\n        for a full cut tree, in the first column each data point is in its own\n        cluster. At the next step, two nodes are merged. Finally, all\n        singleton and non-singleton clusters are in one group. If `n_clusters`\n        or `height` are given, the columns correspond to the columns of\n        `n_clusters` or `height`.\n    Examples\n    --------\n    &gt;&gt;&gt; from scipy import cluster\n    &gt;&gt;&gt; np.random.seed(23)\n    &gt;&gt;&gt; X = np.random.randn(50, 4)\n    &gt;&gt;&gt; Z = cluster.hierarchy.ward(X)\n    &gt;&gt;&gt; cutree = cluster.hierarchy.cut_tree(Z, n_clusters=[5, 10])\n    &gt;&gt;&gt; cutree[:10]\n    array([[0, 0],\n           [1, 1],\n           [2, 2],\n           [3, 3],\n           [3, 4],\n           [2, 2],\n           [0, 0],\n           [1, 5],\n           [3, 6],\n           [4, 7]])\n    \"\"\"\n    nobs = num_obs_linkage(Z)\n    nodes = _order_cluster_tree(Z)\n\n    if height is not None and n_clusters is not None:\n        raise ValueError(\"At least one of either height or n_clusters \"\n                         \"must be None\")\n    elif height is None and n_clusters is None:  # return the full cut tree\n        cols_idx = np.arange(nobs)\n    elif height is not None:\n        heights = np.array([x.dist for x in nodes])\n        cols_idx = np.searchsorted(heights, height)\n    else:\n        cols_idx = nobs - np.searchsorted(np.arange(nobs), n_clusters)\n\n    try:\n        n_cols = len(cols_idx)\n    except TypeError:  # scalar\n        n_cols = 1\n        cols_idx = np.array([cols_idx])\n\n    groups = np.zeros((n_cols, nobs), dtype=int)\n    last_group = np.arange(nobs)\n    if 0 in cols_idx:\n        groups[0] = last_group\n\n    for i, node in enumerate(nodes):\n        idx = node.pre_order()\n        this_group = last_group.copy()\n        this_group[idx] = last_group[idx].min()\n        this_group[this_group &gt; last_group[idx].max()] -= 1\n        if i + 1 in cols_idx:\n            groups[np.nonzero(i + 1 == cols_idx)[0]] = this_group\n        last_group = this_group\n\n    return groups.T\n\n\ndef to_tree(Z, rd=False):\n    \"\"\"\n    Convert a linkage matrix into an easy-to-use tree object.\n    The reference to the root `ClusterNode` object is returned (by default).\n    Each `ClusterNode` object has a ``left``, ``right``, ``dist``, ``id``,\n    and ``count`` attribute. The left and right attributes point to\n    ClusterNode objects that were combined to generate the cluster.\n    If both are None then the `ClusterNode` object is a leaf node, its count\n    must be 1, and its distance is meaningless but set to 0.\n    *Note: This function is provided for the convenience of the library\n    user. ClusterNodes are not used as input to any of the functions in this\n    library.*\n    Parameters\n    ----------\n    Z : ndarray\n        The linkage matrix in proper form (see the `linkage`\n        function documentation).\n    rd : bool, optional\n        When False (default), a reference to the root `ClusterNode` object is\n        returned.  Otherwise, a tuple ``(r, d)`` is returned. ``r`` is a\n        reference to the root node while ``d`` is a list of `ClusterNode`\n        objects - one per original entry in the linkage matrix plus entries\n        for all clustering steps. If a cluster id is\n        less than the number of samples ``n`` in the data that the linkage\n        matrix describes, then it corresponds to a singleton cluster (leaf\n        node).\n        See `linkage` for more information on the assignment of cluster ids\n        to clusters.\n    Returns\n    -------\n    tree : ClusterNode or tuple (ClusterNode, list of ClusterNode)\n        If ``rd`` is False, a `ClusterNode`.\n        If ``rd`` is True, a list of length ``2*n - 1``, with ``n`` the number\n        of samples.  See the description of `rd` above for more details.\n    See Also\n    --------\n    linkage, is_valid_linkage, ClusterNode\n    Examples\n    --------\n    &gt;&gt;&gt; from scipy.cluster import hierarchy\n    &gt;&gt;&gt; x = np.random.rand(10).reshape(5, 2)\n    &gt;&gt;&gt; Z = hierarchy.linkage(x)\n    &gt;&gt;&gt; hierarchy.to_tree(Z)\n    &lt;scipy.cluster.hierarchy.ClusterNode object at ...\n    &gt;&gt;&gt; rootnode, nodelist = hierarchy.to_tree(Z, rd=True)\n    &gt;&gt;&gt; rootnode\n    &lt;scipy.cluster.hierarchy.ClusterNode object at ...\n    &gt;&gt;&gt; len(nodelist)\n    9\n    \"\"\"\n    Z = np.asarray(Z, order='c')\n    is_valid_linkage(Z, throw=True, name='Z')\n\n    # Number of original objects is equal to the number of rows minus 1.\n    n = Z.shape[0] + 1\n\n    # Create a list full of None's to store the node objects\n    d = [None] * (n * 2 - 1)\n\n    # Create the nodes corresponding to the n original objects.\n    for i in range(0, n):\n        d[i] = ClusterNode(i)\n\n    nd = None\n\n    for i in range(0, n - 1):\n        fi = int(Z[i, 0])\n        fj = int(Z[i, 1])\n        if fi &gt; i + n:\n            raise ValueError(('Corrupt matrix Z. Index to derivative cluster '\n                              'is used before it is formed. See row %d, '\n                              'column 0') % fi)\n        if fj &gt; i + n:\n            raise ValueError(('Corrupt matrix Z. Index to derivative cluster '\n                              'is used before it is formed. See row %d, '\n                              'column 1') % fj)\n        nd = ClusterNode(i + n, d[fi], d[fj], Z[i, 2])\n        #                 ^ id   ^ left ^ right ^ dist\n        if Z[i, 3] != nd.count:\n            raise ValueError(('Corrupt matrix Z. The count Z[%d,3] is '\n                              'incorrect.') % i)\n        d[n + i] = nd\n\n    if rd:\n        return (nd, d)\n    else:\n        return nd\n\n\ndef optimal_leaf_ordering(Z, y, metric='euclidean'):\n    \"\"\"\n    Given a linkage matrix Z and distance, reorder the cut tree.\n    Parameters\n    ----------\n    Z : ndarray\n        The hierarchical clustering encoded as a linkage matrix. See\n        `linkage` for more information on the return structure and\n        algorithm.\n    y : ndarray\n        The condensed distance matrix from which Z was generated.\n        Alternatively, a collection of m observation vectors in n\n        dimensions may be passed as an m by n array.\n    metric : str or function, optional\n        The distance metric to use in the case that y is a collection of\n        observation vectors; ignored otherwise. See the ``pdist``\n        function for a list of valid distance metrics. A custom distance\n        function can also be used.\n    Returns\n    -------\n    Z_ordered : ndarray\n        A copy of the linkage matrix Z, reordered to minimize the distance\n        between adjacent leaves.\n    Examples\n    --------\n    &gt;&gt;&gt; from scipy.cluster import hierarchy\n    &gt;&gt;&gt; np.random.seed(23)\n    &gt;&gt;&gt; X = np.random.randn(10,10)\n    &gt;&gt;&gt; Z = hierarchy.ward(X)\n    &gt;&gt;&gt; hierarchy.leaves_list(Z)\n    array([0, 5, 3, 9, 6, 8, 1, 4, 2, 7], dtype=int32)\n    &gt;&gt;&gt; hierarchy.leaves_list(hierarchy.optimal_leaf_ordering(Z, X))\n    array([3, 9, 0, 5, 8, 2, 7, 4, 1, 6], dtype=int32)\n    \"\"\"\n    Z = np.asarray(Z, order='c')\n    is_valid_linkage(Z, throw=True, name='Z')\n\n    y = _convert_to_double(np.asarray(y, order='c'))\n\n    if y.ndim == 1:\n        distance.is_valid_y(y, throw=True, name='y')\n        [y] = _copy_arrays_if_base_present([y])\n    elif y.ndim == 2:\n        if y.shape[0] == y.shape[1] and np.allclose(np.diag(y), 0):\n            if np.all(y &gt;= 0) and np.allclose(y, y.T):\n                _warning('The symmetric non-negative hollow observation '\n                         'matrix looks suspiciously like an uncondensed '\n                         'distance matrix')\n        y = distance.pdist(y, metric)\n    else:\n        raise ValueError(\"`y` must be 1 or 2 dimensional.\")\n\n    if not np.all(np.isfinite(y)):\n        raise ValueError(\"The condensed distance matrix must contain only \"\n                         \"finite values.\")\n\n    return _optimal_leaf_ordering.optimal_leaf_ordering(Z, y)\n\n\ndef _convert_to_bool(X):\n    if X.dtype != bool:\n        X = X.astype(bool)\n    if not X.flags.contiguous:\n        X = X.copy()\n    return X\n\n\ndef _convert_to_double(X):\n    if X.dtype != np.double:\n        X = X.astype(np.double)\n    if not X.flags.contiguous:\n        X = X.copy()\n    return X\n\n\ndef cophenet(Z, Y=None):\n    \"\"\"\n    Calculate the cophenetic distances between each observation in\n    the hierarchical clustering defined by the linkage ``Z``.\n    Suppose ``p`` and ``q`` are original observations in\n    disjoint clusters ``s`` and ``t``, respectively and\n    ``s`` and ``t`` are joined by a direct parent cluster\n    ``u``. The cophenetic distance between observations\n    ``i`` and ``j`` is simply the distance between\n    clusters ``s`` and ``t``.\n    Parameters\n    ----------\n    Z : ndarray\n        The hierarchical clustering encoded as an array\n        (see `linkage` function).\n    Y : ndarray (optional)\n        Calculates the cophenetic correlation coefficient ``c`` of a\n        hierarchical clustering defined by the linkage matrix `Z`\n        of a set of :math:`n` observations in :math:`m`\n        dimensions. `Y` is the condensed distance matrix from which\n        `Z` was generated.\n    Returns\n    -------\n    c : ndarray\n        The cophentic correlation distance (if ``Y`` is passed).\n    d : ndarray\n        The cophenetic distance matrix in condensed form. The\n        :math:`ij` th entry is the cophenetic distance between\n        original observations :math:`i` and :math:`j`.\n    See Also\n    --------\n    linkage: for a description of what a linkage matrix is.\n    scipy.spatial.distance.squareform: transforming condensed matrices into square ones.\n    Examples\n    --------\n    &gt;&gt;&gt; from scipy.cluster.hierarchy import single, cophenet\n    &gt;&gt;&gt; from scipy.spatial.distance import pdist, squareform\n    Given a dataset ``X`` and a linkage matrix ``Z``, the cophenetic distance\n    between two points of ``X`` is the distance between the largest two\n    distinct clusters that each of the points:\n    &gt;&gt;&gt; X = [[0, 0], [0, 1], [1, 0],\n    ...      [0, 4], [0, 3], [1, 4],\n    ...      [4, 0], [3, 0], [4, 1],\n    ...      [4, 4], [3, 4], [4, 3]]\n    ``X`` corresponds to this dataset ::\n        x x    x x\n        x        x\n        x        x\n        x x    x x\n    &gt;&gt;&gt; Z = single(pdist(X))\n    &gt;&gt;&gt; Z\n    array([[ 0.,  1.,  1.,  2.],\n           [ 2., 12.,  1.,  3.],\n           [ 3.,  4.,  1.,  2.],\n           [ 5., 14.,  1.,  3.],\n           [ 6.,  7.,  1.,  2.],\n           [ 8., 16.,  1.,  3.],\n           [ 9., 10.,  1.,  2.],\n           [11., 18.,  1.,  3.],\n           [13., 15.,  2.,  6.],\n           [17., 20.,  2.,  9.],\n           [19., 21.,  2., 12.]])\n    &gt;&gt;&gt; cophenet(Z)\n    array([1., 1., 2., 2., 2., 2., 2., 2., 2., 2., 2., 1., 2., 2., 2., 2., 2.,\n           2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 1., 1., 2., 2.,\n           2., 2., 2., 2., 1., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n           1., 1., 2., 2., 2., 1., 2., 2., 2., 2., 2., 2., 1., 1., 1.])\n    The output of the `scipy.cluster.hierarchy.cophenet` method is\n    represented in condensed form. We can use\n    `scipy.spatial.distance.squareform` to see the output as a\n    regular matrix (where each element ``ij`` denotes the cophenetic distance\n    between each ``i``, ``j`` pair of points in ``X``):\n    &gt;&gt;&gt; squareform(cophenet(Z))\n    array([[0., 1., 1., 2., 2., 2., 2., 2., 2., 2., 2., 2.],\n           [1., 0., 1., 2., 2., 2., 2., 2., 2., 2., 2., 2.],\n           [1., 1., 0., 2., 2., 2., 2., 2., 2., 2., 2., 2.],\n           [2., 2., 2., 0., 1., 1., 2., 2., 2., 2., 2., 2.],\n           [2., 2., 2., 1., 0., 1., 2., 2., 2., 2., 2., 2.],\n           [2., 2., 2., 1., 1., 0., 2., 2., 2., 2., 2., 2.],\n           [2., 2., 2., 2., 2., 2., 0., 1., 1., 2., 2., 2.],\n           [2., 2., 2., 2., 2., 2., 1., 0., 1., 2., 2., 2.],\n           [2., 2., 2., 2., 2., 2., 1., 1., 0., 2., 2., 2.],\n           [2., 2., 2., 2., 2., 2., 2., 2., 2., 0., 1., 1.],\n           [2., 2., 2., 2., 2., 2., 2., 2., 2., 1., 0., 1.],\n           [2., 2., 2., 2., 2., 2., 2., 2., 2., 1., 1., 0.]])\n    In this example, the cophenetic distance between points on ``X`` that are\n    very close (i.e., in the same corner) is 1. For other pairs of points is 2,\n    because the points will be located in clusters at different\n    corners - thus, the distance between these clusters will be larger.\n    \"\"\"\n    Z = np.asarray(Z, order='c')\n    is_valid_linkage(Z, throw=True, name='Z')\n    Zs = Z.shape\n    n = Zs[0] + 1\n\n    zz = np.zeros((n * (n-1)) // 2, dtype=np.double)\n    # Since the C code does not support striding using strides.\n    # The dimensions are used instead.\n    Z = _convert_to_double(Z)\n\n    _hierarchy.cophenetic_distances(Z, zz, int(n))\n    if Y is None:\n        return zz\n\n    Y = np.asarray(Y, order='c')\n    distance.is_valid_y(Y, throw=True, name='Y')\n\n    z = zz.mean()\n    y = Y.mean()\n    Yy = Y - y\n    Zz = zz - z\n    numerator = (Yy * Zz)\n    denomA = Yy**2\n    denomB = Zz**2\n    c = numerator.sum() / np.sqrt((denomA.sum() * denomB.sum()))\n    return (c, zz)\n\n\ndef inconsistent(Z, d=2):\n    r\"\"\"\n    Calculate inconsistency statistics on a linkage matrix.\n    Parameters\n    ----------\n    Z : ndarray\n        The :math:`(n-1)` by 4 matrix encoding the linkage (hierarchical\n        clustering).  See `linkage` documentation for more information on its\n        form.\n    d : int, optional\n        The number of links up to `d` levels below each non-singleton cluster.\n    Returns\n    -------\n    R : ndarray\n        A :math:`(n-1)` by 4 matrix where the ``i``'th row contains the link\n        statistics for the non-singleton cluster ``i``. The link statistics are\n        computed over the link heights for links :math:`d` levels below the\n        cluster ``i``. ``R[i,0]`` and ``R[i,1]`` are the mean and standard\n        deviation of the link heights, respectively; ``R[i,2]`` is the number\n        of links included in the calculation; and ``R[i,3]`` is the\n        inconsistency coefficient,\n        .. math:: \\frac{\\mathtt{Z[i,2]} - \\mathtt{R[i,0]}} {R[i,1]}\n    Notes\n    -----\n    This function behaves similarly to the MATLAB(TM) ``inconsistent``\n    function.\n    Examples\n    --------\n    &gt;&gt;&gt; from scipy.cluster.hierarchy import inconsistent, linkage\n    &gt;&gt;&gt; from matplotlib import pyplot as plt\n    &gt;&gt;&gt; X = [[i] for i in [2, 8, 0, 4, 1, 9, 9, 0]]\n    &gt;&gt;&gt; Z = linkage(X, 'ward')\n    &gt;&gt;&gt; print(Z)\n    [[ 5.          6.          0.          2.        ]\n     [ 2.          7.          0.          2.        ]\n     [ 0.          4.          1.          2.        ]\n     [ 1.          8.          1.15470054  3.        ]\n     [ 9.         10.          2.12132034  4.        ]\n     [ 3.         12.          4.11096096  5.        ]\n     [11.         13.         14.07183949  8.        ]]\n    &gt;&gt;&gt; inconsistent(Z)\n    array([[ 0.        ,  0.        ,  1.        ,  0.        ],\n           [ 0.        ,  0.        ,  1.        ,  0.        ],\n           [ 1.        ,  0.        ,  1.        ,  0.        ],\n           [ 0.57735027,  0.81649658,  2.        ,  0.70710678],\n           [ 1.04044011,  1.06123822,  3.        ,  1.01850858],\n           [ 3.11614065,  1.40688837,  2.        ,  0.70710678],\n           [ 6.44583366,  6.76770586,  3.        ,  1.12682288]])\n    \"\"\"\n    Z = np.asarray(Z, order='c')\n\n    Zs = Z.shape\n    is_valid_linkage(Z, throw=True, name='Z')\n    if (not d == np.floor(d)) or d &lt; 0:\n        raise ValueError('The second argument d must be a nonnegative '\n                         'integer value.')\n\n    # Since the C code does not support striding using strides.\n    # The dimensions are used instead.\n    [Z] = _copy_arrays_if_base_present([Z])\n\n    n = Zs[0] + 1\n    R = np.zeros((n - 1, 4), dtype=np.double)\n\n    _hierarchy.inconsistent(Z, R, int(n), int(d))\n    return R\n\n\ndef from_mlab_linkage(Z):\n    \"\"\"\n    Convert a linkage matrix generated by MATLAB(TM) to a new\n    linkage matrix compatible with this module.\n    The conversion does two things:\n     * the indices are converted from ``1..N`` to ``0..(N-1)`` form,\n       and\n     * a fourth column ``Z[:,3]`` is added where ``Z[i,3]`` represents the\n       number of original observations (leaves) in the non-singleton\n       cluster ``i``.\n    This function is useful when loading in linkages from legacy data\n    files generated by MATLAB.\n    Parameters\n    ----------\n    Z : ndarray\n        A linkage matrix generated by MATLAB(TM).\n    Returns\n    -------\n    ZS : ndarray\n        A linkage matrix compatible with ``scipy.cluster.hierarchy``.\n    See Also\n    --------\n    linkage: for a description of what a linkage matrix is.\n    to_mlab_linkage: transform from SciPy to MATLAB format.\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from scipy.cluster.hierarchy import ward, from_mlab_linkage\n    Given a linkage matrix in MATLAB format ``mZ``, we can use\n    `scipy.cluster.hierarchy.from_mlab_linkage` to import\n    it into SciPy format:\n    &gt;&gt;&gt; mZ = np.array([[1, 2, 1], [4, 5, 1], [7, 8, 1],\n    ...                [10, 11, 1], [3, 13, 1.29099445],\n    ...                [6, 14, 1.29099445],\n    ...                [9, 15, 1.29099445],\n    ...                [12, 16, 1.29099445],\n    ...                [17, 18, 5.77350269],\n    ...                [19, 20, 5.77350269],\n    ...                [21, 22,  8.16496581]])\n    &gt;&gt;&gt; Z = from_mlab_linkage(mZ)\n    &gt;&gt;&gt; Z\n    array([[  0.        ,   1.        ,   1.        ,   2.        ],\n           [  3.        ,   4.        ,   1.        ,   2.        ],\n           [  6.        ,   7.        ,   1.        ,   2.        ],\n           [  9.        ,  10.        ,   1.        ,   2.        ],\n           [  2.        ,  12.        ,   1.29099445,   3.        ],\n           [  5.        ,  13.        ,   1.29099445,   3.        ],\n           [  8.        ,  14.        ,   1.29099445,   3.        ],\n           [ 11.        ,  15.        ,   1.29099445,   3.        ],\n           [ 16.        ,  17.        ,   5.77350269,   6.        ],\n           [ 18.        ,  19.        ,   5.77350269,   6.        ],\n           [ 20.        ,  21.        ,   8.16496581,  12.        ]])\n    As expected, the linkage matrix ``Z`` returned includes an\n    additional column counting the number of original samples in\n    each cluster. Also, all cluster indices are reduced by 1\n    (MATLAB format uses 1-indexing, whereas SciPy uses 0-indexing).\n    \"\"\"\n    Z = np.asarray(Z, dtype=np.double, order='c')\n    Zs = Z.shape\n\n    # If it's empty, return it.\n    if len(Zs) == 0 or (len(Zs) == 1 and Zs[0] == 0):\n        return Z.copy()\n\n    if len(Zs) != 2:\n        raise ValueError(\"The linkage array must be rectangular.\")\n\n    # If it contains no rows, return it.\n    if Zs[0] == 0:\n        return Z.copy()\n\n    Zpart = Z.copy()\n    if Zpart[:, 0:2].min() != 1.0 and Zpart[:, 0:2].max() != 2 * Zs[0]:\n        raise ValueError('The format of the indices is not 1..N')\n\n    Zpart[:, 0:2] -= 1.0\n    CS = np.zeros((Zs[0],), dtype=np.double)\n    _hierarchy.calculate_cluster_sizes(Zpart, CS, int(Zs[0]) + 1)\n    return np.hstack([Zpart, CS.reshape(Zs[0], 1)])\n\n\ndef to_mlab_linkage(Z):\n    \"\"\"\n    Convert a linkage matrix to a MATLAB(TM) compatible one.\n    Converts a linkage matrix ``Z`` generated by the linkage function\n    of this module to a MATLAB(TM) compatible one. The return linkage\n    matrix has the last column removed and the cluster indices are\n    converted to ``1..N`` indexing.\n    Parameters\n    ----------\n    Z : ndarray\n        A linkage matrix generated by ``scipy.cluster.hierarchy``.\n    Returns\n    -------\n    to_mlab_linkage : ndarray\n        A linkage matrix compatible with MATLAB(TM)'s hierarchical\n        clustering functions.\n        The return linkage matrix has the last column removed\n        and the cluster indices are converted to ``1..N`` indexing.\n    See Also\n    --------\n    linkage: for a description of what a linkage matrix is.\n    from_mlab_linkage: transform from Matlab to SciPy format.\n    Examples\n    --------\n    &gt;&gt;&gt; from scipy.cluster.hierarchy import ward, to_mlab_linkage\n    &gt;&gt;&gt; from scipy.spatial.distance import pdist\n    &gt;&gt;&gt; X = [[0, 0], [0, 1], [1, 0],\n    ...      [0, 4], [0, 3], [1, 4],\n    ...      [4, 0], [3, 0], [4, 1],\n    ...      [4, 4], [3, 4], [4, 3]]\n    &gt;&gt;&gt; Z = ward(pdist(X))\n    &gt;&gt;&gt; Z\n    array([[ 0.        ,  1.        ,  1.        ,  2.        ],\n           [ 3.        ,  4.        ,  1.        ,  2.        ],\n           [ 6.        ,  7.        ,  1.        ,  2.        ],\n           [ 9.        , 10.        ,  1.        ,  2.        ],\n           [ 2.        , 12.        ,  1.29099445,  3.        ],\n           [ 5.        , 13.        ,  1.29099445,  3.        ],\n           [ 8.        , 14.        ,  1.29099445,  3.        ],\n           [11.        , 15.        ,  1.29099445,  3.        ],\n           [16.        , 17.        ,  5.77350269,  6.        ],\n           [18.        , 19.        ,  5.77350269,  6.        ],\n           [20.        , 21.        ,  8.16496581, 12.        ]])\n    After a linkage matrix ``Z`` has been created, we can use\n    `scipy.cluster.hierarchy.to_mlab_linkage` to convert it\n    into MATLAB format:\n    &gt;&gt;&gt; mZ = to_mlab_linkage(Z)\n    &gt;&gt;&gt; mZ\n    array([[  1.        ,   2.        ,   1.        ],\n           [  4.        ,   5.        ,   1.        ],\n           [  7.        ,   8.        ,   1.        ],\n           [ 10.        ,  11.        ,   1.        ],\n           [  3.        ,  13.        ,   1.29099445],\n           [  6.        ,  14.        ,   1.29099445],\n           [  9.        ,  15.        ,   1.29099445],\n           [ 12.        ,  16.        ,   1.29099445],\n           [ 17.        ,  18.        ,   5.77350269],\n           [ 19.        ,  20.        ,   5.77350269],\n           [ 21.        ,  22.        ,   8.16496581]])\n    The new linkage matrix ``mZ`` uses 1-indexing for all the\n    clusters (instead of 0-indexing). Also, the last column of\n    the original linkage matrix has been dropped.\n    \"\"\"\n    Z = np.asarray(Z, order='c', dtype=np.double)\n    Zs = Z.shape\n    if len(Zs) == 0 or (len(Zs) == 1 and Zs[0] == 0):\n        return Z.copy()\n    is_valid_linkage(Z, throw=True, name='Z')\n\n    ZP = Z[:, 0:3].copy()\n    ZP[:, 0:2] += 1.0\n\n    return ZP\n\n\ndef is_monotonic(Z):\n    \"\"\"\n    Return True if the linkage passed is monotonic.\n    The linkage is monotonic if for every cluster :math:`s` and :math:`t`\n    joined, the distance between them is no less than the distance\n    between any previously joined clusters.\n    Parameters\n    ----------\n    Z : ndarray\n        The linkage matrix to check for monotonicity.\n    Returns\n    -------\n    b : bool\n        A boolean indicating whether the linkage is monotonic.\n    See Also\n    --------\n    linkage: for a description of what a linkage matrix is.\n    Examples\n    --------\n    &gt;&gt;&gt; from scipy.cluster.hierarchy import median, ward, is_monotonic\n    &gt;&gt;&gt; from scipy.spatial.distance import pdist\n    By definition, some hierarchical clustering algorithms - such as\n    `scipy.cluster.hierarchy.ward` - produce monotonic assignments of\n    samples to clusters; however, this is not always true for other\n    hierarchical methods - e.g. `scipy.cluster.hierarchy.median`.\n    Given a linkage matrix ``Z`` (as the result of a hierarchical clustering\n    method) we can test programmatically whether it has the monotonicity\n    property or not, using `scipy.cluster.hierarchy.is_monotonic`:\n    &gt;&gt;&gt; X = [[0, 0], [0, 1], [1, 0],\n    ...      [0, 4], [0, 3], [1, 4],\n    ...      [4, 0], [3, 0], [4, 1],\n    ...      [4, 4], [3, 4], [4, 3]]\n    &gt;&gt;&gt; Z = ward(pdist(X))\n    &gt;&gt;&gt; Z\n    array([[ 0.        ,  1.        ,  1.        ,  2.        ],\n           [ 3.        ,  4.        ,  1.        ,  2.        ],\n           [ 6.        ,  7.        ,  1.        ,  2.        ],\n           [ 9.        , 10.        ,  1.        ,  2.        ],\n           [ 2.        , 12.        ,  1.29099445,  3.        ],\n           [ 5.        , 13.        ,  1.29099445,  3.        ],\n           [ 8.        , 14.        ,  1.29099445,  3.        ],\n           [11.        , 15.        ,  1.29099445,  3.        ],\n           [16.        , 17.        ,  5.77350269,  6.        ],\n           [18.        , 19.        ,  5.77350269,  6.        ],\n           [20.        , 21.        ,  8.16496581, 12.        ]])\n    &gt;&gt;&gt; is_monotonic(Z)\n    True\n    &gt;&gt;&gt; Z = median(pdist(X))\n    &gt;&gt;&gt; Z\n    array([[ 0.        ,  1.        ,  1.        ,  2.        ],\n           [ 3.        ,  4.        ,  1.        ,  2.        ],\n           [ 9.        , 10.        ,  1.        ,  2.        ],\n           [ 6.        ,  7.        ,  1.        ,  2.        ],\n           [ 2.        , 12.        ,  1.11803399,  3.        ],\n           [ 5.        , 13.        ,  1.11803399,  3.        ],\n           [ 8.        , 15.        ,  1.11803399,  3.        ],\n           [11.        , 14.        ,  1.11803399,  3.        ],\n           [18.        , 19.        ,  3.        ,  6.        ],\n           [16.        , 17.        ,  3.5       ,  6.        ],\n           [20.        , 21.        ,  3.25      , 12.        ]])\n    &gt;&gt;&gt; is_monotonic(Z)\n    False\n    Note that this method is equivalent to just verifying that the distances\n    in the third column of the linkage matrix appear in a monotonically\n    increasing order.\n    \"\"\"\n    Z = np.asarray(Z, order='c')\n    is_valid_linkage(Z, throw=True, name='Z')\n\n    # We expect the i'th value to be greater than its successor.\n    return (Z[1:, 2] &gt;= Z[:-1, 2]).all()\n\n\ndef is_valid_im(R, warning=False, throw=False, name=None):\n    \"\"\"Return True if the inconsistency matrix passed is valid.\n    It must be a :math:`n` by 4 array of doubles. The standard\n    deviations ``R[:,1]`` must be nonnegative. The link counts\n    ``R[:,2]`` must be positive and no greater than :math:`n-1`.\n    Parameters\n    ----------\n    R : ndarray\n        The inconsistency matrix to check for validity.\n    warning : bool, optional\n         When True, issues a Python warning if the linkage\n         matrix passed is invalid.\n    throw : bool, optional\n         When True, throws a Python exception if the linkage\n         matrix passed is invalid.\n    name : str, optional\n         This string refers to the variable name of the invalid\n         linkage matrix.\n    Returns\n    -------\n    b : bool\n        True if the inconsistency matrix is valid.\n    See Also\n    --------\n    linkage: for a description of what a linkage matrix is.\n    inconsistent: for the creation of a inconsistency matrix.\n    Examples\n    --------\n    &gt;&gt;&gt; from scipy.cluster.hierarchy import ward, inconsistent, is_valid_im\n    &gt;&gt;&gt; from scipy.spatial.distance import pdist\n    Given a data set ``X``, we can apply a clustering method to obtain a\n    linkage matrix ``Z``. `scipy.cluster.hierarchy.inconsistent` can\n    be also used to obtain the inconsistency matrix ``R`` associated to\n    this clustering process:\n    &gt;&gt;&gt; X = [[0, 0], [0, 1], [1, 0],\n    ...      [0, 4], [0, 3], [1, 4],\n    ...      [4, 0], [3, 0], [4, 1],\n    ...      [4, 4], [3, 4], [4, 3]]\n    &gt;&gt;&gt; Z = ward(pdist(X))\n    &gt;&gt;&gt; R = inconsistent(Z)\n    &gt;&gt;&gt; Z\n    array([[ 0.        ,  1.        ,  1.        ,  2.        ],\n           [ 3.        ,  4.        ,  1.        ,  2.        ],\n           [ 6.        ,  7.        ,  1.        ,  2.        ],\n           [ 9.        , 10.        ,  1.        ,  2.        ],\n           [ 2.        , 12.        ,  1.29099445,  3.        ],\n           [ 5.        , 13.        ,  1.29099445,  3.        ],\n           [ 8.        , 14.        ,  1.29099445,  3.        ],\n           [11.        , 15.        ,  1.29099445,  3.        ],\n           [16.        , 17.        ,  5.77350269,  6.        ],\n           [18.        , 19.        ,  5.77350269,  6.        ],\n           [20.        , 21.        ,  8.16496581, 12.        ]])\n    &gt;&gt;&gt; R\n    array([[1.        , 0.        , 1.        , 0.        ],\n           [1.        , 0.        , 1.        , 0.        ],\n           [1.        , 0.        , 1.        , 0.        ],\n           [1.        , 0.        , 1.        , 0.        ],\n           [1.14549722, 0.20576415, 2.        , 0.70710678],\n           [1.14549722, 0.20576415, 2.        , 0.70710678],\n           [1.14549722, 0.20576415, 2.        , 0.70710678],\n           [1.14549722, 0.20576415, 2.        , 0.70710678],\n           [2.78516386, 2.58797734, 3.        , 1.15470054],\n           [2.78516386, 2.58797734, 3.        , 1.15470054],\n           [6.57065706, 1.38071187, 3.        , 1.15470054]])\n    Now we can use `scipy.cluster.hierarchy.is_valid_im` to verify that\n    ``R`` is correct:\n    &gt;&gt;&gt; is_valid_im(R)\n    True\n    However, if ``R`` is wrongly constructed (e.g., one of the standard\n    deviations is set to a negative value), then the check will fail:\n    &gt;&gt;&gt; R[-1,1] = R[-1,1] * -1\n    &gt;&gt;&gt; is_valid_im(R)\n    False\n    \"\"\"\n    R = np.asarray(R, order='c')\n    valid = True\n    name_str = \"%r \" % name if name else ''\n    try:\n        if type(R) != np.ndarray:\n            raise TypeError('Variable %spassed as inconsistency matrix is not '\n                            'a numpy array.' % name_str)\n        if R.dtype != np.double:\n            raise TypeError('Inconsistency matrix %smust contain doubles '\n                            '(double).' % name_str)\n        if len(R.shape) != 2:\n            raise ValueError('Inconsistency matrix %smust have shape=2 (i.e. '\n                             'be two-dimensional).' % name_str)\n        if R.shape[1] != 4:\n            raise ValueError('Inconsistency matrix %smust have 4 columns.' %\n                             name_str)\n        if R.shape[0] &lt; 1:\n            raise ValueError('Inconsistency matrix %smust have at least one '\n                             'row.' % name_str)\n        if (R[:, 0] &lt; 0).any():\n            raise ValueError('Inconsistency matrix %scontains negative link '\n                             'height means.' % name_str)\n        if (R[:, 1] &lt; 0).any():\n            raise ValueError('Inconsistency matrix %scontains negative link '\n                             'height standard deviations.' % name_str)\n        if (R[:, 2] &lt; 0).any():\n            raise ValueError('Inconsistency matrix %scontains negative link '\n                             'counts.' % name_str)\n    except Exception as e:\n        if throw:\n            raise\n        if warning:\n            _warning(str(e))\n        valid = False\n\n    return valid\n\n\ndef is_valid_linkage(Z, warning=False, throw=False, name=None):\n    \"\"\"\n    Check the validity of a linkage matrix.\n    A linkage matrix is valid if it is a 2-D array (type double)\n    with :math:`n` rows and 4 columns. The first two columns must contain\n    indices between 0 and :math:`2n-1`. For a given row ``i``, the following\n    two expressions have to hold:\n    .. math::\n        0 \\\\leq \\\\mathtt{Z[i,0]} \\\\leq i+n-1\n        0 \\\\leq Z[i,1] \\\\leq i+n-1\n    I.e., a cluster cannot join another cluster unless the cluster being joined\n    has been generated.\n    Parameters\n    ----------\n    Z : array_like\n        Linkage matrix.\n    warning : bool, optional\n        When True, issues a Python warning if the linkage\n        matrix passed is invalid.\n    throw : bool, optional\n        When True, throws a Python exception if the linkage\n        matrix passed is invalid.\n    name : str, optional\n        This string refers to the variable name of the invalid\n        linkage matrix.\n    Returns\n    -------\n    b : bool\n        True if the inconsistency matrix is valid.\n    See Also\n    --------\n    linkage: for a description of what a linkage matrix is.\n    Examples\n    --------\n    &gt;&gt;&gt; from scipy.cluster.hierarchy import ward, is_valid_linkage\n    &gt;&gt;&gt; from scipy.spatial.distance import pdist\n    All linkage matrices generated by the clustering methods in this module\n    will be valid (i.e., they will have the appropriate dimensions and the two\n    required expressions will hold for all the rows).\n    We can check this using `scipy.cluster.hierarchy.is_valid_linkage`:\n    &gt;&gt;&gt; X = [[0, 0], [0, 1], [1, 0],\n    ...      [0, 4], [0, 3], [1, 4],\n    ...      [4, 0], [3, 0], [4, 1],\n    ...      [4, 4], [3, 4], [4, 3]]\n    &gt;&gt;&gt; Z = ward(pdist(X))\n    &gt;&gt;&gt; Z\n    array([[ 0.        ,  1.        ,  1.        ,  2.        ],\n           [ 3.        ,  4.        ,  1.        ,  2.        ],\n           [ 6.        ,  7.        ,  1.        ,  2.        ],\n           [ 9.        , 10.        ,  1.        ,  2.        ],\n           [ 2.        , 12.        ,  1.29099445,  3.        ],\n           [ 5.        , 13.        ,  1.29099445,  3.        ],\n           [ 8.        , 14.        ,  1.29099445,  3.        ],\n           [11.        , 15.        ,  1.29099445,  3.        ],\n           [16.        , 17.        ,  5.77350269,  6.        ],\n           [18.        , 19.        ,  5.77350269,  6.        ],\n           [20.        , 21.        ,  8.16496581, 12.        ]])\n    &gt;&gt;&gt; is_valid_linkage(Z)\n    True\n    However, if we create a linkage matrix in a wrong way - or if we modify\n    a valid one in a way that any of the required expressions don't hold\n    anymore, then the check will fail:\n    &gt;&gt;&gt; Z[3][1] = 20    # the cluster number 20 is not defined at this point\n    &gt;&gt;&gt; is_valid_linkage(Z)\n    False\n    \"\"\"\n    Z = np.asarray(Z, order='c')\n    valid = True\n    name_str = \"%r \" % name if name else ''\n    try:\n        if type(Z) != np.ndarray:\n            raise TypeError('Passed linkage argument %sis not a valid array.' %\n                            name_str)\n        if Z.dtype != np.double:\n            raise TypeError('Linkage matrix %smust contain doubles.' % name_str)\n        if len(Z.shape) != 2:\n            raise ValueError('Linkage matrix %smust have shape=2 (i.e. be '\n                             'two-dimensional).' % name_str)\n        if Z.shape[1] != 4:\n            raise ValueError('Linkage matrix %smust have 4 columns.' % name_str)\n        if Z.shape[0] == 0:\n            raise ValueError('Linkage must be computed on at least two '\n                             'observations.')\n        n = Z.shape[0]\n        if n &gt; 1:\n            if ((Z[:, 0] &lt; 0).any() or (Z[:, 1] &lt; 0).any()):\n                raise ValueError('Linkage %scontains negative indices.' %\n                                 name_str)\n            if (Z[:, 2] &lt; 0).any():\n                raise ValueError('Linkage %scontains negative distances.' %\n                                 name_str)\n            if (Z[:, 3] &lt; 0).any():\n                raise ValueError('Linkage %scontains negative counts.' %\n                                 name_str)\n        if _check_hierarchy_uses_cluster_before_formed(Z):\n            raise ValueError('Linkage %suses non-singleton cluster before '\n                             'it is formed.' % name_str)\n        if _check_hierarchy_uses_cluster_more_than_once(Z):\n            raise ValueError('Linkage %suses the same cluster more than once.'\n                             % name_str)\n    except Exception as e:\n        if throw:\n            raise\n        if warning:\n            _warning(str(e))\n        valid = False\n\n    return valid\n\n\ndef _check_hierarchy_uses_cluster_before_formed(Z):\n    n = Z.shape[0] + 1\n    for i in range(0, n - 1):\n        if Z[i, 0] &gt;= n + i or Z[i, 1] &gt;= n + i:\n            return True\n    return False\n\n\ndef _check_hierarchy_uses_cluster_more_than_once(Z):\n    n = Z.shape[0] + 1\n    chosen = set([])\n    for i in range(0, n - 1):\n        if (Z[i, 0] in chosen) or (Z[i, 1] in chosen) or Z[i, 0] == Z[i, 1]:\n            return True\n        chosen.add(Z[i, 0])\n        chosen.add(Z[i, 1])\n    return False\n\n\ndef _check_hierarchy_not_all_clusters_used(Z):\n    n = Z.shape[0] + 1\n    chosen = set([])\n    for i in range(0, n - 1):\n        chosen.add(int(Z[i, 0]))\n        chosen.add(int(Z[i, 1]))\n    must_chosen = set(range(0, 2 * n - 2))\n    return len(must_chosen.difference(chosen)) &gt; 0\n\n\ndef num_obs_linkage(Z):\n    \"\"\"\n    Return the number of original observations of the linkage matrix passed.\n    Parameters\n    ----------\n    Z : ndarray\n        The linkage matrix on which to perform the operation.\n    Returns\n    -------\n    n : int\n        The number of original observations in the linkage.\n    Examples\n    --------\n    &gt;&gt;&gt; from scipy.cluster.hierarchy import ward, num_obs_linkage\n    &gt;&gt;&gt; from scipy.spatial.distance import pdist\n    &gt;&gt;&gt; X = [[0, 0], [0, 1], [1, 0],\n    ...      [0, 4], [0, 3], [1, 4],\n    ...      [4, 0], [3, 0], [4, 1],\n    ...      [4, 4], [3, 4], [4, 3]]\n    &gt;&gt;&gt; Z = ward(pdist(X))\n    ``Z`` is a linkage matrix obtained after using the Ward clustering method\n    with ``X``, a dataset with 12 data points.\n    &gt;&gt;&gt; num_obs_linkage(Z)\n    12\n    \"\"\"\n    Z = np.asarray(Z, order='c')\n    is_valid_linkage(Z, throw=True, name='Z')\n    return (Z.shape[0] + 1)\n\n\ndef correspond(Z, Y):\n    \"\"\"\n    Check for correspondence between linkage and condensed distance matrices.\n    They must have the same number of original observations for\n    the check to succeed.\n    This function is useful as a sanity check in algorithms that make\n    extensive use of linkage and distance matrices that must\n    correspond to the same set of original observations.\n    Parameters\n    ----------\n    Z : array_like\n        The linkage matrix to check for correspondence.\n    Y : array_like\n        The condensed distance matrix to check for correspondence.\n    Returns\n    -------\n    b : bool\n        A boolean indicating whether the linkage matrix and distance\n        matrix could possibly correspond to one another.\n    See Also\n    --------\n    linkage: for a description of what a linkage matrix is.\n    Examples\n    --------\n    &gt;&gt;&gt; from scipy.cluster.hierarchy import ward, correspond\n    &gt;&gt;&gt; from scipy.spatial.distance import pdist\n    This method can be used to check if a given linkage matrix ``Z`` has been\n    obtained from the application of a cluster method over a dataset ``X``:\n    &gt;&gt;&gt; X = [[0, 0], [0, 1], [1, 0],\n    ...      [0, 4], [0, 3], [1, 4],\n    ...      [4, 0], [3, 0], [4, 1],\n    ...      [4, 4], [3, 4], [4, 3]]\n    &gt;&gt;&gt; X_condensed = pdist(X)\n    &gt;&gt;&gt; Z = ward(X_condensed)\n    Here, we can compare ``Z`` and ``X`` (in condensed form):\n    &gt;&gt;&gt; correspond(Z, X_condensed)\n    True\n    \"\"\"\n    is_valid_linkage(Z, throw=True)\n    distance.is_valid_y(Y, throw=True)\n    Z = np.asarray(Z, order='c')\n    Y = np.asarray(Y, order='c')\n    return distance.num_obs_y(Y) == num_obs_linkage(Z)\n\n\ndef fcluster(Z, t, criterion='inconsistent', depth=2, R=None, monocrit=None):\n    \"\"\"\n    Form flat clusters from the hierarchical clustering defined by\n    the given linkage matrix.\n    Parameters\n    ----------\n    Z : ndarray\n        The hierarchical clustering encoded with the matrix returned\n        by the `linkage` function.\n    t : scalar\n        For criteria 'inconsistent', 'distance' or 'monocrit',\n         this is the threshold to apply when forming flat clusters.\n        For 'maxclust' or 'maxclust_monocrit' criteria,\n         this would be max number of clusters requested.\n    criterion : str, optional\n        The criterion to use in forming flat clusters. This can\n        be any of the following values:\n          ``inconsistent`` :\n              If a cluster node and all its\n              descendants have an inconsistent value less than or equal\n              to `t`, then all its leaf descendants belong to the\n              same flat cluster. When no non-singleton cluster meets\n              this criterion, every node is assigned to its own\n              cluster. (Default)\n          ``distance`` :\n              Forms flat clusters so that the original\n              observations in each flat cluster have no greater a\n              cophenetic distance than `t`.\n          ``maxclust`` :\n              Finds a minimum threshold ``r`` so that\n              the cophenetic distance between any two original\n              observations in the same flat cluster is no more than\n              ``r`` and no more than `t` flat clusters are formed.\n          ``monocrit`` :\n              Forms a flat cluster from a cluster node c\n              with index i when ``monocrit[j] &lt;= t``.\n              For example, to threshold on the maximum mean distance\n              as computed in the inconsistency matrix R with a\n              threshold of 0.8 do::\n                  MR = maxRstat(Z, R, 3)\n                  fcluster(Z, t=0.8, criterion='monocrit', monocrit=MR)\n          ``maxclust_monocrit`` :\n              Forms a flat cluster from a\n              non-singleton cluster node ``c`` when ``monocrit[i] &lt;=\n              r`` for all cluster indices ``i`` below and including\n              ``c``. ``r`` is minimized such that no more than ``t``\n              flat clusters are formed. monocrit must be\n              monotonic. For example, to minimize the threshold t on\n              maximum inconsistency values so that no more than 3 flat\n              clusters are formed, do::\n                  MI = maxinconsts(Z, R)\n                  fcluster(Z, t=3, criterion='maxclust_monocrit', monocrit=MI)\n    depth : int, optional\n        The maximum depth to perform the inconsistency calculation.\n        It has no meaning for the other criteria. Default is 2.\n    R : ndarray, optional\n        The inconsistency matrix to use for the 'inconsistent'\n        criterion. This matrix is computed if not provided.\n    monocrit : ndarray, optional\n        An array of length n-1. `monocrit[i]` is the\n        statistics upon which non-singleton i is thresholded. The\n        monocrit vector must be monotonic, i.e., given a node c with\n        index i, for all node indices j corresponding to nodes\n        below c, ``monocrit[i] &gt;= monocrit[j]``.\n    Returns\n    -------\n    fcluster : ndarray\n        An array of length ``n``. ``T[i]`` is the flat cluster number to\n        which original observation ``i`` belongs.\n    See Also\n    --------\n    linkage : for information about hierarchical clustering methods work.\n    Examples\n    --------\n    &gt;&gt;&gt; from scipy.cluster.hierarchy import ward, fcluster\n    &gt;&gt;&gt; from scipy.spatial.distance import pdist\n    All cluster linkage methods - e.g., `scipy.cluster.hierarchy.ward`\n    generate a linkage matrix ``Z`` as their output:\n    &gt;&gt;&gt; X = [[0, 0], [0, 1], [1, 0],\n    ...      [0, 4], [0, 3], [1, 4],\n    ...      [4, 0], [3, 0], [4, 1],\n    ...      [4, 4], [3, 4], [4, 3]]\n    &gt;&gt;&gt; Z = ward(pdist(X))\n    &gt;&gt;&gt; Z\n    array([[ 0.        ,  1.        ,  1.        ,  2.        ],\n           [ 3.        ,  4.        ,  1.        ,  2.        ],\n           [ 6.        ,  7.        ,  1.        ,  2.        ],\n           [ 9.        , 10.        ,  1.        ,  2.        ],\n           [ 2.        , 12.        ,  1.29099445,  3.        ],\n           [ 5.        , 13.        ,  1.29099445,  3.        ],\n           [ 8.        , 14.        ,  1.29099445,  3.        ],\n           [11.        , 15.        ,  1.29099445,  3.        ],\n           [16.        , 17.        ,  5.77350269,  6.        ],\n           [18.        , 19.        ,  5.77350269,  6.        ],\n           [20.        , 21.        ,  8.16496581, 12.        ]])\n    This matrix represents a dendrogram, where the first and second elements\n    are the two clusters merged at each step, the third element is the\n    distance between these clusters, and the fourth element is the size of\n    the new cluster - the number of original data points included.\n    `scipy.cluster.hierarchy.fcluster` can be used to flatten the\n    dendrogram, obtaining as a result an assignation of the original data\n    points to single clusters.\n    This assignation mostly depends on a distance threshold ``t`` - the maximum\n    inter-cluster distance allowed:\n    &gt;&gt;&gt; fcluster(Z, t=0.9, criterion='distance')\n    array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12], dtype=int32)\n    &gt;&gt;&gt; fcluster(Z, t=1.1, criterion='distance')\n    array([1, 1, 2, 3, 3, 4, 5, 5, 6, 7, 7, 8], dtype=int32)\n    &gt;&gt;&gt; fcluster(Z, t=3, criterion='distance')\n    array([1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4], dtype=int32)\n    &gt;&gt;&gt; fcluster(Z, t=9, criterion='distance')\n    array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)\n    In the first case, the threshold ``t`` is too small to allow any two\n    samples in the data to form a cluster, so 12 different clusters are\n    returned.\n    In the second case, the threshold is large enough to allow the first\n    4 points to be merged with their nearest neighbors. So, here, only 8\n    clusters are returned.\n    The third case, with a much higher threshold, allows for up to 8 data\n    points to be connected - so 4 clusters are returned here.\n    Lastly, the threshold of the fourth case is large enough to allow for\n    all data points to be merged together - so a single cluster is returned.\n    \"\"\"\n    Z = np.asarray(Z, order='c')\n    is_valid_linkage(Z, throw=True, name='Z')\n\n    n = Z.shape[0] + 1\n    T = np.zeros((n,), dtype='i')\n\n    # Since the C code does not support striding using strides.\n    # The dimensions are used instead.\n    [Z] = _copy_arrays_if_base_present([Z])\n\n    if criterion == 'inconsistent':\n        if R is None:\n            R = inconsistent(Z, depth)\n        else:\n            R = np.asarray(R, order='c')\n            is_valid_im(R, throw=True, name='R')\n            # Since the C code does not support striding using strides.\n            # The dimensions are used instead.\n            [R] = _copy_arrays_if_base_present([R])\n        _hierarchy.cluster_in(Z, R, T, float(t), int(n))\n    elif criterion == 'distance':\n        _hierarchy.cluster_dist(Z, T, float(t), int(n))\n    elif criterion == 'maxclust':\n        _hierarchy.cluster_maxclust_dist(Z, T, int(n), int(t))\n    elif criterion == 'monocrit':\n        [monocrit] = _copy_arrays_if_base_present([monocrit])\n        _hierarchy.cluster_monocrit(Z, monocrit, T, float(t), int(n))\n    elif criterion == 'maxclust_monocrit':\n        [monocrit] = _copy_arrays_if_base_present([monocrit])\n        _hierarchy.cluster_maxclust_monocrit(Z, monocrit, T, int(n), int(t))\n    else:\n        raise ValueError('Invalid cluster formation criterion: %s'\n                         % str(criterion))\n    return T\n\n\ndef fclusterdata(X, t, criterion='inconsistent',\n                 metric='euclidean', depth=2, method='single', R=None):\n    \"\"\"\n    Cluster observation data using a given metric.\n    Clusters the original observations in the n-by-m data\n    matrix X (n observations in m dimensions), using the euclidean\n    distance metric to calculate distances between original observations,\n    performs hierarchical clustering using the single linkage algorithm,\n    and forms flat clusters using the inconsistency method with `t` as the\n    cut-off threshold.\n    A 1-D array ``T`` of length ``n`` is returned. ``T[i]`` is\n    the index of the flat cluster to which the original observation ``i``\n    belongs.\n    Parameters\n    ----------\n    X : (N, M) ndarray\n        N by M data matrix with N observations in M dimensions.\n    t : scalar\n        For criteria 'inconsistent', 'distance' or 'monocrit',\n         this is the threshold to apply when forming flat clusters.\n        For 'maxclust' or 'maxclust_monocrit' criteria,\n         this would be max number of clusters requested.\n    criterion : str, optional\n        Specifies the criterion for forming flat clusters. Valid\n        values are 'inconsistent' (default), 'distance', or 'maxclust'\n        cluster formation algorithms. See `fcluster` for descriptions.\n    metric : str or function, optional\n        The distance metric for calculating pairwise distances. See\n        ``distance.pdist`` for descriptions and linkage to verify\n        compatibility with the linkage method.\n    depth : int, optional\n        The maximum depth for the inconsistency calculation. See\n        `inconsistent` for more information.\n    method : str, optional\n        The linkage method to use (single, complete, average,\n        weighted, median centroid, ward). See `linkage` for more\n        information. Default is \"single\".\n    R : ndarray, optional\n        The inconsistency matrix. It will be computed if necessary\n        if it is not passed.\n    Returns\n    -------\n    fclusterdata : ndarray\n        A vector of length n. T[i] is the flat cluster number to\n        which original observation i belongs.\n    See Also\n    --------\n    scipy.spatial.distance.pdist : pairwise distance metrics\n    Notes\n    -----\n    This function is similar to the MATLAB function ``clusterdata``.\n    Examples\n    --------\n    &gt;&gt;&gt; from scipy.cluster.hierarchy import fclusterdata\n    This is a convenience method that abstracts all the steps to perform in a\n    typical SciPy's hierarchical clustering workflow.\n    * Transform the input data into a condensed matrix with `scipy.spatial.distance.pdist`.\n    * Apply a clustering method.\n    * Obtain flat clusters at a user defined distance threshold ``t`` using `scipy.cluster.hierarchy.fcluster`.\n    &gt;&gt;&gt; X = [[0, 0], [0, 1], [1, 0],\n    ...      [0, 4], [0, 3], [1, 4],\n    ...      [4, 0], [3, 0], [4, 1],\n    ...      [4, 4], [3, 4], [4, 3]]\n    &gt;&gt;&gt; fclusterdata(X, t=1)\n    array([3, 3, 3, 4, 4, 4, 2, 2, 2, 1, 1, 1], dtype=int32)\n    The output here (for the dataset ``X``, distance threshold ``t``, and the\n    default settings) is four clusters with three data points each.\n    \"\"\"\n    X = np.asarray(X, order='c', dtype=np.double)\n\n    if type(X) != np.ndarray or len(X.shape) != 2:\n        raise TypeError('The observation matrix X must be an n by m numpy '\n                        'array.')\n\n    Y = distance.pdist(X, metric=metric)\n    Z = linkage(Y, method=method)\n    if R is None:\n        R = inconsistent(Z, d=depth)\n    else:\n        R = np.asarray(R, order='c')\n    T = fcluster(Z, criterion=criterion, depth=depth, R=R, t=t)\n    return T\n\n\ndef leaves_list(Z):\n    \"\"\"\n    Return a list of leaf node ids.\n    The return corresponds to the observation vector index as it appears\n    in the tree from left to right. Z is a linkage matrix.\n    Parameters\n    ----------\n    Z : ndarray\n        The hierarchical clustering encoded as a matrix.  `Z` is\n        a linkage matrix.  See `linkage` for more information.\n    Returns\n    -------\n    leaves_list : ndarray\n        The list of leaf node ids.\n    See Also\n    --------\n    dendrogram: for information about dendrogram structure.\n    Examples\n    --------\n    &gt;&gt;&gt; from scipy.cluster.hierarchy import ward, dendrogram, leaves_list\n    &gt;&gt;&gt; from scipy.spatial.distance import pdist\n    &gt;&gt;&gt; from matplotlib import pyplot as plt\n    &gt;&gt;&gt; X = [[0, 0], [0, 1], [1, 0],\n    ...      [0, 4], [0, 3], [1, 4],\n    ...      [4, 0], [3, 0], [4, 1],\n    ...      [4, 4], [3, 4], [4, 3]]\n    &gt;&gt;&gt; Z = ward(pdist(X))\n    The linkage matrix ``Z`` represents a dendrogram, that is, a tree that\n    encodes the structure of the clustering performed.\n    `scipy.cluster.hierarchy.leaves_list` shows the mapping between\n    indices in the ``X`` dataset and leaves in the dendrogram:\n    &gt;&gt;&gt; leaves_list(Z)\n    array([ 2,  0,  1,  5,  3,  4,  8,  6,  7, 11,  9, 10], dtype=int32)\n    &gt;&gt;&gt; fig = plt.figure(figsize=(25, 10))\n    &gt;&gt;&gt; dn = dendrogram(Z)\n    &gt;&gt;&gt; plt.show()\n    \"\"\"\n    Z = np.asarray(Z, order='c')\n    is_valid_linkage(Z, throw=True, name='Z')\n    n = Z.shape[0] + 1\n    ML = np.zeros((n,), dtype='i')\n    [Z] = _copy_arrays_if_base_present([Z])\n    _hierarchy.prelist(Z, ML, int(n))\n    return ML\n\n\n# Maps number of leaves to text size.\n#\n# p &lt;= 20, size=\"12\"\n# 20 &lt; p &lt;= 30, size=\"10\"\n# 30 &lt; p &lt;= 50, size=\"8\"\n# 50 &lt; p &lt;= np.inf, size=\"6\"\n\n_dtextsizes = {20: 12, 30: 10, 50: 8, 85: 6, np.inf: 5}\n_drotation = {20: 0, 40: 45, np.inf: 90}\n_dtextsortedkeys = list(_dtextsizes.keys())\n_dtextsortedkeys.sort()\n_drotationsortedkeys = list(_drotation.keys())\n_drotationsortedkeys.sort()\n\n\ndef _remove_dups(L):\n    \"\"\"\n    Remove duplicates AND preserve the original order of the elements.\n    The set class is not guaranteed to do this.\n    \"\"\"\n    seen_before = set([])\n    L2 = []\n    for i in L:\n        if i not in seen_before:\n            seen_before.add(i)\n            L2.append(i)\n    return L2\n\n\ndef _get_tick_text_size(p):\n    for k in _dtextsortedkeys:\n        if p &lt;= k:\n            return _dtextsizes[k]\n\n\ndef _get_tick_rotation(p):\n    for k in _drotationsortedkeys:\n        if p &lt;= k:\n            return _drotation[k]\n\n\ndef _plot_dendrogram(icoords, dcoords, ivl, p, n, mh, orientation,\n                     no_labels, color_list, leaf_font_size=None,\n                     leaf_rotation=None, contraction_marks=None,\n                     ax=None, above_threshold_color='C0'):\n    # Import matplotlib here so that it's not imported unless dendrograms\n    # are plotted. Raise an informative error if importing fails.\n    try:\n        # if an axis is provided, don't use pylab at all\n        if ax is None:\n            import matplotlib.pylab\n        import matplotlib.patches\n        import matplotlib.collections\n    except ImportError as e:\n        raise ImportError(\"You must install the matplotlib library to plot \"\n                          \"the dendrogram. Use no_plot=True to calculate the \"\n                          \"dendrogram without plotting.\") from e\n\n    if ax is None:\n        ax = matplotlib.pylab.gca()\n        # if we're using pylab, we want to trigger a draw at the end\n        trigger_redraw = True\n    else:\n        trigger_redraw = False\n\n    # Independent variable plot width\n    ivw = len(ivl) * 10\n    # Dependent variable plot height\n    dvw = mh + mh * 0.05\n\n    iv_ticks = np.arange(5, len(ivl) * 10 + 5, 10)\n    if orientation in ('top', 'bottom'):\n        if orientation == 'top':\n            ax.set_ylim([0, dvw])\n            ax.set_xlim([0, ivw])\n        else:\n            ax.set_ylim([dvw, 0])\n            ax.set_xlim([0, ivw])\n\n        xlines = icoords\n        ylines = dcoords\n        if no_labels:\n            ax.set_xticks([])\n            ax.set_xticklabels([])\n        else:\n            ax.set_xticks(iv_ticks)\n\n            if orientation == 'top':\n                ax.xaxis.set_ticks_position('bottom')\n            else:\n                ax.xaxis.set_ticks_position('top')\n\n            # Make the tick marks invisible because they cover up the links\n            for line in ax.get_xticklines():\n                line.set_visible(False)\n\n            leaf_rot = (float(_get_tick_rotation(len(ivl)))\n                        if (leaf_rotation is None) else leaf_rotation)\n            leaf_font = (float(_get_tick_text_size(len(ivl)))\n                         if (leaf_font_size is None) else leaf_font_size)\n            ax.set_xticklabels(ivl, rotation=leaf_rot, size=leaf_font)\n\n    elif orientation in ('left', 'right'):\n        if orientation == 'left':\n            ax.set_xlim([dvw, 0])\n            ax.set_ylim([0, ivw])\n        else:\n            ax.set_xlim([0, dvw])\n            ax.set_ylim([0, ivw])\n\n        xlines = dcoords\n        ylines = icoords\n        if no_labels:\n            ax.set_yticks([])\n            ax.set_yticklabels([])\n        else:\n            ax.set_yticks(iv_ticks)\n\n            if orientation == 'left':\n                ax.yaxis.set_ticks_position('right')\n            else:\n                ax.yaxis.set_ticks_position('left')\n\n            # Make the tick marks invisible because they cover up the links\n            for line in ax.get_yticklines():\n                line.set_visible(False)\n\n            leaf_font = (float(_get_tick_text_size(len(ivl)))\n                         if (leaf_font_size is None) else leaf_font_size)\n\n            if leaf_rotation is not None:\n                ax.set_yticklabels(ivl, rotation=leaf_rotation, size=leaf_font)\n            else:\n                ax.set_yticklabels(ivl, size=leaf_font)\n\n    # Let's use collections instead. This way there is a separate legend item\n    # for each tree grouping, rather than stupidly one for each line segment.\n    colors_used = _remove_dups(color_list)\n    color_to_lines = {}\n    for color in colors_used:\n        color_to_lines[color] = []\n    for (xline, yline, color) in zip(xlines, ylines, color_list):\n        color_to_lines[color].append(list(zip(xline, yline)))\n\n    colors_to_collections = {}\n    # Construct the collections.\n    for color in colors_used:\n        coll = matplotlib.collections.LineCollection(color_to_lines[color],\n                                                     colors=(color,))\n        colors_to_collections[color] = coll\n\n    # Add all the groupings below the color threshold.\n    for color in colors_used:\n        if color != above_threshold_color:\n            ax.add_collection(colors_to_collections[color])\n    # If there's a grouping of links above the color threshold, it goes last.\n    if above_threshold_color in colors_to_collections:\n        ax.add_collection(colors_to_collections[above_threshold_color])\n\n    if contraction_marks is not None:\n        Ellipse = matplotlib.patches.Ellipse\n        for (x, y) in contraction_marks:\n            if orientation in ('left', 'right'):\n                e = Ellipse((y, x), width=dvw / 100, height=1.0)\n            else:\n                e = Ellipse((x, y), width=1.0, height=dvw / 100)\n            ax.add_artist(e)\n            e.set_clip_box(ax.bbox)\n            e.set_alpha(0.5)\n            e.set_facecolor('k')\n\n    if trigger_redraw:\n        matplotlib.pylab.draw_if_interactive()\n\n\n# C0  is used for above threshhold color\n_link_line_colors_default = ('C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9')\n_link_line_colors = list(_link_line_colors_default)\n\n\ndef set_link_color_palette(palette):\n    \"\"\"\n    Set list of matplotlib color codes for use by dendrogram.\n    Note that this palette is global (i.e., setting it once changes the colors\n    for all subsequent calls to `dendrogram`) and that it affects only the\n    the colors below ``color_threshold``.\n    Note that `dendrogram` also accepts a custom coloring function through its\n    ``link_color_func`` keyword, which is more flexible and non-global.\n    Parameters\n    ----------\n    palette : list of str or None\n        A list of matplotlib color codes.  The order of the color codes is the\n        order in which the colors are cycled through when color thresholding in\n        the dendrogram.\n        If ``None``, resets the palette to its default (which are matplotlib\n        default colors C1 to C9).\n    Returns\n    -------\n    None\n    See Also\n    --------\n    dendrogram\n    Notes\n    -----\n    Ability to reset the palette with ``None`` added in SciPy 0.17.0.\n    Examples\n    --------\n    &gt;&gt;&gt; from scipy.cluster import hierarchy\n    &gt;&gt;&gt; ytdist = np.array([662., 877., 255., 412., 996., 295., 468., 268.,\n    ...                    400., 754., 564., 138., 219., 869., 669.])\n    &gt;&gt;&gt; Z = hierarchy.linkage(ytdist, 'single')\n    &gt;&gt;&gt; dn = hierarchy.dendrogram(Z, no_plot=True)\n    &gt;&gt;&gt; dn['color_list']\n    ['C1', 'C0', 'C0', 'C0', 'C0']\n    &gt;&gt;&gt; hierarchy.set_link_color_palette(['c', 'm', 'y', 'k'])\n    &gt;&gt;&gt; dn = hierarchy.dendrogram(Z, no_plot=True, above_threshold_color='b')\n    &gt;&gt;&gt; dn['color_list']\n    ['c', 'b', 'b', 'b', 'b']\n    &gt;&gt;&gt; dn = hierarchy.dendrogram(Z, no_plot=True, color_threshold=267,\n    ...                           above_threshold_color='k')\n    &gt;&gt;&gt; dn['color_list']\n    ['c', 'm', 'm', 'k', 'k']\n    Now, reset the color palette to its default:\n    &gt;&gt;&gt; hierarchy.set_link_color_palette(None)\n    \"\"\"\n    if palette is None:\n        # reset to its default\n        palette = _link_line_colors_default\n    elif type(palette) not in (list, tuple):\n        raise TypeError(\"palette must be a list or tuple\")\n    _ptypes = [isinstance(p, str) for p in palette]\n\n    if False in _ptypes:\n        raise TypeError(\"all palette list elements must be color strings\")\n\n    global _link_line_colors\n    _link_line_colors = palette\n\n\ndef dendrogram(Z, p=30, truncate_mode=None, color_threshold=None,\n               get_leaves=True, orientation='top', labels=None,\n               count_sort=False, distance_sort=False, show_leaf_counts=True,\n               no_plot=False, no_labels=False, leaf_font_size=None,\n               leaf_rotation=None, leaf_label_func=None,\n               show_contracted=False, link_color_func=None, ax=None,\n               above_threshold_color='C0'):\n    \"\"\"\n    Plot the hierarchical clustering as a dendrogram.\n    The dendrogram illustrates how each cluster is\n    composed by drawing a U-shaped link between a non-singleton\n    cluster and its children. The top of the U-link indicates a\n    cluster merge. The two legs of the U-link indicate which clusters\n    were merged. The length of the two legs of the U-link represents\n    the distance between the child clusters. It is also the\n    cophenetic distance between original observations in the two\n    children clusters. Customized to track cluster index traversal in\n    construction of dendrogram for labeling purposes.\n\n    Parameters\n    ----------\n    Z : ndarray\n        The linkage matrix encoding the hierarchical clustering to\n        render as a dendrogram. See the ``linkage`` function for more\n        information on the format of ``Z``.\n    p : int, optional\n        The ``p`` parameter for ``truncate_mode``.\n    truncate_mode : str, optional\n        The dendrogram can be hard to read when the original\n        observation matrix from which the linkage is derived is\n        large. Truncation is used to condense the dendrogram. There\n        are several modes:\n        ``None``\n          No truncation is performed (default).\n          Note: ``'none'`` is an alias for ``None`` that's kept for\n          backward compatibility.\n        ``'lastp'``\n          The last ``p`` non-singleton clusters formed in the linkage are the\n          only non-leaf nodes in the linkage; they correspond to rows\n          ``Z[n-p-2:end]`` in ``Z``. All other non-singleton clusters are\n          contracted into leaf nodes.\n        ``'level'``\n          No more than ``p`` levels of the dendrogram tree are displayed.\n          A \"level\" includes all nodes with ``p`` merges from the last merge.\n          Note: ``'mtica'`` is an alias for ``'level'`` that's kept for\n          backward compatibility.\n    color_threshold : double, optional\n        For brevity, let :math:`t` be the ``color_threshold``.\n        Colors all the descendent links below a cluster node\n        :math:`k` the same color if :math:`k` is the first node below\n        the cut threshold :math:`t`. All links connecting nodes with\n        distances greater than or equal to the threshold are colored\n        with de default matplotlib color ``'C0'``. If :math:`t` is less\n        than or equal to zero, all nodes are colored ``'C0'``.\n        If ``color_threshold`` is None or 'default',\n        corresponding with MATLAB(TM) behavior, the threshold is set to\n        ``0.7*max(Z[:,2])``.\n    get_leaves : bool, optional\n        Includes a list ``R['leaves']=H`` in the result\n        dictionary. For each :math:`i`, ``H[i] == j``, cluster node\n        ``j`` appears in position ``i`` in the left-to-right traversal\n        of the leaves, where :math:`j &lt; 2n-1` and :math:`i &lt; n`.\n    orientation : str, optional\n        The direction to plot the dendrogram, which can be any\n        of the following strings:\n        ``'top'``\n          Plots the root at the top, and plot descendent links going downwards.\n          (default).\n        ``'bottom'``\n          Plots the root at the bottom, and plot descendent links going\n          upwards.\n        ``'left'``\n          Plots the root at the left, and plot descendent links going right.\n        ``'right'``\n          Plots the root at the right, and plot descendent links going left.\n    labels : ndarray, optional\n        By default, ``labels`` is None so the index of the original observation\n        is used to label the leaf nodes.  Otherwise, this is an :math:`n`-sized\n        sequence, with ``n == Z.shape[0] + 1``. The ``labels[i]`` value is the\n        text to put under the :math:`i` th leaf node only if it corresponds to\n        an original observation and not a non-singleton cluster.\n    count_sort : str or bool, optional\n        For each node n, the order (visually, from left-to-right) n's\n        two descendent links are plotted is determined by this\n        parameter, which can be any of the following values:\n        ``False``\n          Nothing is done.\n        ``'ascending'`` or ``True``\n          The child with the minimum number of original objects in its cluster\n          is plotted first.\n        ``'descending'``\n          The child with the maximum number of original objects in its cluster\n          is plotted first.\n        Note, ``distance_sort`` and ``count_sort`` cannot both be True.\n    distance_sort : str or bool, optional\n        For each node n, the order (visually, from left-to-right) n's\n        two descendent links are plotted is determined by this\n        parameter, which can be any of the following values:\n        ``False``\n          Nothing is done.\n        ``'ascending'`` or ``True``\n          The child with the minimum distance between its direct descendents is\n          plotted first.\n        ``'descending'``\n          The child with the maximum distance between its direct descendents is\n          plotted first.\n        Note ``distance_sort`` and ``count_sort`` cannot both be True.\n    show_leaf_counts : bool, optional\n         When True, leaf nodes representing :math:`k&gt;1` original\n         observation are labeled with the number of observations they\n         contain in parentheses.\n    no_plot : bool, optional\n        When True, the final rendering is not performed. This is\n        useful if only the data structures computed for the rendering\n        are needed or if matplotlib is not available.\n    no_labels : bool, optional\n        When True, no labels appear next to the leaf nodes in the\n        rendering of the dendrogram.\n    leaf_rotation : double, optional\n        Specifies the angle (in degrees) to rotate the leaf\n        labels. When unspecified, the rotation is based on the number of\n        nodes in the dendrogram (default is 0).\n    leaf_font_size : int, optional\n        Specifies the font size (in points) of the leaf labels. When\n        unspecified, the size based on the number of nodes in the\n        dendrogram.\n    leaf_label_func : lambda or function, optional\n        When leaf_label_func is a callable function, for each\n        leaf with cluster index :math:`k &lt; 2n-1`. The function\n        is expected to return a string with the label for the\n        leaf.\n        Indices :math:`k &lt; n` correspond to original observations\n        while indices :math:`k \\\\geq n` correspond to non-singleton\n        clusters.\n        For example, to label singletons with their node id and\n        non-singletons with their id, count, and inconsistency\n        coefficient, simply do::\n            # First define the leaf label function.\n            def llf(id):\n                if id &lt; n:\n                    return str(id)\n                else:\n                    return '[%d %d %1.2f]' % (id, count, R[n-id,3])\n            # The text for the leaf nodes is going to be big so force\n            # a rotation of 90 degrees.\n            dendrogram(Z, leaf_label_func=llf, leaf_rotation=90)\n    show_contracted : bool, optional\n        When True the heights of non-singleton nodes contracted\n        into a leaf node are plotted as crosses along the link\n        connecting that leaf node.  This really is only useful when\n        truncation is used (see ``truncate_mode`` parameter).\n    link_color_func : callable, optional\n        If given, `link_color_function` is called with each non-singleton id\n        corresponding to each U-shaped link it will paint. The function is\n        expected to return the color to paint the link, encoded as a matplotlib\n        color string code. For example::\n            dendrogram(Z, link_color_func=lambda k: colors[k])\n        colors the direct links below each untruncated non-singleton node\n        ``k`` using ``colors[k]``.\n    ax : matplotlib Axes instance, optional\n        If None and `no_plot` is not True, the dendrogram will be plotted\n        on the current axes.  Otherwise if `no_plot` is not True the\n        dendrogram will be plotted on the given ``Axes`` instance. This can be\n        useful if the dendrogram is part of a more complex figure.\n    above_threshold_color : str, optional\n        This matplotlib color string sets the color of the links above the\n        color_threshold. The default is ``'C0'``.\n    Returns\n    -------\n    R : dict\n        A dictionary of data structures computed to render the\n        dendrogram. Its has the following keys:\n        ``'color_list'``\n          A list of color names. The k'th element represents the color of the\n          k'th link.\n        ``'icoord'`` and ``'dcoord'``\n          Each of them is a list of lists. Let ``icoord = [I1, I2, ..., Ip]``\n          where ``Ik = [xk1, xk2, xk3, xk4]`` and ``dcoord = [D1, D2, ..., Dp]``\n          where ``Dk = [yk1, yk2, yk3, yk4]``, then the k'th link painted is\n          ``(xk1, yk1)`` - ``(xk2, yk2)`` - ``(xk3, yk3)`` - ``(xk4, yk4)``.\n        ``'ivl'``\n          A list of labels corresponding to the leaf nodes.\n        ``'leaves'``\n          For each i, ``H[i] == j``, cluster node ``j`` appears in position\n          ``i`` in the left-to-right traversal of the leaves, where\n          :math:`j &lt; 2n-1` and :math:`i &lt; n`. If ``j`` is less than ``n``, the\n          ``i``-th leaf node corresponds to an original observation.\n          Otherwise, it corresponds to a non-singleton cluster.\n        ``'leaves_color_list'``\n          A list of color names. The k'th element represents the color of the\n          k'th leaf.\n    See Also\n    --------\n    linkage, set_link_color_palette\n    Notes\n    -----\n    It is expected that the distances in ``Z[:,2]`` be monotonic, otherwise\n    crossings appear in the dendrogram.\n    Examples\n    --------\n    &gt;&gt;&gt; from scipy.cluster import hierarchy\n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n    A very basic example:\n    &gt;&gt;&gt; ytdist = np.array([662., 877., 255., 412., 996., 295., 468., 268.,\n    ...                    400., 754., 564., 138., 219., 869., 669.])\n    &gt;&gt;&gt; Z = hierarchy.linkage(ytdist, 'single')\n    &gt;&gt;&gt; plt.figure()\n    &gt;&gt;&gt; dn = hierarchy.dendrogram(Z)\n    Now, plot in given axes, improve the color scheme and use both vertical and\n    horizontal orientations:\n    &gt;&gt;&gt; hierarchy.set_link_color_palette(['m', 'c', 'y', 'k'])\n    &gt;&gt;&gt; fig, axes = plt.subplots(1, 2, figsize=(8, 3))\n    &gt;&gt;&gt; dn1 = hierarchy.dendrogram(Z, ax=axes[0], above_threshold_color='y',\n    ...                            orientation='top')\n    &gt;&gt;&gt; dn2 = hierarchy.dendrogram(Z, ax=axes[1],\n    ...                            above_threshold_color='#bcbddc',\n    ...                            orientation='right')\n    &gt;&gt;&gt; hierarchy.set_link_color_palette(None)  # reset to default after use\n    &gt;&gt;&gt; plt.show()\n    \"\"\"\n    # This feature was thought about but never implemented (still useful?):\n    #\n    #         ... = dendrogram(..., leaves_order=None)\n    #\n    #         Plots the leaves in the order specified by a vector of\n    #         original observation indices. If the vector contains duplicates\n    #         or results in a crossing, an exception will be thrown. Passing\n    #         None orders leaf nodes based on the order they appear in the\n    #         pre-order traversal.\n    Z = np.asarray(Z, order='c')\n\n    if orientation not in [\"top\", \"left\", \"bottom\", \"right\"]:\n        raise ValueError(\"orientation must be one of 'top', 'left', \"\n                         \"'bottom', or 'right'\")\n\n    if labels is not None and Z.shape[0] + 1 != len(labels):\n        raise ValueError(\"Dimensions of Z and labels must be consistent.\")\n\n    is_valid_linkage(Z, throw=True, name='Z')\n    Zs = Z.shape\n    n = Zs[0] + 1\n    if type(p) in (int, float):\n        p = int(p)\n    else:\n        raise TypeError('The second argument must be a number')\n\n    if truncate_mode not in ('lastp', 'mlab', 'mtica', 'level', 'none', None):\n        # 'mlab' and 'mtica' are kept working for backwards compat.\n        raise ValueError('Invalid truncation mode.')\n\n    if truncate_mode == 'lastp' or truncate_mode == 'mlab':\n        if p &gt; n or p == 0:\n            p = n\n\n    if truncate_mode == 'mtica':\n        # 'mtica' is an alias\n        truncate_mode = 'level'\n\n    if truncate_mode == 'level':\n        if p &lt;= 0:\n            p = np.inf\n\n    if get_leaves:\n        lvs = []\n    else:\n        lvs = None\n\n    icoord_list = []\n    dcoord_list = []\n    color_list = []\n    current_color = [0]\n    currently_below_threshold = [False]\n    ivl = []  # list of leaves\n\n    if color_threshold is None or (isinstance(color_threshold, str) and\n                                   color_threshold == 'default'):\n        color_threshold = max(Z[:, 2]) * 0.7\n\n    R = {'icoord': icoord_list, 'dcoord': dcoord_list, 'ivl': ivl,\n         'leaves': lvs, 'color_list': color_list}\n\n    # Empty list will be filled in _dendrogram_calculate_info\n    contraction_marks = [] if show_contracted else None\n\n    traversal = []\n    _dendrogram_calculate_info(\n        Z=Z, p=p,\n        truncate_mode=truncate_mode,\n        color_threshold=color_threshold,\n        get_leaves=get_leaves,\n        orientation=orientation,\n        labels=labels,\n        count_sort=count_sort,\n        distance_sort=distance_sort,\n        show_leaf_counts=show_leaf_counts,\n        i=2*n - 2,\n        iv=0.0,\n        ivl=ivl,\n        n=n,\n        icoord_list=icoord_list,\n        dcoord_list=dcoord_list,\n        lvs=lvs,\n        current_color=current_color,\n        color_list=color_list,\n        currently_below_threshold=currently_below_threshold,\n        leaf_label_func=leaf_label_func,\n        contraction_marks=contraction_marks,\n        link_color_func=link_color_func,\n        above_threshold_color=above_threshold_color,\n        traversal=traversal)\n\n    if not no_plot:\n        mh = max(Z[:, 2])\n        _plot_dendrogram(icoord_list, dcoord_list, ivl, p, n, mh, orientation,\n                         no_labels, color_list,\n                         leaf_font_size=leaf_font_size,\n                         leaf_rotation=leaf_rotation,\n                         contraction_marks=contraction_marks,\n                         ax=ax,\n                         above_threshold_color=above_threshold_color)\n\n    R[\"leaves_color_list\"] = _get_leaves_color_list(R)\n    R['traversal'] = traversal\n    return R\n\n\ndef _get_leaves_color_list(R):\n    leaves_color_list = [None] * len(R['leaves'])\n    for link_x, link_y, link_color in zip(R['icoord'],\n                                          R['dcoord'],\n                                          R['color_list']):\n        for (xi, yi) in zip(link_x, link_y):\n            if yi == 0.0:  # if yi is 0.0, the point is a leaf\n                # xi of leaves are      5, 15, 25, 35, ... (see `iv_ticks`)\n                # index of leaves are   0,  1,  2,  3, ... as below\n                leaf_index = (int(xi) - 5) // 10\n                # each leaf has a same color of its link.\n                leaves_color_list[leaf_index] = link_color\n    return leaves_color_list\n\n\ndef _append_singleton_leaf_node(Z, p, n, level, lvs, ivl, leaf_label_func,\n                                i, labels):\n    # If the leaf id structure is not None and is a list then the caller\n    # to dendrogram has indicated that cluster id's corresponding to the\n    # leaf nodes should be recorded.\n\n    if lvs is not None:\n        lvs.append(int(i))\n\n    # If leaf node labels are to be displayed...\n    if ivl is not None:\n        # If a leaf_label_func has been provided, the label comes from the\n        # string returned from the leaf_label_func, which is a function\n        # passed to dendrogram.\n        if leaf_label_func:\n            ivl.append(leaf_label_func(int(i)))\n        else:\n            # Otherwise, if the dendrogram caller has passed a labels list\n            # for the leaf nodes, use it.\n            if labels is not None:\n                ivl.append(labels[int(i - n)])\n            else:\n                # Otherwise, use the id as the label for the leaf.x\n                ivl.append(str(int(i)))\n\n\ndef _append_nonsingleton_leaf_node(Z, p, n, level, lvs, ivl, leaf_label_func,\n                                   i, labels, show_leaf_counts):\n    # If the leaf id structure is not None and is a list then the caller\n    # to dendrogram has indicated that cluster id's corresponding to the\n    # leaf nodes should be recorded.\n\n    if lvs is not None:\n        lvs.append(int(i))\n    if ivl is not None:\n        if leaf_label_func:\n            ivl.append(leaf_label_func(int(i)))\n        else:\n            if show_leaf_counts:\n                ivl.append(\"(\" + str(int(Z[i - n, 3])) + \")\")\n            else:\n                ivl.append(\"\")\n\n\ndef _append_contraction_marks(Z, iv, i, n, contraction_marks):\n    _append_contraction_marks_sub(Z, iv, int(Z[i - n, 0]), n, contraction_marks)\n    _append_contraction_marks_sub(Z, iv, int(Z[i - n, 1]), n, contraction_marks)\n\n\ndef _append_contraction_marks_sub(Z, iv, i, n, contraction_marks):\n    if i &gt;= n:\n        contraction_marks.append((iv, Z[i - n, 2]))\n        _append_contraction_marks_sub(Z, iv, int(Z[i - n, 0]), n, contraction_marks)\n        _append_contraction_marks_sub(Z, iv, int(Z[i - n, 1]), n, contraction_marks)\n\n\ndef _dendrogram_calculate_info(Z, p, truncate_mode,\n                               color_threshold=np.inf, get_leaves=True,\n                               orientation='top', labels=None,\n                               count_sort=False, distance_sort=False,\n                               show_leaf_counts=False, i=-1, iv=0.0,\n                               ivl=[], n=0, icoord_list=[], dcoord_list=[],\n                               lvs=None, mhr=False,\n                               current_color=[], color_list=[],\n                               currently_below_threshold=[],\n                               leaf_label_func=None, level=0,\n                               contraction_marks=None,\n                               link_color_func=None,\n                               above_threshold_color='C0',\n                               traversal=[]):\n    \"\"\"\n    Calculate the endpoints of the links as well as the labels for the\n    the dendrogram rooted at the node with index i. iv is the independent\n    variable value to plot the left-most leaf node below the root node i\n    (if orientation='top', this would be the left-most x value where the\n    plotting of this root node i and its descendents should begin).\n    ivl is a list to store the labels of the leaf nodes. The leaf_label_func\n    is called whenever ivl != None, labels == None, and\n    leaf_label_func != None. When ivl != None and labels != None, the\n    labels list is used only for labeling the leaf nodes. When\n    ivl == None, no labels are generated for leaf nodes.\n    When get_leaves==True, a list of leaves is built as they are visited\n    in the dendrogram.\n    Returns a tuple with l being the independent variable coordinate that\n    corresponds to the midpoint of cluster to the left of cluster i if\n    i is non-singleton, otherwise the independent coordinate of the leaf\n    node if i is a leaf node.\n    Returns\n    -------\n    A tuple (left, w, h, md), where:\n      * left is the independent variable coordinate of the center of the\n        the U of the subtree\n      * w is the amount of space used for the subtree (in independent\n        variable units)\n      * h is the height of the subtree in dependent variable units\n      * md is the ``max(Z[*,2]``) for all nodes ``*`` below and including\n        the target node.\n    \"\"\"\n    traversal.append(i)\n    \n    if n == 0:\n        raise ValueError(\"Invalid singleton cluster count n.\")\n\n    if i == -1:\n        raise ValueError(\"Invalid root cluster index i.\")\n\n    if truncate_mode == 'lastp':\n        # If the node is a leaf node but corresponds to a non-singleton\n        # cluster, its label is either the empty string or the number of\n        # original observations belonging to cluster i.\n        if 2*n - p &gt; i &gt;= n:\n            d = Z[i - n, 2]\n            _append_nonsingleton_leaf_node(Z, p, n, level, lvs, ivl,\n                                           leaf_label_func, i, labels,\n                                           show_leaf_counts)\n            if contraction_marks is not None:\n                _append_contraction_marks(Z, iv + 5.0, i, n, contraction_marks)\n            return (iv + 5.0, 10.0, 0.0, d)\n        elif i &lt; n:\n            _append_singleton_leaf_node(Z, p, n, level, lvs, ivl,\n                                        leaf_label_func, i, labels)\n            return (iv + 5.0, 10.0, 0.0, 0.0)\n    elif truncate_mode == 'level':\n        if i &gt; n and level &gt; p:\n            d = Z[i - n, 2]\n            _append_nonsingleton_leaf_node(Z, p, n, level, lvs, ivl,\n                                           leaf_label_func, i, labels,\n                                           show_leaf_counts)\n            if contraction_marks is not None:\n                _append_contraction_marks(Z, iv + 5.0, i, n, contraction_marks)\n            return (iv + 5.0, 10.0, 0.0, d)\n        elif i &lt; n:\n            _append_singleton_leaf_node(Z, p, n, level, lvs, ivl,\n                                        leaf_label_func, i, labels)\n            return (iv + 5.0, 10.0, 0.0, 0.0)\n    elif truncate_mode in ('mlab',):\n        msg = \"Mode 'mlab' is deprecated in scipy 0.19.0 (it never worked).\"\n        warnings.warn(msg, DeprecationWarning)\n\n    # Otherwise, only truncate if we have a leaf node.\n    #\n    # Only place leaves if they correspond to original observations.\n    if i &lt; n:\n        _append_singleton_leaf_node(Z, p, n, level, lvs, ivl,\n                                    leaf_label_func, i, labels)\n        return (iv + 5.0, 10.0, 0.0, 0.0)\n\n    # !!! Otherwise, we don't have a leaf node, so work on plotting a\n    # non-leaf node.\n    # Actual indices of a and b\n    aa = int(Z[i - n, 0])\n    ab = int(Z[i - n, 1])\n    if aa &gt;= n:\n        # The number of singletons below cluster a\n        na = Z[aa - n, 3]\n        # The distance between a's two direct children.\n        da = Z[aa - n, 2]\n    else:\n        na = 1\n        da = 0.0\n    if ab &gt;= n:\n        nb = Z[ab - n, 3]\n        db = Z[ab - n, 2]\n    else:\n        nb = 1\n        db = 0.0\n\n    if count_sort == 'ascending' or count_sort == True:\n        # If a has a count greater than b, it and its descendents should\n        # be drawn to the right. Otherwise, to the left.\n        if na &gt; nb:\n            # The cluster index to draw to the left (ua) will be ab\n            # and the one to draw to the right (ub) will be aa\n            ua = ab\n            ub = aa\n        else:\n            ua = aa\n            ub = ab\n    elif count_sort == 'descending':\n        # If a has a count less than or equal to b, it and its\n        # descendents should be drawn to the left. Otherwise, to\n        # the right.\n        if na &gt; nb:\n            ua = aa\n            ub = ab\n        else:\n            ua = ab\n            ub = aa\n    elif distance_sort == 'ascending' or distance_sort == True:\n        # If a has a distance greater than b, it and its descendents should\n        # be drawn to the right. Otherwise, to the left.\n        if da &gt; db:\n            ua = ab\n            ub = aa\n        else:\n            ua = aa\n            ub = ab\n    elif distance_sort == 'descending':\n        # If a has a distance less than or equal to b, it and its\n        # descendents should be drawn to the left. Otherwise, to\n        # the right.\n        if da &gt; db:\n            ua = aa\n            ub = ab\n        else:\n            ua = ab\n            ub = aa\n    else:\n        ua = aa\n        ub = ab\n\n    # Updated iv variable and the amount of space used.\n    (uiva, uwa, uah, uamd) = \\\n        _dendrogram_calculate_info(\n            Z=Z, p=p,\n            truncate_mode=truncate_mode,\n            color_threshold=color_threshold,\n            get_leaves=get_leaves,\n            orientation=orientation,\n            labels=labels,\n            count_sort=count_sort,\n            distance_sort=distance_sort,\n            show_leaf_counts=show_leaf_counts,\n            i=ua, iv=iv, ivl=ivl, n=n,\n            icoord_list=icoord_list,\n            dcoord_list=dcoord_list, lvs=lvs,\n            current_color=current_color,\n            color_list=color_list,\n            currently_below_threshold=currently_below_threshold,\n            leaf_label_func=leaf_label_func,\n            level=level + 1, contraction_marks=contraction_marks,\n            link_color_func=link_color_func,\n            above_threshold_color=above_threshold_color,\n            traversal=traversal)\n\n    h = Z[i - n, 2]\n    if h &gt;= color_threshold or color_threshold &lt;= 0:\n        c = above_threshold_color\n\n        if currently_below_threshold[0]:\n            current_color[0] = (current_color[0] + 1) % len(_link_line_colors)\n        currently_below_threshold[0] = False\n    else:\n        currently_below_threshold[0] = True\n        c = _link_line_colors[current_color[0]]\n\n    (uivb, uwb, ubh, ubmd) = \\\n        _dendrogram_calculate_info(\n            Z=Z, p=p,\n            truncate_mode=truncate_mode,\n            color_threshold=color_threshold,\n            get_leaves=get_leaves,\n            orientation=orientation,\n            labels=labels,\n            count_sort=count_sort,\n            distance_sort=distance_sort,\n            show_leaf_counts=show_leaf_counts,\n            i=ub, iv=iv + uwa, ivl=ivl, n=n,\n            icoord_list=icoord_list,\n            dcoord_list=dcoord_list, lvs=lvs,\n            current_color=current_color,\n            color_list=color_list,\n            currently_below_threshold=currently_below_threshold,\n            leaf_label_func=leaf_label_func,\n            level=level + 1, contraction_marks=contraction_marks,\n            link_color_func=link_color_func,\n            above_threshold_color=above_threshold_color,\n            traversal=traversal)\n\n    max_dist = max(uamd, ubmd, h)\n\n    icoord_list.append([uiva, uiva, uivb, uivb])\n    dcoord_list.append([uah, h, h, ubh])\n    if link_color_func is not None:\n        v = link_color_func(int(i))\n        if not isinstance(v, str):\n            raise TypeError(\"link_color_func must return a matplotlib \"\n                            \"color string!\")\n        color_list.append(v)\n    else:\n        color_list.append(c)\n\n    return (((uiva + uivb) / 2), uwa + uwb, h, max_dist)\n\n\ndef is_isomorphic(T1, T2):\n    \"\"\"\n    Determine if two different cluster assignments are equivalent.\n    Parameters\n    ----------\n    T1 : array_like\n        An assignment of singleton cluster ids to flat cluster ids.\n    T2 : array_like\n        An assignment of singleton cluster ids to flat cluster ids.\n    Returns\n    -------\n    b : bool\n        Whether the flat cluster assignments `T1` and `T2` are\n        equivalent.\n    See Also\n    --------\n    linkage: for a description of what a linkage matrix is.\n    fcluster: for the creation of flat cluster assignments.\n    Examples\n    --------\n    &gt;&gt;&gt; from scipy.cluster.hierarchy import fcluster, is_isomorphic\n    &gt;&gt;&gt; from scipy.cluster.hierarchy import single, complete\n    &gt;&gt;&gt; from scipy.spatial.distance import pdist\n    Two flat cluster assignments can be isomorphic if they represent the same\n    cluster assignment, with different labels.\n    For example, we can use the `scipy.cluster.hierarchy.single`: method\n    and flatten the output to four clusters:\n    &gt;&gt;&gt; X = [[0, 0], [0, 1], [1, 0],\n    ...      [0, 4], [0, 3], [1, 4],\n    ...      [4, 0], [3, 0], [4, 1],\n    ...      [4, 4], [3, 4], [4, 3]]\n    &gt;&gt;&gt; Z = single(pdist(X))\n    &gt;&gt;&gt; T = fcluster(Z, 1, criterion='distance')\n    &gt;&gt;&gt; T\n    array([3, 3, 3, 4, 4, 4, 2, 2, 2, 1, 1, 1], dtype=int32)\n    We can then do the same using the\n    `scipy.cluster.hierarchy.complete`: method:\n    &gt;&gt;&gt; Z = complete(pdist(X))\n    &gt;&gt;&gt; T_ = fcluster(Z, 1.5, criterion='distance')\n    &gt;&gt;&gt; T_\n    array([1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4], dtype=int32)\n    As we can see, in both cases we obtain four clusters and all the data\n    points are distributed in the same way - the only thing that changes\n    are the flat cluster labels (3 =&gt; 1, 4 =&gt;2, 2 =&gt;3 and 4 =&gt;1), so both\n    cluster assignments are isomorphic:\n    &gt;&gt;&gt; is_isomorphic(T, T_)\n    True\n    \"\"\"\n    T1 = np.asarray(T1, order='c')\n    T2 = np.asarray(T2, order='c')\n\n    if type(T1) != np.ndarray:\n        raise TypeError('T1 must be a numpy array.')\n    if type(T2) != np.ndarray:\n        raise TypeError('T2 must be a numpy array.')\n\n    T1S = T1.shape\n    T2S = T2.shape\n\n    if len(T1S) != 1:\n        raise ValueError('T1 must be one-dimensional.')\n    if len(T2S) != 1:\n        raise ValueError('T2 must be one-dimensional.')\n    if T1S[0] != T2S[0]:\n        raise ValueError('T1 and T2 must have the same number of elements.')\n    n = T1S[0]\n    d1 = {}\n    d2 = {}\n    for i in range(0, n):\n        if T1[i] in d1:\n            if not T2[i] in d2:\n                return False\n            if d1[T1[i]] != T2[i] or d2[T2[i]] != T1[i]:\n                return False\n        elif T2[i] in d2:\n            return False\n        else:\n            d1[T1[i]] = T2[i]\n            d2[T2[i]] = T1[i]\n    return True\n\n\ndef maxdists(Z):\n    \"\"\"\n    Return the maximum distance between any non-singleton cluster.\n    Parameters\n    ----------\n    Z : ndarray\n        The hierarchical clustering encoded as a matrix. See\n        ``linkage`` for more information.\n    Returns\n    -------\n    maxdists : ndarray\n        A ``(n-1)`` sized numpy array of doubles; ``MD[i]`` represents\n        the maximum distance between any cluster (including\n        singletons) below and including the node with index i. More\n        specifically, ``MD[i] = Z[Q(i)-n, 2].max()`` where ``Q(i)`` is the\n        set of all node indices below and including node i.\n    See Also\n    --------\n    linkage: for a description of what a linkage matrix is.\n    is_monotonic: for testing for monotonicity of a linkage matrix.\n    Examples\n    --------\n    &gt;&gt;&gt; from scipy.cluster.hierarchy import median, maxdists\n    &gt;&gt;&gt; from scipy.spatial.distance import pdist\n    Given a linkage matrix ``Z``, `scipy.cluster.hierarchy.maxdists`\n    computes for each new cluster generated (i.e., for each row of the linkage\n    matrix) what is the maximum distance between any two child clusters.\n    Due to the nature of hierarchical clustering, in many cases this is going\n    to be just the distance between the two child clusters that were merged\n    to form the current one - that is, Z[:,2].\n    However, for non-monotonic cluster assignments such as\n    `scipy.cluster.hierarchy.median` clustering this is not always the\n    case: There may be cluster formations were the distance between the two\n    clusters merged is smaller than the distance between their children.\n    We can see this in an example:\n    &gt;&gt;&gt; X = [[0, 0], [0, 1], [1, 0],\n    ...      [0, 4], [0, 3], [1, 4],\n    ...      [4, 0], [3, 0], [4, 1],\n    ...      [4, 4], [3, 4], [4, 3]]\n    &gt;&gt;&gt; Z = median(pdist(X))\n    &gt;&gt;&gt; Z\n    array([[ 0.        ,  1.        ,  1.        ,  2.        ],\n           [ 3.        ,  4.        ,  1.        ,  2.        ],\n           [ 9.        , 10.        ,  1.        ,  2.        ],\n           [ 6.        ,  7.        ,  1.        ,  2.        ],\n           [ 2.        , 12.        ,  1.11803399,  3.        ],\n           [ 5.        , 13.        ,  1.11803399,  3.        ],\n           [ 8.        , 15.        ,  1.11803399,  3.        ],\n           [11.        , 14.        ,  1.11803399,  3.        ],\n           [18.        , 19.        ,  3.        ,  6.        ],\n           [16.        , 17.        ,  3.5       ,  6.        ],\n           [20.        , 21.        ,  3.25      , 12.        ]])\n    &gt;&gt;&gt; maxdists(Z)\n    array([1.        , 1.        , 1.        , 1.        , 1.11803399,\n           1.11803399, 1.11803399, 1.11803399, 3.        , 3.5       ,\n           3.5       ])\n    Note that while the distance between the two clusters merged when creating the\n    last cluster is 3.25, there are two children (clusters 16 and 17) whose distance\n    is larger (3.5). Thus, `scipy.cluster.hierarchy.maxdists` returns 3.5 in\n    this case.\n    \"\"\"\n    Z = np.asarray(Z, order='c', dtype=np.double)\n    is_valid_linkage(Z, throw=True, name='Z')\n\n    n = Z.shape[0] + 1\n    MD = np.zeros((n - 1,))\n    [Z] = _copy_arrays_if_base_present([Z])\n    _hierarchy.get_max_dist_for_each_cluster(Z, MD, int(n))\n    return MD\n\n\ndef maxinconsts(Z, R):\n    \"\"\"\n    Return the maximum inconsistency coefficient for each\n    non-singleton cluster and its children.\n    Parameters\n    ----------\n    Z : ndarray\n        The hierarchical clustering encoded as a matrix. See\n        `linkage` for more information.\n    R : ndarray\n        The inconsistency matrix.\n    Returns\n    -------\n    MI : ndarray\n        A monotonic ``(n-1)``-sized numpy array of doubles.\n    See Also\n    --------\n    linkage: for a description of what a linkage matrix is.\n    inconsistent: for the creation of a inconsistency matrix.\n    Examples\n    --------\n    &gt;&gt;&gt; from scipy.cluster.hierarchy import median, inconsistent, maxinconsts\n    &gt;&gt;&gt; from scipy.spatial.distance import pdist\n    Given a data set ``X``, we can apply a clustering method to obtain a\n    linkage matrix ``Z``. `scipy.cluster.hierarchy.inconsistent` can\n    be also used to obtain the inconsistency matrix ``R`` associated to\n    this clustering process:\n    &gt;&gt;&gt; X = [[0, 0], [0, 1], [1, 0],\n    ...      [0, 4], [0, 3], [1, 4],\n    ...      [4, 0], [3, 0], [4, 1],\n    ...      [4, 4], [3, 4], [4, 3]]\n    &gt;&gt;&gt; Z = median(pdist(X))\n    &gt;&gt;&gt; R = inconsistent(Z)\n    &gt;&gt;&gt; Z\n    array([[ 0.        ,  1.        ,  1.        ,  2.        ],\n           [ 3.        ,  4.        ,  1.        ,  2.        ],\n           [ 9.        , 10.        ,  1.        ,  2.        ],\n           [ 6.        ,  7.        ,  1.        ,  2.        ],\n           [ 2.        , 12.        ,  1.11803399,  3.        ],\n           [ 5.        , 13.        ,  1.11803399,  3.        ],\n           [ 8.        , 15.        ,  1.11803399,  3.        ],\n           [11.        , 14.        ,  1.11803399,  3.        ],\n           [18.        , 19.        ,  3.        ,  6.        ],\n           [16.        , 17.        ,  3.5       ,  6.        ],\n           [20.        , 21.        ,  3.25      , 12.        ]])\n    &gt;&gt;&gt; R\n    array([[1.        , 0.        , 1.        , 0.        ],\n           [1.        , 0.        , 1.        , 0.        ],\n           [1.        , 0.        , 1.        , 0.        ],\n           [1.        , 0.        , 1.        , 0.        ],\n           [1.05901699, 0.08346263, 2.        , 0.70710678],\n           [1.05901699, 0.08346263, 2.        , 0.70710678],\n           [1.05901699, 0.08346263, 2.        , 0.70710678],\n           [1.05901699, 0.08346263, 2.        , 0.70710678],\n           [1.74535599, 1.08655358, 3.        , 1.15470054],\n           [1.91202266, 1.37522872, 3.        , 1.15470054],\n           [3.25      , 0.25      , 3.        , 0.        ]])\n    Here, `scipy.cluster.hierarchy.maxinconsts` can be used to compute\n    the maximum value of the inconsistency statistic (the last column of\n    ``R``) for each non-singleton cluster and its children:\n    &gt;&gt;&gt; maxinconsts(Z, R)\n    array([0.        , 0.        , 0.        , 0.        , 0.70710678,\n           0.70710678, 0.70710678, 0.70710678, 1.15470054, 1.15470054,\n           1.15470054])\n    \"\"\"\n    Z = np.asarray(Z, order='c')\n    R = np.asarray(R, order='c')\n    is_valid_linkage(Z, throw=True, name='Z')\n    is_valid_im(R, throw=True, name='R')\n\n    n = Z.shape[0] + 1\n    if Z.shape[0] != R.shape[0]:\n        raise ValueError(\"The inconsistency matrix and linkage matrix each \"\n                         \"have a different number of rows.\")\n    MI = np.zeros((n - 1,))\n    [Z, R] = _copy_arrays_if_base_present([Z, R])\n    _hierarchy.get_max_Rfield_for_each_cluster(Z, R, MI, int(n), 3)\n    return MI\n\n\ndef maxRstat(Z, R, i):\n    \"\"\"\n    Return the maximum statistic for each non-singleton cluster and its\n    children.\n    Parameters\n    ----------\n    Z : array_like\n        The hierarchical clustering encoded as a matrix. See `linkage` for more\n        information.\n    R : array_like\n        The inconsistency matrix.\n    i : int\n        The column of `R` to use as the statistic.\n    Returns\n    -------\n    MR : ndarray\n        Calculates the maximum statistic for the i'th column of the\n        inconsistency matrix `R` for each non-singleton cluster\n        node. ``MR[j]`` is the maximum over ``R[Q(j)-n, i]``, where\n        ``Q(j)`` the set of all node ids corresponding to nodes below\n        and including ``j``.\n    See Also\n    --------\n    linkage: for a description of what a linkage matrix is.\n    inconsistent: for the creation of a inconsistency matrix.\n    Examples\n    --------\n    &gt;&gt;&gt; from scipy.cluster.hierarchy import median, inconsistent, maxRstat\n    &gt;&gt;&gt; from scipy.spatial.distance import pdist\n    Given a data set ``X``, we can apply a clustering method to obtain a\n    linkage matrix ``Z``. `scipy.cluster.hierarchy.inconsistent` can\n    be also used to obtain the inconsistency matrix ``R`` associated to\n    this clustering process:\n    &gt;&gt;&gt; X = [[0, 0], [0, 1], [1, 0],\n    ...      [0, 4], [0, 3], [1, 4],\n    ...      [4, 0], [3, 0], [4, 1],\n    ...      [4, 4], [3, 4], [4, 3]]\n    &gt;&gt;&gt; Z = median(pdist(X))\n    &gt;&gt;&gt; R = inconsistent(Z)\n    &gt;&gt;&gt; R\n    array([[1.        , 0.        , 1.        , 0.        ],\n           [1.        , 0.        , 1.        , 0.        ],\n           [1.        , 0.        , 1.        , 0.        ],\n           [1.        , 0.        , 1.        , 0.        ],\n           [1.05901699, 0.08346263, 2.        , 0.70710678],\n           [1.05901699, 0.08346263, 2.        , 0.70710678],\n           [1.05901699, 0.08346263, 2.        , 0.70710678],\n           [1.05901699, 0.08346263, 2.        , 0.70710678],\n           [1.74535599, 1.08655358, 3.        , 1.15470054],\n           [1.91202266, 1.37522872, 3.        , 1.15470054],\n           [3.25      , 0.25      , 3.        , 0.        ]])\n    `scipy.cluster.hierarchy.maxRstat` can be used to compute\n    the maximum value of each column of ``R``, for each non-singleton\n    cluster and its children:\n    &gt;&gt;&gt; maxRstat(Z, R, 0)\n    array([1.        , 1.        , 1.        , 1.        , 1.05901699,\n           1.05901699, 1.05901699, 1.05901699, 1.74535599, 1.91202266,\n           3.25      ])\n    &gt;&gt;&gt; maxRstat(Z, R, 1)\n    array([0.        , 0.        , 0.        , 0.        , 0.08346263,\n           0.08346263, 0.08346263, 0.08346263, 1.08655358, 1.37522872,\n           1.37522872])\n    &gt;&gt;&gt; maxRstat(Z, R, 3)\n    array([0.        , 0.        , 0.        , 0.        , 0.70710678,\n           0.70710678, 0.70710678, 0.70710678, 1.15470054, 1.15470054,\n           1.15470054])\n    \"\"\"\n    Z = np.asarray(Z, order='c')\n    R = np.asarray(R, order='c')\n    is_valid_linkage(Z, throw=True, name='Z')\n    is_valid_im(R, throw=True, name='R')\n    if type(i) is not int:\n        raise TypeError('The third argument must be an integer.')\n    if i &lt; 0 or i &gt; 3:\n        raise ValueError('i must be an integer between 0 and 3 inclusive.')\n\n    if Z.shape[0] != R.shape[0]:\n        raise ValueError(\"The inconsistency matrix and linkage matrix each \"\n                         \"have a different number of rows.\")\n\n    n = Z.shape[0] + 1\n    MR = np.zeros((n - 1,))\n    [Z, R] = _copy_arrays_if_base_present([Z, R])\n    _hierarchy.get_max_Rfield_for_each_cluster(Z, R, MR, int(n), i)\n    return MR\n\n\ndef leaders(Z, T):\n    \"\"\"\n    Return the root nodes in a hierarchical clustering.\n    Returns the root nodes in a hierarchical clustering corresponding\n    to a cut defined by a flat cluster assignment vector ``T``. See\n    the ``fcluster`` function for more information on the format of ``T``.\n    For each flat cluster :math:`j` of the :math:`k` flat clusters\n    represented in the n-sized flat cluster assignment vector ``T``,\n    this function finds the lowest cluster node :math:`i` in the linkage\n    tree Z, such that:\n      * leaf descendants belong only to flat cluster j\n        (i.e., ``T[p]==j`` for all :math:`p` in :math:`S(i)`, where\n        :math:`S(i)` is the set of leaf ids of descendant leaf nodes\n        with cluster node :math:`i`)\n      * there does not exist a leaf that is not a descendant with\n        :math:`i` that also belongs to cluster :math:`j`\n        (i.e., ``T[q]!=j`` for all :math:`q` not in :math:`S(i)`). If\n        this condition is violated, ``T`` is not a valid cluster\n        assignment vector, and an exception will be thrown.\n    Parameters\n    ----------\n    Z : ndarray\n        The hierarchical clustering encoded as a matrix. See\n        `linkage` for more information.\n    T : ndarray\n        The flat cluster assignment vector.\n    Returns\n    -------\n    L : ndarray\n        The leader linkage node id's stored as a k-element 1-D array,\n        where ``k`` is the number of flat clusters found in ``T``.\n        ``L[j]=i`` is the linkage cluster node id that is the\n        leader of flat cluster with id M[j]. If ``i &lt; n``, ``i``\n        corresponds to an original observation, otherwise it\n        corresponds to a non-singleton cluster.\n    M : ndarray\n        The leader linkage node id's stored as a k-element 1-D array, where\n        ``k`` is the number of flat clusters found in ``T``. This allows the\n        set of flat cluster ids to be any arbitrary set of ``k`` integers.\n        For example: if ``L[3]=2`` and ``M[3]=8``, the flat cluster with\n        id 8's leader is linkage node 2.\n    See Also\n    --------\n    fcluster: for the creation of flat cluster assignments.\n    Examples\n    --------\n    &gt;&gt;&gt; from scipy.cluster.hierarchy import ward, fcluster, leaders\n    &gt;&gt;&gt; from scipy.spatial.distance import pdist\n    Given a linkage matrix ``Z`` - obtained after apply a clustering method\n    to a dataset ``X`` - and a flat cluster assignment array ``T``:\n    &gt;&gt;&gt; X = [[0, 0], [0, 1], [1, 0],\n    ...      [0, 4], [0, 3], [1, 4],\n    ...      [4, 0], [3, 0], [4, 1],\n    ...      [4, 4], [3, 4], [4, 3]]\n    &gt;&gt;&gt; Z = ward(pdist(X))\n    &gt;&gt;&gt; Z\n    array([[ 0.        ,  1.        ,  1.        ,  2.        ],\n           [ 3.        ,  4.        ,  1.        ,  2.        ],\n           [ 6.        ,  7.        ,  1.        ,  2.        ],\n           [ 9.        , 10.        ,  1.        ,  2.        ],\n           [ 2.        , 12.        ,  1.29099445,  3.        ],\n           [ 5.        , 13.        ,  1.29099445,  3.        ],\n           [ 8.        , 14.        ,  1.29099445,  3.        ],\n           [11.        , 15.        ,  1.29099445,  3.        ],\n           [16.        , 17.        ,  5.77350269,  6.        ],\n           [18.        , 19.        ,  5.77350269,  6.        ],\n           [20.        , 21.        ,  8.16496581, 12.        ]])\n    &gt;&gt;&gt; T = fcluster(Z, 3, criterion='distance')\n    &gt;&gt;&gt; T\n    array([1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4], dtype=int32)\n    `scipy.cluster.hierarchy.leaders` returns the indices of the nodes\n    in the dendrogram that are the leaders of each flat cluster:\n    &gt;&gt;&gt; L, M = leaders(Z, T)\n    &gt;&gt;&gt; L\n    array([16, 17, 18, 19], dtype=int32)\n    (remember that indices 0-11 point to the 12 data points in ``X``,\n    whereas indices 12-22 point to the 11 rows of ``Z``)\n    `scipy.cluster.hierarchy.leaders` also returns the indices of\n    the flat clusters in ``T``:\n    &gt;&gt;&gt; M\n    array([1, 2, 3, 4], dtype=int32)\n    \"\"\"\n    Z = np.asarray(Z, order='c')\n    T = np.asarray(T, order='c')\n    if type(T) != np.ndarray or T.dtype != 'i':\n        raise TypeError('T must be a one-dimensional numpy array of integers.')\n    is_valid_linkage(Z, throw=True, name='Z')\n    if len(T) != Z.shape[0] + 1:\n        raise ValueError('Mismatch: len(T)!=Z.shape[0] + 1.')\n\n    Cl = np.unique(T)\n    kk = len(Cl)\n    L = np.zeros((kk,), dtype='i')\n    M = np.zeros((kk,), dtype='i')\n    n = Z.shape[0] + 1\n    [Z, T] = _copy_arrays_if_base_present([Z, T])\n    s = _hierarchy.leaders(Z, T, L, M, int(kk), int(n))\n    if s &gt;= 0:\n        raise ValueError(('T is not a valid assignment vector. Error found '\n                          'when examining linkage node %d (&lt; 2n-1).') % s)\n    return (L, M)\n\n\n\n\n\n\n\nTo implement cuts, the basic modification to be made is including truncation parameters such as truncate_mode and p to all wrappers around the scipy’s dendrogram function. The following changes are made:\n\ncreate_dendrogram now includes p and truncate_mode as parameters\nthe plotly class _Dendrogram now has both p and truncate_mode as object parameters\nthe get_dendrogram_traces method now passes on these parameters to its call to scipy’s dendogram function\n\n\nfrom __future__ import absolute_import\n\nfrom collections import OrderedDict\n\nfrom plotly import exceptions, optional_imports\nfrom plotly.graph_objs import graph_objs\n\n# Optional imports, may be None for users that only use our core functionality.\nnp = optional_imports.get_module(\"numpy\")\nscp = optional_imports.get_module(\"scipy\")\nsch = optional_imports.get_module(\"scipy.cluster.hierarchy\")\nscs = optional_imports.get_module(\"scipy.spatial\")\n\n\ndef create_dendrogram(\n    X,\n    p=30,\n    truncate_mode=None,\n    orientation=\"bottom\",\n    labels=None,\n    colorscale=None,\n    distfun=None,\n    linkagefun=lambda x: sch.linkage(x, \"complete\"),\n    hovertext=None,\n    color_threshold=None,\n    leaf_label_func=None,\n    \n):\n    \"\"\"\n    Function that returns a dendrogram Plotly figure object. This is a thin, modified\n    wrapper around scipy.cluster.hierarchy.dendrogram that includes truncation parameters.\n    \n    See also https://dash.plot.ly/dash-bio/clustergram.\n    :param (ndarray) X: Matrix of observations as array of arrays\n    :param (str) orientation: 'top', 'right', 'bottom', or 'left'\n    :param (list) labels: List of axis category labels(observation labels)\n    :param (list) colorscale: Optional colorscale for the dendrogram tree.\n                              Requires 8 colors to be specified, the 7th of\n                              which is ignored.  With scipy&gt;=1.5.0, the 2nd, 3rd\n                              and 6th are used twice as often as the others.\n                              Given a shorter list, the missing values are\n                              replaced with defaults and with a longer list the\n                              extra values are ignored.\n    :param (function) distfun: Function to compute the pairwise distance from\n                               the observations\n    :param (function) linkagefun: Function to compute the linkage matrix from\n                               the pairwise distances\n    :param (list[list]) hovertext: List of hovertext for constituent traces of dendrogram\n                               clusters\n    :param (double) color_threshold: Value at which the separation of clusters will be made\n    Example 1: Simple bottom oriented dendrogram\n    &gt;&gt;&gt; from plotly.figure_factory import create_dendrogram\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; X = np.random.rand(10,10)\n    &gt;&gt;&gt; fig = create_dendrogram(X)\n    &gt;&gt;&gt; fig.show()\n    Example 2: Dendrogram to put on the left of the heatmap\n    \n    &gt;&gt;&gt; from plotly.figure_factory import create_dendrogram\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; X = np.random.rand(5,5)\n    &gt;&gt;&gt; names = ['Jack', 'Oxana', 'John', 'Chelsea', 'Mark']\n    &gt;&gt;&gt; dendro = create_dendrogram(X, orientation='right', labels=names)\n    &gt;&gt;&gt; dendro.update_layout({'width':700, 'height':500}) # doctest: +SKIP\n    &gt;&gt;&gt; dendro.show()\n    Example 3: Dendrogram with Pandas\n    \n    &gt;&gt;&gt; from plotly.figure_factory import create_dendrogram\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; import pandas as pd\n    &gt;&gt;&gt; Index= ['A','B','C','D','E','F','G','H','I','J']\n    &gt;&gt;&gt; df = pd.DataFrame(abs(np.random.randn(10, 10)), index=Index)\n    &gt;&gt;&gt; fig = create_dendrogram(df, labels=Index)\n    &gt;&gt;&gt; fig.show()\n    \"\"\"\n    if not scp or not scs or not sch:\n        raise ImportError(\n            \"FigureFactory.create_dendrogram requires scipy, \\\n                            scipy.spatial and scipy.hierarchy\"\n        )\n\n    s = X.shape\n    if len(s) != 2:\n        exceptions.PlotlyError(\"X should be 2-dimensional array.\")\n\n    if distfun is None:\n        distfun = scs.distance.pdist\n\n    dendrogram = _Dendrogram(\n        X=X,\n        p=p,\n        truncate_mode=truncate_mode,\n        orientation=orientation,\n        labels=labels,\n        colorscale=colorscale,\n        distfun=distfun,\n        linkagefun=linkagefun,\n        hovertext=hovertext,\n        color_threshold=color_threshold,\n        leaf_label_func=leaf_label_func,\n    )\n\n    return graph_objs.Figure(data=dendrogram.data, layout=dendrogram.layout)\n\n\nclass _Dendrogram(object):\n    \"\"\"Refer to FigureFactory.create_dendrogram() for docstring.\"\"\"\n\n    def __init__(\n        self,\n        X,\n        p=30,\n        truncate_mode=None,\n        orientation=\"bottom\",\n        labels=None,\n        colorscale=None,\n        width=np.inf,\n        height=np.inf,\n        xaxis=\"xaxis\",\n        yaxis=\"yaxis\",\n        distfun=None,\n        linkagefun=lambda x: sch.linkage(x, \"complete\"),\n        hovertext=None,\n        color_threshold=None,\n        leaf_label_func=None,\n    ):\n        self.p = p\n        self.truncate_mode=truncate_mode\n        self.orientation=orientation\n        self.labels = labels\n        self.xaxis = xaxis\n        self.yaxis = yaxis\n        self.data = []\n        self.leaves = []\n        self.sign = {self.xaxis: 1, self.yaxis: 1}\n        self.layout = {self.xaxis: {}, self.yaxis: {}}\n        self.leaf_label_func = leaf_label_func\n\n        if self.orientation in [\"left\", \"bottom\"]:\n            self.sign[self.xaxis] = 1\n        else:\n            self.sign[self.xaxis] = -1\n\n        if self.orientation in [\"right\", \"bottom\"]:\n            self.sign[self.yaxis] = 1\n        else:\n            self.sign[self.yaxis] = -1\n\n        if distfun is None:\n            distfun = scs.distance.pdist\n\n        (dd_traces, xvals, yvals, ordered_labels, leaves) = self.get_dendrogram_traces(\n            X, colorscale, distfun, linkagefun, hovertext, color_threshold, \n        )\n\n        self.labels = ordered_labels\n        self.leaves = leaves\n        yvals_flat = yvals.flatten()\n        xvals_flat = xvals.flatten()\n\n        self.zero_vals = []\n\n        for i in range(len(yvals_flat)):\n            if yvals_flat[i] == 0.0 and xvals_flat[i] not in self.zero_vals:\n                self.zero_vals.append(xvals_flat[i])\n\n        if len(self.zero_vals) &gt; len(yvals) + 1:\n            # If the length of zero_vals is larger than the length of yvals,\n            # it means that there are wrong vals because of the identicial samples.\n            # Three and more identicial samples will make the yvals of spliting\n            # center into 0 and it will accidentally take it as leaves.\n            l_border = int(min(self.zero_vals))\n            r_border = int(max(self.zero_vals))\n            correct_leaves_pos = range(\n                l_border, r_border + 1, int((r_border - l_border) / len(yvals))\n            )\n            # Regenerating the leaves pos from the self.zero_vals with equally intervals.\n            self.zero_vals = [v for v in correct_leaves_pos]\n\n        self.zero_vals.sort()\n        self.layout = self.set_figure_layout(width, height)\n        self.data = dd_traces\n\n    def get_color_dict(self, colorscale):\n        \"\"\"\n        Returns colorscale used for dendrogram tree clusters.\n        :param (list) colorscale: Colors to use for the plot in rgb format.\n        :rtype (dict): A dict of default colors mapped to the user colorscale.\n        \"\"\"\n\n        # These are the color codes returned for dendrograms\n        # We're replacing them with nicer colors\n        # This list is the colors that can be used by dendrogram, which were\n        # determined as the combination of the default above_threshold_color and\n        # the default color palette (see scipy/cluster/hierarchy.py)\n        d = {\n            \"r\": \"red\",\n            \"g\": \"green\",\n            \"b\": \"blue\",\n            \"c\": \"cyan\",\n            \"m\": \"magenta\",\n            \"y\": \"yellow\",\n            \"k\": \"black\",\n            # TODO: 'w' doesn't seem to be in the default color\n            # palette in scipy/cluster/hierarchy.py\n            \"w\": \"white\",\n        }\n        default_colors = OrderedDict(sorted(d.items(), key=lambda t: t[0]))\n\n        if colorscale is None:\n            rgb_colorscale = [\n                \"rgb(0,116,217)\",  # blue\n                \"rgb(35,205,205)\",  # cyan\n                \"rgb(61,153,112)\",  # green\n                \"rgb(40,35,35)\",  # black\n                \"rgb(133,20,75)\",  # magenta\n                \"rgb(255,65,54)\",  # red\n                \"rgb(255,255,255)\",  # white\n                \"rgb(255,220,0)\",  # yellow\n            ]\n        else:\n            rgb_colorscale = colorscale\n\n        for i in range(len(default_colors.keys())):\n            k = list(default_colors.keys())[i]  # PY3 won't index keys\n            if i &lt; len(rgb_colorscale):\n                default_colors[k] = rgb_colorscale[i]\n\n        # add support for cyclic format colors as introduced in scipy===1.5.0\n        # before this, the colors were named 'r', 'b', 'y' etc., now they are\n        # named 'C0', 'C1', etc. To keep the colors consistent regardless of the\n        # scipy version, we try as much as possible to map the new colors to the\n        # old colors\n        # this mapping was found by inpecting scipy/cluster/hierarchy.py (see\n        # comment above).\n        new_old_color_map = [\n            (\"C0\", \"b\"),\n            (\"C1\", \"g\"),\n            (\"C2\", \"r\"),\n            (\"C3\", \"c\"),\n            (\"C4\", \"m\"),\n            (\"C5\", \"y\"),\n            (\"C6\", \"k\"),\n            (\"C7\", \"g\"),\n            (\"C8\", \"r\"),\n            (\"C9\", \"c\"),\n        ]\n        for nc, oc in new_old_color_map:\n            try:\n                default_colors[nc] = default_colors[oc]\n            except KeyError:\n                # it could happen that the old color isn't found (if a custom\n                # colorscale was specified), in this case we set it to an\n                # arbitrary default.\n                default_colors[n] = \"rgb(0,116,217)\"\n\n        return default_colors\n\n    def set_axis_layout(self, axis_key):\n        \"\"\"\n        Sets and returns default axis object for dendrogram figure.\n        :param (str) axis_key: E.g., 'xaxis', 'xaxis1', 'yaxis', yaxis1', etc.\n        :rtype (dict): An axis_key dictionary with set parameters.\n        \"\"\"\n        axis_defaults = {\n            \"type\": \"linear\",\n            \"ticks\": \"outside\",\n            \"mirror\": \"allticks\",\n            \"rangemode\": \"tozero\",\n            \"showticklabels\": True,\n            \"zeroline\": False,\n            \"showgrid\": False,\n            \"showline\": True,\n        }\n\n        if len(self.labels) != 0:\n            axis_key_labels = self.xaxis\n            if self.orientation in [\"left\", \"right\"]:\n                axis_key_labels = self.yaxis\n            if axis_key_labels not in self.layout:\n                self.layout[axis_key_labels] = {}\n            self.layout[axis_key_labels][\"tickvals\"] = [\n                zv * self.sign[axis_key] for zv in self.zero_vals\n            ]\n            self.layout[axis_key_labels][\"ticktext\"] = self.labels\n            self.layout[axis_key_labels][\"tickmode\"] = \"array\"\n\n        self.layout[axis_key].update(axis_defaults)\n\n        return self.layout[axis_key]\n\n    def set_figure_layout(self, width, height):\n        \"\"\"\n        Sets and returns default layout object for dendrogram figure.\n        \"\"\"\n        self.layout.update(\n            {\n                \"showlegend\": False,\n                \"autosize\": False,\n                \"hovermode\": \"closest\",\n                \"width\": width,\n                \"height\": height,\n            }\n        )\n\n        self.set_axis_layout(self.xaxis)\n        self.set_axis_layout(self.yaxis)\n\n        return self.layout\n\n    def get_dendrogram_traces(\n        self, X, colorscale, distfun, linkagefun, hovertext, color_threshold\n    ):\n        \"\"\"\n        Calculates all the elements needed for plotting a dendrogram.\n        :param (ndarray) X: Matrix of observations as array of arrays\n        :param (list) colorscale: Color scale for dendrogram tree clusters\n        :param (function) distfun: Function to compute the pairwise distance\n                                   from the observations\n        :param (function) linkagefun: Function to compute the linkage matrix\n                                      from the pairwise distances\n        :param (list) hovertext: List of hovertext for constituent traces of dendrogram\n        :rtype (tuple): Contains all the traces in the following order:\n            (a) trace_list: List of Plotly trace objects for dendrogram tree\n            (b) icoord: All X points of the dendrogram tree as array of arrays\n                with length 4\n            (c) dcoord: All Y points of the dendrogram tree as array of arrays\n                with length 4\n            (d) ordered_labels: leaf labels in the order they are going to\n                appear on the plot\n            (e) P['leaves']: left-to-right traversal of the leaves\n        \"\"\"\n        d = distfun(X)\n        Z = linkagefun(d)\n        P = sch.dendrogram(\n            Z,\n            p=self.p,\n            truncate_mode=self.truncate_mode,\n            orientation=self.orientation,\n            labels=self.labels,\n            no_plot=True,\n            color_threshold=color_threshold,\n            leaf_label_func=self.leaf_label_func\n        )\n\n        icoord = np.array(P[\"icoord\"])\n        dcoord = np.array(P[\"dcoord\"])\n        ordered_labels = np.array(P[\"ivl\"])\n        color_list = np.array(P[\"color_list\"])\n        colors = self.get_color_dict(colorscale)\n\n        trace_list = []\n\n        for i in range(len(icoord)):\n            # xs and ys are arrays of 4 points that make up the '∩' shapes\n            # of the dendrogram tree\n            if self.orientation in [\"top\", \"bottom\"]:\n                xs = icoord[i]\n            else:\n                xs = dcoord[i]\n\n            if self.orientation in [\"top\", \"bottom\"]:\n                ys = dcoord[i]\n            else:\n                ys = icoord[i]\n            color_key = color_list[i]\n            hovertext_label = None\n            if hovertext:\n                hovertext_label = hovertext[i]\n            trace = dict(\n                type=\"scatter\",\n                x=np.multiply(self.sign[self.xaxis], xs), \n                y=np.multiply(self.sign[self.yaxis], ys),\n                mode=\"lines\",\n                marker=dict(color=colors[color_key]),\n                text=hovertext_label,\n                hoverinfo=\"text\",\n            )\n\n            try:\n                x_index = int(self.xaxis[-1])\n            except ValueError:\n                x_index = \"\"\n\n            try:\n                y_index = int(self.yaxis[-1])\n            except ValueError:\n                y_index = \"\"\n\n            trace[\"xaxis\"] = \"x\" + x_index\n            trace[\"yaxis\"] = \"y\" + y_index\n\n            trace_list.append(trace)\n\n        return trace_list, icoord, dcoord, ordered_labels, P[\"leaves\"]\n\n\n\n\n\nIn Dendrograms with hovertext ranging a few hundred nodes, truncation is necessary\nThe Z matrix contains contains \\(n-1\\) rows each representing new clusters, where \\(n\\) is then number of data points.\n\nhigher rows signify larger clusters, ordered by the distance metric of their child clusters.\nthe row indices also correspond to the ClusterNode instances in the node list returned by scipy.cluster.hierarchy.to_tree\ndefault indexing will have the root cluster as \\(n-2\\)\n\nTruncation when truncate_mode is set to ‘lastp’ is governed by parameter p\n\np becomes the number of leaves to be displayed\nthere will be a total of \\(p - 1\\) new clusters displayed\nthe node indices will then be in range [2n-2-p, 2n-2], or range(2*n - 2 - p, 2*n - 1)\n\nIf indices are shown\nFor UI considerations, a shift in p Simpler indices, \\(j\\) can be transformations of default indices \\(i\\) for UI considerations can be given by: \\[j = 2n-2 - i\\]"
  },
  {
    "objectID": "notes/ClusteringNotes.html#dbscan",
    "href": "notes/ClusteringNotes.html#dbscan",
    "title": "Clustering Notes",
    "section": "",
    "text": "Consider a set of points in some space to be clustered. Let \\(X = \\{x_1, ..., x_n\\}\\), a set of \\(n\\) objects, \\(\\epsilon\\) be a parameter specifying a radius of a neighborhood with respect to some point. Points in DBSCAN fall into 3 categories: - a point \\(p\\) is a core point if there are at least a minimum number of points within \\(\\epsilon\\) of it, including itself - a point \\(q\\) is directly reachable from point \\(p\\) if \\(q\\) is within distance \\(\\epsilon\\), but only if \\(p\\) is a core point - a point \\(q\\) is reachable form \\(p\\) if there is a path \\(p_1, ..., p_n\\) with \\(p_1 = p\\) and \\(p_n = q\\) where each \\(p_{i+1}\\) is directly reachable from \\(p_i\\). This implies that the initial point and all points on the path must be core points, with the potential exception of \\(q\\) - all points not reachable from any other point are outliers or noise points\nReachability is not a symmetric relation, so another concept of connection is required for the extent of clusters: - two points are density-connected if there is a point \\(o\\) such that both \\(p\\) and \\(q\\) are reachable from \\(o\\)\nClusters satisfy two properties: - all points within a cluster are mutually density-connected - if a point is density-reachable from some point of the cluster, it is part of the cluster as well\n\n\n\n\nChoose minimum points \\(m_{pts}\\) threshold and \\(\\epsilon\\)\nFind the points in the \\(\\epsilon\\) neighborhood of every point, and identify the core points with more than \\(m_{pts}\\) neighbors\nFind the connected components of core points on the neighbor graph, ignoring all non-core points\nAssign each non-core point to a nearby cluster if the cluster is an \\(\\epsilon\\) neighbor, otherwise assign it to noise\n\n\n\n\nFor any possible clustering \\(C = \\{C_1, ..., C_t\\}\\) out of the set of all clusterings \\(\\mathcal{C}\\), DBSCAN minimizes the number of clusters such that every pair of points, \\(p, q\\) in a cluster is density-reachable.\n\\[\\underset{C \\subset\\mathcal{C}, d_{db}(p, q) \\le \\epsilon \\forall p, q \\in C_i \\forall C_i \\in C}\\min |C|\\]"
  },
  {
    "objectID": "notes/ClusteringNotes.html#hdbscan",
    "href": "notes/ClusteringNotes.html#hdbscan",
    "title": "Clustering Notes",
    "section": "",
    "text": "This is a hierarchical extension of DBSCAN. For a chosen \\(m_{pts}\\)\n\n\n\n\napplies to object \\(x_p \\in X\\), with respect to \\(m_{pts}\\), is defined as the distance from \\(x_p\\) to its \\(m_{pts}\\)-nearest neighbor, including itself\n\n\n\nAny object \\(x_p \\in X\\) is where the following holds: \\[d_{core}(x_p) \\le \\epsilon\\]\n\n\n\nis defined as \\[d_{mreach}(x_p, x_q) \\equiv\\max\\{d_{core}(x_p), d_{core}(x_q), d(x_p, x_q) \\}\\]\n\n\n\nThe complete graph in which the objects of \\(X\\) are vertices and the weight of each edge is the mutual reachability distance (wrt \\(m_{pts}\\)) between the respective pair of objects.\n\n\n\nIf we take a subgraph \\(G_{m_{pts}, \\epsilon}\\) obtained by removing all edges from \\(G_{m_{pts}}\\) with weights greater than \\(\\epsilon\\), then the connected components are the \\(\\epsilon\\)-core objects in the DBSCAN clusters, and remaining objects noise. Thus, DBSCAN partitions for \\(\\epsilon \\in [0, \\infty)\\) can be produced in a hierarchical way: at 0 we have the finest clusters, or leaves of a dendrogram, while at higher values, we approach the root of the dendrogram.\n\n\n\n\n\n\n\nCompute core distance wrt \\(m_{pts}\\) for all data objects in \\(X\\)\nCompute minimum spanning tree (MST) of \\(G_{m_{pts}}\\), the mutual reachability graph\nExtend the MST to obtain \\(MST_{ext}\\) by adding for each vertex, a “self edge” with the core distance of the corresponding object as weight\nExtract the HDBSCAN hierarchy as a dendrogram from \\(MST_{ext}\\)\n\n4.1 For the root of the tree, assign all objects the same label\n4.2 Iteratively remove all edges from the \\(MST_{ext}\\) in decreasing order of weight, with ties being removed simultaenously\n\n4.2.1 Before each removal, set the dendrogram scale value of the current hierarchical level as the weight of the edge(s) to be removed\n4.2.2 After each removal, assign labels to connected component(s) that contain the end vertices of removed edges, a new cluster label to a component if it still has at least one edge, else noise\n\n\n\n\n\n\nWhen reaching a lower \\(\\epsilon\\) level in the tree, we must consider what happens to the clusters. We can define a separate minimum cluster size which can be applied to the following: - if all cluster’s subcomponents are spurious, it disappears - if only one of cluster’s subcomponents is not spurious, keep original label, ie cluster shrinkage - if two or more of cluster’s subcomponents are not spurious, “true” cluster split\n\n\n\n\n\n\nWhen increasing the density threshold, we can see prominent clusters remain and shrink or split while others disappear. A cluster can be seen as a set of points that whose density \\(f(x)\\), exceed a threshold, ie. \\(\\lambda = 1/{\\epsilon}\\). For a density contour cluster \\(C_i\\) that appears at density level \\(\\lambda_{min}(C_i)\\), we can formalize its stability by defining its excess of mass as:\n\\[E(C_i) = \\int_{x \\in C_{i}}\\Big( f(x) - \\lambda_{min}(C_i)\\Big)dx\\]\nThis exhibits monotonic behavior along branches of the dendrogram, so it cannot be used to compare stabilities of nested clusters. Instead, we can define the relative excess of mass:\n\\[E_R(C_i) = \\int_{x \\in C_{i}}\\Big( \\lambda_{max}(x, C_i) - \\lambda_{min}(C_i)\\Big)dx\\]\nwhere \\(\\lambda_{max}(C_i)\\) is the density level at which \\(C_i\\) is split or disappears, and \\(\\lambda_{max}(x, C_i) = \\min\\{f(x), \\lambda_{max}(C_i)\\}\\)\nFor an HDBSCAN hierarchy, where we have finite data set \\(X\\), cluster labels, and desnity thresholds associated with each hierarchical level, we can adapt the previous expression to define the stability of a cluster \\(C_i\\) as:\n\\[ S(C_i) = \\sum_{x_j \\in C_i} \\Big( \\lambda_{max}(x, C_i) - \\lambda_{min}(C_i)\\Big) = \\sum_{x_j \\in C_i} \\Big(\\frac{1}{\\epsilon_{min}(x_j, C_i)} - \\frac{1}{\\epsilon_{max}(C_i)}\\Big)\\]\nwhere \\(\\lambda_{min}(C_i)\\) is the minimum density level at which \\(C_i\\) exists, \\(\\lambda_{max}(x_j, C_i)\\) is the density level beyond which object \\(x_j\\) no longer belongs to cluster \\(C_i\\) and \\(\\epsilon_{max}(C_i)\\) and \\(\\epsilon_{min}(x_j, C_i)\\) are the corresponding values for the threshold \\(\\epsilon\\).\n\n\n\nWith the notion of cluster stability developed, we can have extraction of the most prominent clusters formulated as an optimization problem of maximizing cluster stabilities subject to constraints:\n\\[\\underset{\\delta_2, ..., \\delta_{\\kappa}}\\max J = \\sum^\\kappa_{i=2}\\delta_iS(C_i)\\]\n\\[\\text{subject to} \\begin{cases}\n                    \\delta_i \\in \\{0,1\\}, i=2,...,\\kappa \\\\                    \n                    \\sum_{j \\in I_h} \\delta_j = 1, \\forall h \\in L\\\\\n                    \\end{cases}\\]\nwhere \\(\\delta_i\\) indicates if cluster \\(i\\) is in the flat solution, \\(L = \\{h | C_h \\text{is a leaf cluster}\\}\\) is the set of indices of leaf clusters, and \\(I_h = \\{j | j \\ne 1 \\text{ and }C_j \\text{ is an ascendant of } C_h\\}\\), the set of indices of all clusters on the path from \\(C_h\\) (included) to the root (excluded). The constraints prevent nested clusters on the same path from being selected at the same time.\nSelecting the clusters requires bottom up processing of nodes excluding the root, starting with the leaves, deciding whether to keep node \\(C_i\\) or a best so far selection of clusters in its subtrees. In the process, the total stability, \\(\\hat{S}(C_i)\\) is updated:\n\\[\\hat{S}(C_i) = \\begin{cases}\n                S(C_i), &\\text{if } C_i \\text{ is a leaf node}\\\\\n                \\max\\{S(C_i), \\hat{S}(C_{i_l}) + \\hat{S}(C_{i_r}) &\\text{if } C_i \\text{ is an internal node}\\\\\n                \\end{cases}\n                \\]\n\n\n\nInitialize \\(\\delta_2 = ... \\delta_\\kappa = 1\\) and for all leaf nodes, set \\(\\hat{S}(C_h) = S(C_h)\\)\nStarting from the deepest levels, do bottom-up (excluding root):\n\n2.1 If \\(S(C_i) &lt; \\hat{S}(C_{i_r}) + \\hat{S}(C_{i_l})\\), set \\(\\hat{S}(C_i) = \\hat{S}(C_{i_l}) + \\hat{S}(C_{i_r})\\) and set \\(\\delta_i=0\\)\n2.2 Else, set \\(\\hat{S}(C_i) = S(C_i)\\) and set \\(\\delta_{(.)} = 0\\) for all clusters in \\(C_i\\)’s subtrees.\n\n\n\n\n\n\n\nHierarchical clustering can typically be slow but thanks to parallelization, can be sped up via GPUs. Nvidia’s blog entry, Faster HDBSCAN Soft Clustering with RAPIDS cuML, shows comparisons with multiple methods on datasets of varying sizes.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn.datasets as data\n%matplotlib inline\nsns.set_context('poster')\nsns.set_style('white')\nsns.set_color_codes()\nplot_kwds = {'alpha' : 0.5, 's' : 80, 'linewidths':0}\n\n\nmoons, _ = data.make_moons(n_samples=50, noise=0.05)\nblobs, _ = data.make_blobs(n_samples=50, centers=[(-0.75,2.25), (1.0, 2.0)], cluster_std=0.25)\ntest_data = np.vstack([moons, blobs])\nplt.scatter(test_data.T[0], test_data.T[1], color='b', **plot_kwds)\n\n\n\n\n\n\n\n\n\nimport hdbscan\n\nclusterer = hdbscan.HDBSCAN(min_cluster_size=5, gen_min_span_tree=True)\nclusterer.fit(test_data)\n\nHDBSCAN(gen_min_span_tree=True)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.HDBSCANHDBSCAN(gen_min_span_tree=True)\n\n\n\nclusterer.minimum_spanning_tree_.plot(edge_cmap='viridis',\n                                      edge_alpha=0.6,\n                                      node_size=80,\n                                      edge_linewidth=2)\n\n\n\n\n\n\n\n\n\nclusterer.single_linkage_tree_.plot(cmap='viridis', colorbar=True)\n\n\n\n\n\n\n\n\n\nclusterer.condensed_tree_.plot()\n\n\n\n\n\n\n\n\n\nclusterer.condensed_tree_.plot(select_clusters=True, selection_palette=sns.color_palette())\n\n\n\n\n\n\n\n\n\npalette = sns.color_palette()\ncluster_colors = [sns.desaturate(palette[col], sat)\n                  if col &gt;= 0 else (0.5, 0.5, 0.5) for col, sat in\n                  zip(clusterer.labels_, clusterer.probabilities_)]\nplt.scatter(test_data.T[0], test_data.T[1], c=cluster_colors, **plot_kwds)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHard clustering assigns a single cluster label or noise\nSoft clustering assigns a vector of probabilities\n\n\n\n\n\nimport hdbscan\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport matplotlib as mpl\n\nfrom scipy.spatial.distance import cdist\n\n%matplotlib inline\nsns.set_context('poster')\nsns.set_style('white')\nsns.set_color_codes()\n\nplot_kwds={'alpha':0.25, 's':60, 'linewidths':0}\npalette = sns.color_palette('deep', 12)\n\n\ndata = np.load('clusterable_data.npy')\nfig = plt.figure()\nax = fig.add_subplot(111)\nplt.scatter(data.T[0], data.T[1], **plot_kwds)\nax.set_xticks([])\nax.set_yticks([]);\n\n\n\n\n\n\n\n\n\nlen(data)\n\n2309\n\n\n\n# build a clustering object to fit the data\nclusterer = hdbscan.HDBSCAN(min_cluster_size=15).fit(data)\nclusterer.labels_\n\narray([ 5,  5,  5, ..., -1, -1,  5], dtype=int64)\n\n\n\nlen(clusterer.labels_)\n\n2309\n\n\n\nclusterer.probabilities_\n\narray([1.        , 0.85883269, 0.90828071, ..., 0.        , 0.        ,\n       1.        ])\n\n\n\nclusterer.probabilities_\n\narray([1.        , 0.85883269, 0.90828071, ..., 0.        , 0.        ,\n       1.        ])\n\n\n\n# visualize clustering using scores as saturation\npal = sns.color_palette('deep', 8)\ncolors = [sns.desaturate(pal[col], sat) for col, sat in zip(clusterer.labels_,\n                                                            clusterer.probabilities_)]\nplt.scatter(data.T[0], data.T[1], c=colors, **plot_kwds)\n\n\n\n\n\n\n\n\n\n\nThis takes advantage of features to examine clustering further, as pointed out in these docs. In particular, we use the CondensedTree object, obtained via `clusterer. which is a dendrogram, or family tree of various clusters.\n\n_raw_tree is the tree in the form of a numpy recarray allowing lookup based on field names where each row represents an edge between a parent and child clusters:\n\nparent: id of parent cluster\nchild: id of child cluster\nlambda_val: inverse distance, aka density at which edge forms\nchild_size: number of points in the child cluster\n\n\n\ndef exemplars(cluster_id, condensed_tree):\n    raw_tree = condensed_tree._raw_tree\n    # Just the cluster elements of the tree, excluding singleton points\n    clusterer_tree = raw_tree[raw_tree['child_size'] &gt; 1]\n    # Get the leaf cluster nodes under the cluster we are considering\n    leaves = hdbscan.plots._recurse_leaf_dfs(clusterer_tree, cluster_id)\n    # Now collect up the last remaining points of each leaf cluster (the heart of the leaf)\n    result = np.array([])\n    for leaf in leaves:\n        max_lambda = raw_tree['lambda_val'][raw_tree['parent'] == leaf].max()\n        points = raw_tree['child'][(raw_tree['parent'] == leaf) &\n                                   (raw_tree['lambda_val'] == max_lambda)]\n        result = np.hstack((result, points))\n    return result.astype('int32')\n\ntree = clusterer.condensed_tree_\nplt.scatter(data.T[0], data.T[1], c='grey', **plot_kwds)\nfor i, c in enumerate(tree._select_clusters()):\n    c_exemplars = exemplars(c, tree)\n    plt.scatter(data.T[0][c_exemplars], data.T[1][c_exemplars], c=palette[i], **plot_kwds)\n\nC:\\Users\\Jonathan\\AppData\\Local\\Temp\\ipykernel_16588\\4108267181.py:20: UserWarning: *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n  plt.scatter(data.T[0][c_exemplars], data.T[1][c_exemplars], c=palette[i], **plot_kwds)\n\n\n\n\n\n\n\n\n\n\ndef min_dist_to_exemplar(point, cluster_exemplars, data):\n    dists = cdist([data[point]], data[cluster_exemplars.astype(np.int32)])\n    return dists.min()\n\ndef dist_vector(point, exemplar_dict, data):\n    result = {}\n    for cluster in exemplar_dict:\n        result[cluster] = min_dist_to_exemplar(point, exemplar_dict[cluster], data)\n    return np.array(list(result.values()))\n\ndef dist_membership_vector(point, exemplar_dict, data, softmax=False):\n    if softmax:\n        result = np.exp(1./dist_vector(point, exemplar_dict, data))\n        result[~np.isfinite(result)] = np.finfo(np.double).max\n    else:\n        result = 1./dist_vector(point, exemplar_dict, data)\n        result[~np.isfinite(result)] = np.finfo(np.double).max\n    result /= result.sum()\n    return result\n\n\nexemplar_dict = {c:exemplars(c, tree) for c in tree._select_clusters()}\ncolors = np.empty((data.shape[0], 3))\nfor x in range(data.shape[0]):\n    membership_vector = dist_membership_vector(x, exemplar_dict, data)\n    color = np.argmax(membership_vector)\n    saturation = membership_vector[color]\n    colors[x] = sns.desaturate(pal[color], saturation)\nplt.scatter(data.T[0], data.T[1], c=colors, **plot_kwds);\n\nC:\\Users\\Jonathan\\AppData\\Local\\Temp\\ipykernel_16588\\1075745609.py:16: RuntimeWarning: divide by zero encountered in divide\n  result = 1./dist_vector(point, exemplar_dict, data)\n\n\n\n\n\n\n\n\n\n\n\n\n\nthese are density-based memberships using cluster persistence as a baseline of comparison\nGLOSH algorithm compares cluster persistence with how long a point stayed in the cluster\nwe can compare cluster persistence with the merge height of the point with a fixed cluster in the dendrogram\n\nmerge height represents the disimilarity between two clusters\nwe can do that with multiple clusters to form a vector which can be normalized into memership scores\n\n\n\ndef max_lambda_val(cluster, tree):\n    cluster_tree = tree[tree['child_size'] &gt; 1]\n    leaves = hdbscan.plots._recurse_leaf_dfs(cluster_tree, cluster)\n    max_lambda = 0.0\n    for leaf in leaves:\n        max_lambda = max(max_lambda,\n                         tree['lambda_val'][tree['parent'] == leaf].max())\n    return max_lambda\n\ndef points_in_cluster(cluster, tree):\n    leaves = hdbscan.plots._recurse_leaf_dfs(tree, cluster)\n    return leaves\n\ndef merge_height(point, cluster, tree, point_dict):\n    cluster_row = tree[tree['child'] == cluster]\n    cluster_height = cluster_row['lambda_val'][0]\n    if point in point_dict[cluster]:\n        merge_row = tree[tree['child'] == float(point)][0]\n        return merge_row['lambda_val']\n        \n    else:\n        while point not in point_dict[cluster]:\n            parent_row = tree[tree['child'] == cluster]\n            cluster = parent_row['parent'].astype(np.float64)[0]\n        for row in tree[tree['parent'] == cluster]:\n            child_cluster = float(row['child'])\n            if child_cluster == point:\n                return row['lambda_val']\n            if child_cluster in point_dict and point in point_dict[child_cluster]:\n                return row['lambda_val']\n            \ndef per_cluster_scores(point, cluster_ids, tree, max_lambda_dict, point_dict):\n    result = {}\n    point_row = tree[tree['child'] == point]\n    point_cluster = float(point_row[0]['parent'])\n    max_lambda = max_lambda_dict[point_cluster] + 1e-8 # avoid zero lambda vals in odd cases\n    \n    for c in cluster_ids:\n        height = merge_height(point, c, tree, point_dict)\n        result[c] = (max_lambda / (max_lambda - height))\n    return result\n\ndef outlier_membership_vector(point, cluster_ids, tree,\n                              max_lambda_dict, point_dict, softmax=True):\n    if softmax:\n        result = np.exp(np.array(list(per_cluster_scores(point,\n                                                         cluster_ids,\n                                                         tree,\n                                                         max_lambda_dict,\n                                                         point_dict\n                                                         ).values())))\n        result[~np.isfinite(result)] = np.finfo(np.double).max\n    else:\n        result = np.array(list(per_cluster_scores(point,\n                                                  cluster_ids,\n                                                  tree,\n                                                  max_lambda_dict,\n                                                  point_dict\n                                                 ).values()))\n        result /= result.sum()\n    return result\n\ncluster_ids = tree._select_clusters()\nraw_tree = tree._raw_tree\nall_possible_clusters = np.arange(data.shape[0], raw_tree['parent'].max() + 1).astype(np.float64)\nmax_lambda_dict = {c:max_lambda_val(c, raw_tree) for c in all_possible_clusters}\npoint_dict = {c:set(points_in_cluster(c, raw_tree)) for c in all_possible_clusters}\ncolors = np.empty((data.shape[0], 3))\nfor x in range(data.shape[0]):\n    membership_vector = outlier_membership_vector(x, cluster_ids, raw_tree,\n                                                  max_lambda_dict, point_dict, False)\n    color = np.argmax(membership_vector)\n    saturation = membership_vector[color]\n    colors[x] = sns.desaturate(pal[color], saturation)\nplt.scatter(data.T[0], data.T[1], c=colors, **plot_kwds);\n\n\n\n\n\n\n\n\n\n\n\nTo combine distance and membership approaches we can: - view membership vectors as probability distributions - combining them can be achieved via Bayes’ rule\n\ndef combined_membership_vector(point, data, tree, exemplar_dict, cluster_ids,\n                    max_lambda_dict, point_dict, softmax=False):\n    raw_tree = tree._raw_tree\n    dist_vec = dist_membership_vector(point, exemplar_dict, data, softmax)\n    outl_vec = outlier_membership_vector(point, cluster_ids, raw_tree,\n                                         max_lambda_dict, point_dict, softmax)\n    result = dist_vec * outl_vec\n    result /= result.sum()\n    return result\n\ncolors = np.empty((data.shape[0], 3))\nfor x in range(data.shape[0]):\n    membership_vector = combined_membership_vector(x, data, tree, exemplar_dict, cluster_ids,\n                                                   max_lambda_dict, point_dict, False)\n    color = np.argmax(membership_vector)\n    saturation = membership_vector[color]\n    colors[x] = sns.desaturate(pal[color], saturation)\nplt.scatter(data.T[0], data.T[1], c=colors, **plot_kwds);\n\nC:\\Users\\Jonathan\\AppData\\Local\\Temp\\ipykernel_16588\\1075745609.py:16: RuntimeWarning: divide by zero encountered in divide\n  result = 1./dist_vector(point, exemplar_dict, data)\n\n\n\n\n\n\n\n\n\n\n\n\nThe previous computation was the probability vector that a point is in each cluster, given that the point is in a cluster. To convert the conditional probability to a joint one, we need to: - multiply it by the probability that that there is a cluster to which the point belongs - this can be estimated from the merge height and comparing it with the max density - this should result in a number between 0 and 1\n\nplotting can show the $ $ cluster and the corresponding color\n\n\ndef prob_in_some_cluster(point, tree, cluster_ids, point_dict, max_lambda_dict):\n    heights = []\n    for cluster in cluster_ids:\n        heights.append(merge_height(point, cluster, tree._raw_tree, point_dict))\n    height = max(heights)\n    nearest_cluster = cluster_ids[np.argmax(heights)]\n    max_lambda = max_lambda_dict[nearest_cluster]\n    return height / max_lambda\n\ncolors = np.empty((data.shape[0], 3))\nfor x in range(data.shape[0]):\n    membership_vector = combined_membership_vector(x, data, tree, exemplar_dict, cluster_ids,\n                                                   max_lambda_dict, point_dict, False)\n    membership_vector *= prob_in_some_cluster(x, tree, cluster_ids, point_dict, max_lambda_dict)\n    color = np.argmax(membership_vector)\n    saturation = membership_vector[color]\n    colors[x] = sns.desaturate(pal[color], saturation)\nplt.scatter(data.T[0], data.T[1], c=colors, **plot_kwds);\n\nC:\\Users\\Jonathan\\AppData\\Local\\Temp\\ipykernel_16588\\1075745609.py:16: RuntimeWarning: divide by zero encountered in divide\n  result = 1./dist_vector(point, exemplar_dict, data)\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat are the general steps involved in performing a soft clustering analysis?\n\ndimensionality reduction as a prerequisite to 2D visualization:\n\nPCA\nTSNE\nUMAP\n\n\nvisualization of raw data\nsoft clustering via HDBSCAN\nvisualization of cluster membership\n\nhard clustering color coding\nsoft clustering color saturation\n\n\nquantitative analysis of cluster membership probabilities\n\n\nfrom sklearn import datasets\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport hdbscan\n\n# loading raw data, 8x8 gray scale of handwritten digits, plotting low dimensional projection\ndigits = datasets.load_digits()\ndata = digits.data\nprojection = TSNE().fit_transform(data)\nplt.scatter(*projection.T, **plot_kwds)\n\n\n# setting up a clustering object, fitting and clustering the data \nclusterer = hdbscan.HDBSCAN(min_cluster_size=10, prediction_data=True).fit(data)\ncolor_palette = sns.color_palette('Paired', 12)\ncluster_colors = [color_palette[x] if x &gt;= 0\n                  else (0.5, 0.5, 0.5)\n                  for x in clusterer.labels_]\ncluster_member_colors = [sns.desaturate(x, p) for x, p in\n                         zip(cluster_colors, clusterer.probabilities_)]\nplt.scatter(*projection.T, s=50, linewidth=0, c=cluster_member_colors, alpha=0.25)\n\n\n\n\n\n\n\n\nSome of the data is noisy, so we can examine the noisy ones more through soft clustering.\n\nsoft_clusters = hdbscan.all_points_membership_vectors(clusterer)\ncolor_palette = sns.color_palette('Paired', 12)\ncluster_colors = [color_palette[np.argmax(x)]\n                  for x in soft_clusters]\nplt.scatter(*projection.T, s=50, linewidth=0, c=cluster_colors, alpha=0.25)\n\n\n\n\n\n\n\n\nWe can show uncertainty by coupling probability with desaturation of color. The higher the probability, the more saturated or pure the color. The lower the probability, the more gray it will be. Here we can see that desaturation is a harsh treatment with lots of gray, and that a lower limit may be more visually meaningful.\n\ncolor_palette = sns.color_palette('Paired', 12)\ncluster_colors = [sns.desaturate(color_palette[np.argmax(x)], np.max(x))\n                  for x in soft_clusters]\nplt.scatter(*projection.T, s=50, linewidth=0, c=cluster_colors, alpha=0.25)\n\n\n\n\n\n\n\n\nOne question to investigate is what points have high likelihoods for two clusters and low for the others. It’s worthwhile to note that the probabilities are joint ones, and that points have a probability of not being in a cluster.\n\ndef top_two_probs_diff(probs):\n    sorted_probs = np.sort(probs)\n    return sorted_probs[-1] - sorted_probs[-2]\n\n# Compute the differences between the top two probabilities\ndiffs = np.array([top_two_probs_diff(x) for x in soft_clusters])\n# Select out the indices that have a small difference, and a larger total probability, extract from tuple form\nmixed_points = np.where((diffs &lt; 0.001)  & (np.sum(soft_clusters, axis=1) &gt; 0.5))[0]\n\ncolors = [(0.75, 0.1, 0.1) if x in mixed_points\n          else (0.5, 0.5, 0.5) for x in range(data.shape[0])]\nplt.scatter(*projection.T, s=50, linewidth=0, c=colors, alpha=0.5)\n\n\n\n\n\n\n\n\n\nfig = plt.figure()\nfor i, image in enumerate(digits.images[mixed_points][:16]):\n    ax = fig.add_subplot(4,4,i+1)\n    ax.imshow(image)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nplt.hist(np.sum(soft_clusters, axis=1))\n\n(array([ 36.,  47.,  92., 171., 332., 229., 181., 297., 153., 259.]),\n array([0.58730357, 0.62857321, 0.66984285, 0.7111125 , 0.75238214,\n        0.79365178, 0.83492143, 0.87619107, 0.91746071, 0.95873036,\n        1.        ]),\n &lt;BarContainer object of 10 artists&gt;)"
  },
  {
    "objectID": "notes/ClusteringNotes.html#k-means-clustering",
    "href": "notes/ClusteringNotes.html#k-means-clustering",
    "title": "Clustering Notes",
    "section": "",
    "text": "Given a set of observations \\((x_1, x_2, ..., x_n)\\) in a \\(d\\)-dimensional vector space, k-means clustering partitions the \\(n\\) observations into \\(k\\) sets, \\(\\textbf{S} = \\{S_1, S_2, ... S_k\\}\\). The goal is to minimize the variance within each cluster:\n\\[\\underset{\\textbf{S}}{\\arg\\min} \\sum^k_{i=1} \\sum_{x\\in S_i} ||x - \\mu_i||^2 = \\underset{\\textbf{S}}{\\arg\\min}\\sum^k_{i=1}|S_i|\\text{Var}S_i\\]\nwhere \\(\\mu_i\\) is the mean or centroid of the points in \\(S_i\\).\n\n\n\n\n\nFirst choose \\(k\\) and initialize randomly chosen \\(k\\) centroids at \\(t = 0\\), the initial step.\n\n\n\nAssign each observation to the cluster with the nearest mean, in terms of Euclidean distance. Each set is thus:\n\\[S^{(t)}_i = \\big\\{x_p : ||x_p - m_i^{(t)}||^2 \\le ||x_p - m_j^{(t)}||^2 \\forall j, 1 \\le j \\le k \\big\\}\\]\n\n\n\nAfter assignment, update to find the new means for the next iteration, \\(t + 1\\):\n\\[ m_i^{t + 1} = \\frac{1}{|S_i^{(t)}|}\\sum_{x_j \\in S_i^{(t)}}x_j\\]\nThe number of computations scales with the number of samples \\(n\\), the number of centers \\(k\\), the number of dimensions \\(d\\) and the number of iterations \\(i\\), for \\(O(nkdi)\\)\n\n\n\n\n\n# import statements\nfrom sklearn.datasets import make_blobs\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set('notebook')\n# Generate and plot data with predetermined model and classes\nX, y = make_blobs(n_samples=200, n_features=2, centers=4,\n                  cluster_std=1.6, random_state=50)\n\nplt.scatter(X[:,0], X[:,1], c=y, cmap='viridis')\nplt.xlim(-15,15)\nplt.ylim(-15,15)\nplt.show()\n\n\n\n\n\n\n\n\n\nY = np.zeros((200, 2))\nY[:,0] = y\nY\n\narray([[2., 0.],\n       [0., 0.],\n       [3., 0.],\n       [3., 0.],\n       [1., 0.],\n       [0., 0.],\n       [2., 0.],\n       [0., 0.],\n       [0., 0.],\n       [1., 0.],\n       [1., 0.],\n       [2., 0.],\n       [1., 0.],\n       [1., 0.],\n       [1., 0.],\n       [3., 0.],\n       [2., 0.],\n       [2., 0.],\n       [1., 0.],\n       [1., 0.],\n       [1., 0.],\n       [3., 0.],\n       [2., 0.],\n       [2., 0.],\n       [2., 0.],\n       [3., 0.],\n       [2., 0.],\n       [1., 0.],\n       [3., 0.],\n       [2., 0.],\n       [2., 0.],\n       [1., 0.],\n       [1., 0.],\n       [0., 0.],\n       [0., 0.],\n       [1., 0.],\n       [1., 0.],\n       [1., 0.],\n       [3., 0.],\n       [3., 0.],\n       [2., 0.],\n       [1., 0.],\n       [3., 0.],\n       [3., 0.],\n       [0., 0.],\n       [0., 0.],\n       [2., 0.],\n       [3., 0.],\n       [3., 0.],\n       [2., 0.],\n       [0., 0.],\n       [1., 0.],\n       [3., 0.],\n       [2., 0.],\n       [3., 0.],\n       [3., 0.],\n       [0., 0.],\n       [1., 0.],\n       [1., 0.],\n       [3., 0.],\n       [0., 0.],\n       [0., 0.],\n       [2., 0.],\n       [0., 0.],\n       [0., 0.],\n       [2., 0.],\n       [0., 0.],\n       [3., 0.],\n       [2., 0.],\n       [2., 0.],\n       [0., 0.],\n       [1., 0.],\n       [2., 0.],\n       [0., 0.],\n       [3., 0.],\n       [3., 0.],\n       [1., 0.],\n       [3., 0.],\n       [3., 0.],\n       [0., 0.],\n       [1., 0.],\n       [3., 0.],\n       [1., 0.],\n       [1., 0.],\n       [1., 0.],\n       [1., 0.],\n       [2., 0.],\n       [0., 0.],\n       [2., 0.],\n       [1., 0.],\n       [0., 0.],\n       [2., 0.],\n       [0., 0.],\n       [3., 0.],\n       [3., 0.],\n       [1., 0.],\n       [0., 0.],\n       [3., 0.],\n       [0., 0.],\n       [3., 0.],\n       [1., 0.],\n       [0., 0.],\n       [3., 0.],\n       [2., 0.],\n       [3., 0.],\n       [0., 0.],\n       [3., 0.],\n       [1., 0.],\n       [1., 0.],\n       [2., 0.],\n       [0., 0.],\n       [1., 0.],\n       [2., 0.],\n       [2., 0.],\n       [1., 0.],\n       [3., 0.],\n       [2., 0.],\n       [2., 0.],\n       [3., 0.],\n       [0., 0.],\n       [2., 0.],\n       [2., 0.],\n       [2., 0.],\n       [2., 0.],\n       [0., 0.],\n       [2., 0.],\n       [2., 0.],\n       [3., 0.],\n       [2., 0.],\n       [0., 0.],\n       [1., 0.],\n       [0., 0.],\n       [0., 0.],\n       [3., 0.],\n       [2., 0.],\n       [3., 0.],\n       [0., 0.],\n       [2., 0.],\n       [2., 0.],\n       [1., 0.],\n       [2., 0.],\n       [3., 0.],\n       [2., 0.],\n       [1., 0.],\n       [1., 0.],\n       [2., 0.],\n       [0., 0.],\n       [3., 0.],\n       [0., 0.],\n       [1., 0.],\n       [2., 0.],\n       [1., 0.],\n       [3., 0.],\n       [0., 0.],\n       [0., 0.],\n       [3., 0.],\n       [2., 0.],\n       [0., 0.],\n       [0., 0.],\n       [0., 0.],\n       [3., 0.],\n       [3., 0.],\n       [1., 0.],\n       [0., 0.],\n       [1., 0.],\n       [1., 0.],\n       [1., 0.],\n       [3., 0.],\n       [0., 0.],\n       [0., 0.],\n       [0., 0.],\n       [0., 0.],\n       [1., 0.],\n       [3., 0.],\n       [0., 0.],\n       [2., 0.],\n       [1., 0.],\n       [0., 0.],\n       [2., 0.],\n       [3., 0.],\n       [0., 0.],\n       [3., 0.],\n       [3., 0.],\n       [2., 0.],\n       [1., 0.],\n       [0., 0.],\n       [1., 0.],\n       [2., 0.],\n       [3., 0.],\n       [0., 0.],\n       [3., 0.],\n       [3., 0.],\n       [1., 0.],\n       [3., 0.],\n       [0., 0.],\n       [2., 0.],\n       [1., 0.],\n       [3., 0.],\n       [1., 0.],\n       [2., 0.]])\n\n\n\nfrom sklearn.cluster import KMeans\n\n\nn_clusters=4\n\n# Fit and predict clusters\nkmeans = KMeans(n_clusters=n_clusters)\nkmeans.fit(X)\nprint(kmeans.cluster_centers_)\ny_km = kmeans.fit_predict(X)\n\n# Plot predictions, one class at a time\nfor i, c in enumerate(['red', 'black', 'blue', 'cyan']):\n    plt.scatter(X[y_km ==i,0], X[y_km == i,1], s=100, c=c)\nplt.show()\n\n[[-5.56465793 -2.34988939]\n [-1.92101646  5.21673484]\n [ 0.05161133 -5.35489826]\n [-2.40167949 10.17352695]]"
  },
  {
    "objectID": "notes/ClusteringNotes.html#hierarchical-clustering",
    "href": "notes/ClusteringNotes.html#hierarchical-clustering",
    "title": "Clustering Notes",
    "section": "",
    "text": "Hierarchical clustering involves starting with treating each observation as a set, and then at the following step, creating a new cluster from the two “nearest” clusters, according to the defined distance metric. There are multiple ways the idea of a distance metric can extend to clusters. Some common choices include the average, closest members, or farthest nembers.\n\n# import hierarchical clustering libraries\nfrom sklearn.cluster import AgglomerativeClustering\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom scipy.cluster.hierarchy import dendrogram, linkage, to_tree\n%load_ext autoreload\n%autoreload 2\nsns.set()\ndata = [[i] for i in [-30, 4, 1, 2, 5, 6, 10, 50, 75, 100]]\n\n\ndef llf(id):\n    return str(id)\n\n\nZ = linkage(data, 'ward')\nfig = plt.figure(figsize=(20, 10))\ndn_truncated = dendrogram(Z, orientation='right', p=4, truncate_mode='lastp', leaf_label_func=llf)\n\n\n\n\n\n\n\n\n\ndn = dendrogram(Z, orientation='right', p=4, truncate_mode=None, leaf_label_func=llf)\n\n\n\n\n\n\n\n\n\ndn_truncated\n\n{'icoord': [[5.0, 5.0, 15.0, 15.0],\n  [25.0, 25.0, 35.0, 35.0],\n  [10.0, 10.0, 30.0, 30.0]],\n 'dcoord': [[0.0, 43.30127018922193, 43.30127018922193, 0.0],\n  [0.0, 45.389321169086415, 45.389321169086415, 0.0],\n  [43.30127018922193,\n   154.2898015331631,\n   154.2898015331631,\n   45.389321169086415]],\n 'ivl': ['9', '15', '0', '14'],\n 'leaves': [9, 15, 0, 14],\n 'color_list': ['C1', 'C2', 'C0'],\n 'leaves_color_list': ['C1', 'C1', 'C2', 'C2'],\n 'traversal': [18, 16, 9, 15, 17, 0, 14]}\n\n\n\ndn['icoord']\n\n[[15.0, 15.0, 25.0, 25.0],\n [5.0, 5.0, 20.0, 20.0],\n [55.0, 55.0, 65.0, 65.0],\n [85.0, 85.0, 95.0, 95.0],\n [75.0, 75.0, 90.0, 90.0],\n [60.0, 60.0, 82.5, 82.5],\n [45.0, 45.0, 71.25, 71.25],\n [35.0, 35.0, 58.125, 58.125],\n [12.5, 12.5, 46.5625, 46.5625]]\n\n\n\ndn['dcoord']\n\n[[0.0, 25.0, 25.0, 0.0],\n [0.0, 43.30127018922193, 43.30127018922193, 25.0],\n [0.0, 1.0, 1.0, 0.0],\n [0.0, 1.0, 1.0, 0.0],\n [0.0, 1.7320508075688772, 1.7320508075688772, 1.0],\n [1.0, 5.422176684690383, 5.422176684690383, 1.7320508075688772],\n [0.0, 8.262364471909155, 8.262364471909155, 5.422176684690383],\n [0.0, 45.389321169086415, 45.389321169086415, 8.262364471909155],\n [43.30127018922193, 154.2898015331631, 154.2898015331631, 45.389321169086415]]\n\n\n\ndn_truncated['icoord']\n\n[[5.0, 5.0, 15.0, 15.0], [25.0, 25.0, 35.0, 35.0], [10.0, 10.0, 30.0, 30.0]]\n\n\n\ndn_truncated['dcoord']\n\n[[0.0, 43.30127018922193, 43.30127018922193, 0.0],\n [0.0, 45.389321169086415, 45.389321169086415, 0.0],\n [43.30127018922193, 154.2898015331631, 154.2898015331631, 45.389321169086415]]\n\n\n\nk = 4\nfor i in zip(dn['icoord'][k], dn['dcoord'][k]):\n    print(i)\n\n(75.0, 0.0)\n(75.0, 1.7320508075688772)\n(90.0, 1.7320508075688772)\n(90.0, 1.0)\n\n\n\n\nFull documentation for Scipy’s implementation is found here. The original observations are treated as \\(n\\) clusters. There are \\(n-1\\) new clusters created beyond these total, with indices ranging from \\(n+1\\) to \\(2n-1\\), ordered by the inter-cluster distance. Each of thse new clusters is made from two other clusters. The \\(n-1 \\times 4\\) linkage matrix \\(Z\\) encodes information about the new clusters in the following manner: Each row \\(i\\) (keeping in mind zero indexing) describes the \\((i+1)\\)th cluster by listing the indices of the two source clusters, the inter-cluster distance between these two clusters, and finally, the number of members of the new cluster.\n\nZ\n\narray([[  2.        ,   3.        ,   1.        ,   2.        ],\n       [  1.        ,   4.        ,   1.        ,   2.        ],\n       [  5.        ,  11.        ,   1.73205081,   3.        ],\n       [ 10.        ,  12.        ,   5.42217668,   5.        ],\n       [  6.        ,  13.        ,   8.26236447,   6.        ],\n       [  7.        ,   8.        ,  25.        ,   2.        ],\n       [  9.        ,  15.        ,  43.30127019,   3.        ],\n       [  0.        ,  14.        ,  45.38932117,   7.        ],\n       [ 16.        ,  17.        , 154.28980153,  10.        ]])\n\n\n\nplt.show()\n\n\n\n\n\nto_tree(Z)\nrootnode, nodelist = to_tree(Z, rd=True)\nrootnode\n\n&lt;__main__.ClusterNode at 0x29427b86370&gt;\n\n\n\nrootnode.get_right().pre_order()\n\n[0, 6, 2, 3, 5, 1, 4]\n\n\n\nlen(nodelist)\n\n19\n\n\n\nnodelist[4].pre_order()\n\n[4]\n\n\n\n\n\n\np =6\n\n\nnp.array(data)\n\narray([[-30],\n       [  4],\n       [  1],\n       [  2],\n       [  5],\n       [  6],\n       [ 10],\n       [ 50],\n       [ 75],\n       [100]])\n\n\n\nn = len(data)\n\n\nlen(range(2*n-p + 1, 2*n))\n\n5\n\n\n\ntraversal = dn_truncated['traversal']\n\n\ntraversal.reverse()\n\n\nlen(traversal)\n\n7\n\n\n\ntraversal[p:]\n\n[14]\n\n\n\n# import plotly.figure_factory as ff\nimport numpy as np\nnp.random.seed(1)\nn = len(data)\np = 4\nfig = create_dendrogram(np.array(data), hovertext=traversal[p:],\n                        labels=data, orientation='left', p=p, truncate_mode='lastp', leaf_label_func=llf)\nfig.update_layout(width=800, height=500)\nfig.show()\n\n                                                \n\n\n\nnodelist[13].pre_order()\n\n[2, 3, 5, 1, 4]\n\n\n\ndn_truncated\n\n{'icoord': [[15.0, 15.0, 25.0, 25.0],\n  [5.0, 5.0, 20.0, 20.0],\n  [45.0, 45.0, 55.0, 55.0],\n  [35.0, 35.0, 50.0, 50.0],\n  [12.5, 12.5, 42.5, 42.5]],\n 'dcoord': [[0.0, 25.0, 25.0, 0.0],\n  [0.0, 43.30127018922193, 43.30127018922193, 25.0],\n  [0.0, 8.262364471909155, 8.262364471909155, 0.0],\n  [0.0, 45.389321169086415, 45.389321169086415, 8.262364471909155],\n  [43.30127018922193,\n   154.2898015331631,\n   154.2898015331631,\n   45.389321169086415]],\n 'ivl': ['9', '7', '8', '0', '6', '13'],\n 'leaves': [9, 7, 8, 0, 6, 13],\n 'color_list': ['C1', 'C1', 'C2', 'C2', 'C0'],\n 'leaves_color_list': ['C1', 'C1', 'C1', 'C2', 'C2', 'C2']}\n\n\n\nZ\n\narray([[  2.        ,   3.        ,   1.        ,   2.        ],\n       [  1.        ,   4.        ,   1.        ,   2.        ],\n       [  5.        ,  11.        ,   1.73205081,   3.        ],\n       [ 10.        ,  12.        ,   5.42217668,   5.        ],\n       [  6.        ,  13.        ,   8.26236447,   6.        ],\n       [  7.        ,   8.        ,  25.        ,   2.        ],\n       [  9.        ,  15.        ,  43.30127019,   3.        ],\n       [  0.        ,  14.        ,  45.38932117,   7.        ],\n       [ 16.        ,  17.        , 154.28980153,  10.        ]])\n\n\n\ndn\n\n{'icoord': [[15.0, 15.0, 25.0, 25.0],\n  [5.0, 5.0, 20.0, 20.0],\n  [55.0, 55.0, 65.0, 65.0],\n  [85.0, 85.0, 95.0, 95.0],\n  [75.0, 75.0, 90.0, 90.0],\n  [60.0, 60.0, 82.5, 82.5],\n  [45.0, 45.0, 71.25, 71.25],\n  [35.0, 35.0, 58.125, 58.125],\n  [12.5, 12.5, 46.5625, 46.5625]],\n 'dcoord': [[0.0, 25.0, 25.0, 0.0],\n  [0.0, 43.30127018922193, 43.30127018922193, 25.0],\n  [0.0, 1.0, 1.0, 0.0],\n  [0.0, 1.0, 1.0, 0.0],\n  [0.0, 1.7320508075688772, 1.7320508075688772, 1.0],\n  [1.0, 5.422176684690383, 5.422176684690383, 1.7320508075688772],\n  [0.0, 8.262364471909155, 8.262364471909155, 5.422176684690383],\n  [0.0, 45.389321169086415, 45.389321169086415, 8.262364471909155],\n  [43.30127018922193,\n   154.2898015331631,\n   154.2898015331631,\n   45.389321169086415]],\n 'ivl': ['9', '7', '8', '0', '6', '2', '3', '5', '1', '4'],\n 'leaves': [9, 7, 8, 0, 6, 2, 3, 5, 1, 4],\n 'color_list': ['C1', 'C1', 'C2', 'C2', 'C2', 'C2', 'C2', 'C2', 'C0'],\n 'leaves_color_list': ['C1',\n  'C1',\n  'C1',\n  'C2',\n  'C2',\n  'C2',\n  'C2',\n  'C2',\n  'C2',\n  'C2'],\n 'traversal': [18,\n  16,\n  9,\n  15,\n  7,\n  8,\n  17,\n  0,\n  14,\n  6,\n  13,\n  10,\n  2,\n  3,\n  12,\n  5,\n  11,\n  1,\n  4]}\n\n\n\ndn_truncated\n\n{'icoord': [[15.0, 15.0, 25.0, 25.0],\n  [5.0, 5.0, 20.0, 20.0],\n  [45.0, 45.0, 55.0, 55.0],\n  [35.0, 35.0, 50.0, 50.0],\n  [12.5, 12.5, 42.5, 42.5]],\n 'dcoord': [[0.0, 25.0, 25.0, 0.0],\n  [0.0, 43.30127018922193, 43.30127018922193, 25.0],\n  [0.0, 8.262364471909155, 8.262364471909155, 0.0],\n  [0.0, 45.389321169086415, 45.389321169086415, 8.262364471909155],\n  [43.30127018922193,\n   154.2898015331631,\n   154.2898015331631,\n   45.389321169086415]],\n 'ivl': ['9', '7', '8', '0', '6', '(5)'],\n 'leaves': [9, 7, 8, 0, 6, 13],\n 'color_list': ['C1', 'C1', 'C2', 'C2', 'C0'],\n 'leaves_color_list': ['C1', 'C1', 'C1', 'C2', 'C2', 'C2']}\n\n\n\ni = 0\nprint(dn_truncated['dcoord'][i])\nprint(dn_truncated['icoord'][i])\n\n[0.0, 25.0, 25.0, 0.0]\n[15.0, 15.0, 25.0, 25.0]\n\n\n\ndn_truncated['dcoord']\n\n[[0.0, 25.0, 25.0, 0.0],\n [0.0, 43.30127018922193, 43.30127018922193, 25.0],\n [0.0, 8.262364471909155, 8.262364471909155, 0.0],\n [0.0, 45.389321169086415, 45.389321169086415, 8.262364471909155],\n [43.30127018922193, 154.2898015331631, 154.2898015331631, 45.389321169086415]]\n\n\n\ndn_truncated['icoord']\n\n[[15.0, 15.0, 25.0, 25.0],\n [5.0, 5.0, 20.0, 20.0],\n [45.0, 45.0, 55.0, 55.0],\n [35.0, 35.0, 50.0, 50.0],\n [12.5, 12.5, 42.5, 42.5]]\n\n\n\nnodelist[].pre_order()\n\n[4]\n\n\n\nnodelist[9].id\n\n\ndata[2]\n\n[1]\n\n\n\n#from plotly.figure_factory import create_dendrogram\nimport numpy as np\nimport pandas as pd\nIndex= ['A','B','C','D','E','F','G','H','I','J']\ndf = pd.DataFrame(abs(np.random.randn(10, 10)), index=Index)\nfig = create_dendrogram(df, labels=Index, orientation='left', hovertext=list(range(19)))\nfig.show()"
  },
  {
    "objectID": "notes/ClusteringNotes.html#implementing-cluster-order-in-scipy",
    "href": "notes/ClusteringNotes.html#implementing-cluster-order-in-scipy",
    "title": "Clustering Notes",
    "section": "",
    "text": "_dendrogram_calculate_info now has a list to track the cluster indices during construction of the dendrogram\n\n\n\"\"\"\nHierarchical clustering (:mod:`scipy.cluster.hierarchy`)\n========================================================\n.. currentmodule:: scipy.cluster.hierarchy\nThese functions cut hierarchical clusterings into flat clusterings\nor find the roots of the forest formed by a cut by providing the flat\ncluster ids of each observation.\n.. autosummary::\n   :toctree: generated/\n   fcluster\n   fclusterdata\n   leaders\nThese are routines for agglomerative clustering.\n.. autosummary::\n   :toctree: generated/\n   linkage\n   single\n   complete\n   average\n   weighted\n   centroid\n   median\n   ward\nThese routines compute statistics on hierarchies.\n.. autosummary::\n   :toctree: generated/\n   cophenet\n   from_mlab_linkage\n   inconsistent\n   maxinconsts\n   maxdists\n   maxRstat\n   to_mlab_linkage\nRoutines for visualizing flat clusters.\n.. autosummary::\n   :toctree: generated/\n   dendrogram\nThese are data structures and routines for representing hierarchies as\ntree objects.\n.. autosummary::\n   :toctree: generated/\n   ClusterNode\n   leaves_list\n   to_tree\n   cut_tree\n   optimal_leaf_ordering\nThese are predicates for checking the validity of linkage and\ninconsistency matrices as well as for checking isomorphism of two\nflat cluster assignments.\n.. autosummary::\n   :toctree: generated/\n   is_valid_im\n   is_valid_linkage\n   is_isomorphic\n   is_monotonic\n   correspond\n   num_obs_linkage\nUtility routines for plotting:\n.. autosummary::\n   :toctree: generated/\n   set_link_color_palette\nUtility classes:\n.. autosummary::\n   :toctree: generated/\n   DisjointSet -- data structure for incremental connectivity queries\n\"\"\"\n# Copyright (C) Damian Eads, 2007-2008. New BSD License.\n\n# hierarchy.py (derived from cluster.py, http://scipy-cluster.googlecode.com)\n#\n# Author: Damian Eads\n# Date:   September 22, 2007\n#\n# Copyright (c) 2007, 2008, Damian Eads\n#\n# All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#   - Redistributions of source code must retain the above\n#     copyright notice, this list of conditions and the\n#     following disclaimer.\n#   - Redistributions in binary form must reproduce the above copyright\n#     notice, this list of conditions and the following disclaimer\n#     in the documentation and/or other materials provided with the\n#     distribution.\n#   - Neither the name of the author nor the names of its\n#     contributors may be used to endorse or promote products derived\n#     from this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n# \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n# A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n# OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n# SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n# LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n# DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n# THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nimport warnings\nimport bisect\nfrom collections import deque\n\nimport numpy as np\nfrom scipy.cluster import _hierarchy, _optimal_leaf_ordering\nimport scipy.spatial.distance as distance\nfrom scipy._lib._disjoint_set import DisjointSet\n\n\n_LINKAGE_METHODS = {'single': 0, 'complete': 1, 'average': 2, 'centroid': 3,\n                    'median': 4, 'ward': 5, 'weighted': 6}\n_EUCLIDEAN_METHODS = ('centroid', 'median', 'ward')\n\n__all__ = ['ClusterNode', 'DisjointSet', 'average', 'centroid', 'complete',\n           'cophenet', 'correspond', 'cut_tree', 'dendrogram', 'fcluster',\n           'fclusterdata', 'from_mlab_linkage', 'inconsistent',\n           'is_isomorphic', 'is_monotonic', 'is_valid_im', 'is_valid_linkage',\n           'leaders', 'leaves_list', 'linkage', 'maxRstat', 'maxdists',\n           'maxinconsts', 'median', 'num_obs_linkage', 'optimal_leaf_ordering',\n           'set_link_color_palette', 'single', 'to_mlab_linkage', 'to_tree',\n           'ward', 'weighted', 'distance']\n\n\nclass ClusterWarning(UserWarning):\n    pass\n\n\ndef _warning(s):\n    warnings.warn('scipy.cluster: %s' % s, ClusterWarning, stacklevel=3)\n\n\ndef _copy_array_if_base_present(a):\n    \"\"\"\n    Copy the array if its base points to a parent array.\n    \"\"\"\n    if a.base is not None:\n        return a.copy()\n    elif np.issubsctype(a, np.float32):\n        return np.array(a, dtype=np.double)\n    else:\n        return a\n\n\ndef _copy_arrays_if_base_present(T):\n    \"\"\"\n    Accept a tuple of arrays T. Copies the array T[i] if its base array\n    points to an actual array. Otherwise, the reference is just copied.\n    This is useful if the arrays are being passed to a C function that\n    does not do proper striding.\n    \"\"\"\n    l = [_copy_array_if_base_present(a) for a in T]\n    return l\n\n\ndef _randdm(pnts):\n    \"\"\"\n    Generate a random distance matrix stored in condensed form.\n    Parameters\n    ----------\n    pnts : int\n        The number of points in the distance matrix. Has to be at least 2.\n    Returns\n    -------\n    D : ndarray\n        A ``pnts * (pnts - 1) / 2`` sized vector is returned.\n    \"\"\"\n    if pnts &gt;= 2:\n        D = np.random.rand(pnts * (pnts - 1) / 2)\n    else:\n        raise ValueError(\"The number of points in the distance matrix \"\n                         \"must be at least 2.\")\n    return D\n\n\ndef single(y):\n    \"\"\"\n    Perform single/min/nearest linkage on the condensed distance matrix ``y``.\n    Parameters\n    ----------\n    y : ndarray\n        The upper triangular of the distance matrix. The result of\n        ``pdist`` is returned in this form.\n    Returns\n    -------\n    Z : ndarray\n        The linkage matrix.\n    See Also\n    --------\n    linkage: for advanced creation of hierarchical clusterings.\n    scipy.spatial.distance.pdist : pairwise distance metrics\n    Examples\n    --------\n    &gt;&gt;&gt; from scipy.cluster.hierarchy import single, fcluster\n    &gt;&gt;&gt; from scipy.spatial.distance import pdist\n    First, we need a toy dataset to play with::\n        x x    x x\n        x        x\n        x        x\n        x x    x x\n    &gt;&gt;&gt; X = [[0, 0], [0, 1], [1, 0],\n    ...      [0, 4], [0, 3], [1, 4],\n    ...      [4, 0], [3, 0], [4, 1],\n    ...      [4, 4], [3, 4], [4, 3]]\n    Then, we get a condensed distance matrix from this dataset:\n    &gt;&gt;&gt; y = pdist(X)\n    Finally, we can perform the clustering:\n    &gt;&gt;&gt; Z = single(y)\n    &gt;&gt;&gt; Z\n    array([[ 0.,  1.,  1.,  2.],\n           [ 2., 12.,  1.,  3.],\n           [ 3.,  4.,  1.,  2.],\n           [ 5., 14.,  1.,  3.],\n           [ 6.,  7.,  1.,  2.],\n           [ 8., 16.,  1.,  3.],\n           [ 9., 10.,  1.,  2.],\n           [11., 18.,  1.,  3.],\n           [13., 15.,  2.,  6.],\n           [17., 20.,  2.,  9.],\n           [19., 21.,  2., 12.]])\n    The linkage matrix ``Z`` represents a dendrogram - see\n    `scipy.cluster.hierarchy.linkage` for a detailed explanation of its\n    contents.\n    We can use `scipy.cluster.hierarchy.fcluster` to see to which cluster\n    each initial point would belong given a distance threshold:\n    &gt;&gt;&gt; fcluster(Z, 0.9, criterion='distance')\n    array([ 7,  8,  9, 10, 11, 12,  4,  5,  6,  1,  2,  3], dtype=int32)\n    &gt;&gt;&gt; fcluster(Z, 1, criterion='distance')\n    array([3, 3, 3, 4, 4, 4, 2, 2, 2, 1, 1, 1], dtype=int32)\n    &gt;&gt;&gt; fcluster(Z, 2, criterion='distance')\n    array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)\n    Also, `scipy.cluster.hierarchy.dendrogram` can be used to generate a\n    plot of the dendrogram.\n    \"\"\"\n    return linkage(y, method='single', metric='euclidean')\n\n\ndef complete(y):\n    \"\"\"\n    Perform complete/max/farthest point linkage on a condensed distance matrix.\n    Parameters\n    ----------\n    y : ndarray\n        The upper triangular of the distance matrix. The result of\n        ``pdist`` is returned in this form.\n    Returns\n    -------\n    Z : ndarray\n        A linkage matrix containing the hierarchical clustering. See\n        the `linkage` function documentation for more information\n        on its structure.\n    See Also\n    --------\n    linkage: for advanced creation of hierarchical clusterings.\n    scipy.spatial.distance.pdist : pairwise distance metrics\n    Examples\n    --------\n    &gt;&gt;&gt; from scipy.cluster.hierarchy import complete, fcluster\n    &gt;&gt;&gt; from scipy.spatial.distance import pdist\n    First, we need a toy dataset to play with::\n        x x    x x\n        x        x\n        x        x\n        x x    x x\n    &gt;&gt;&gt; X = [[0, 0], [0, 1], [1, 0],\n    ...      [0, 4], [0, 3], [1, 4],\n    ...      [4, 0], [3, 0], [4, 1],\n    ...      [4, 4], [3, 4], [4, 3]]\n    Then, we get a condensed distance matrix from this dataset:\n    &gt;&gt;&gt; y = pdist(X)\n    Finally, we can perform the clustering:\n    &gt;&gt;&gt; Z = complete(y)\n    &gt;&gt;&gt; Z\n    array([[ 0.        ,  1.        ,  1.        ,  2.        ],\n           [ 3.        ,  4.        ,  1.        ,  2.        ],\n           [ 6.        ,  7.        ,  1.        ,  2.        ],\n           [ 9.        , 10.        ,  1.        ,  2.        ],\n           [ 2.        , 12.        ,  1.41421356,  3.        ],\n           [ 5.        , 13.        ,  1.41421356,  3.        ],\n           [ 8.        , 14.        ,  1.41421356,  3.        ],\n           [11.        , 15.        ,  1.41421356,  3.        ],\n           [16.        , 17.        ,  4.12310563,  6.        ],\n           [18.        , 19.        ,  4.12310563,  6.        ],\n           [20.        , 21.        ,  5.65685425, 12.        ]])\n    The linkage matrix ``Z`` represents a dendrogram - see\n    `scipy.cluster.hierarchy.linkage` for a detailed explanation of its\n    contents.\n    We can use `scipy.cluster.hierarchy.fcluster` to see to which cluster\n    each initial point would belong given a distance threshold:\n    &gt;&gt;&gt; fcluster(Z, 0.9, criterion='distance')\n    array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12], dtype=int32)\n    &gt;&gt;&gt; fcluster(Z, 1.5, criterion='distance')\n    array([1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4], dtype=int32)\n    &gt;&gt;&gt; fcluster(Z, 4.5, criterion='distance')\n    array([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2], dtype=int32)\n    &gt;&gt;&gt; fcluster(Z, 6, criterion='distance')\n    array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)\n    Also, `scipy.cluster.hierarchy.dendrogram` can be used to generate a\n    plot of the dendrogram.\n    \"\"\"\n    return linkage(y, method='complete', metric='euclidean')\n\n\ndef average(y):\n    \"\"\"\n    Perform average/UPGMA linkage on a condensed distance matrix.\n    Parameters\n    ----------\n    y : ndarray\n        The upper triangular of the distance matrix. The result of\n        ``pdist`` is returned in this form.\n    Returns\n    -------\n    Z : ndarray\n        A linkage matrix containing the hierarchical clustering. See\n        `linkage` for more information on its structure.\n    See Also\n    --------\n    linkage: for advanced creation of hierarchical clusterings.\n    scipy.spatial.distance.pdist : pairwise distance metrics\n    Examples\n    --------\n    &gt;&gt;&gt; from scipy.cluster.hierarchy import average, fcluster\n    &gt;&gt;&gt; from scipy.spatial.distance import pdist\n    First, we need a toy dataset to play with::\n        x x    x x\n        x        x\n        x        x\n        x x    x x\n    &gt;&gt;&gt; X = [[0, 0], [0, 1], [1, 0],\n    ...      [0, 4], [0, 3], [1, 4],\n    ...      [4, 0], [3, 0], [4, 1],\n    ...      [4, 4], [3, 4], [4, 3]]\n    Then, we get a condensed distance matrix from this dataset:\n    &gt;&gt;&gt; y = pdist(X)\n    Finally, we can perform the clustering:\n    &gt;&gt;&gt; Z = average(y)\n    &gt;&gt;&gt; Z\n    array([[ 0.        ,  1.        ,  1.        ,  2.        ],\n           [ 3.        ,  4.        ,  1.        ,  2.        ],\n           [ 6.        ,  7.        ,  1.        ,  2.        ],\n           [ 9.        , 10.        ,  1.        ,  2.        ],\n           [ 2.        , 12.        ,  1.20710678,  3.        ],\n           [ 5.        , 13.        ,  1.20710678,  3.        ],\n           [ 8.        , 14.        ,  1.20710678,  3.        ],\n           [11.        , 15.        ,  1.20710678,  3.        ],\n           [16.        , 17.        ,  3.39675184,  6.        ],\n           [18.        , 19.        ,  3.39675184,  6.        ],\n           [20.        , 21.        ,  4.09206523, 12.        ]])\n    The linkage matrix ``Z`` represents a dendrogram - see\n    `scipy.cluster.hierarchy.linkage` for a detailed explanation of its\n    contents.\n    We can use `scipy.cluster.hierarchy.fcluster` to see to which cluster\n    each initial point would belong given a distance threshold:\n    &gt;&gt;&gt; fcluster(Z, 0.9, criterion='distance')\n    array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12], dtype=int32)\n    &gt;&gt;&gt; fcluster(Z, 1.5, criterion='distance')\n    array([1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4], dtype=int32)\n    &gt;&gt;&gt; fcluster(Z, 4, criterion='distance')\n    array([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2], dtype=int32)\n    &gt;&gt;&gt; fcluster(Z, 6, criterion='distance')\n    array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)\n    Also, `scipy.cluster.hierarchy.dendrogram` can be used to generate a\n    plot of the dendrogram.\n    \"\"\"\n    return linkage(y, method='average', metric='euclidean')\n\n\ndef weighted(y):\n    \"\"\"\n    Perform weighted/WPGMA linkage on the condensed distance matrix.\n    See `linkage` for more information on the return\n    structure and algorithm.\n    Parameters\n    ----------\n    y : ndarray\n        The upper triangular of the distance matrix. The result of\n        ``pdist`` is returned in this form.\n    Returns\n    -------\n    Z : ndarray\n        A linkage matrix containing the hierarchical clustering. See\n        `linkage` for more information on its structure.\n    See Also\n    --------\n    linkage : for advanced creation of hierarchical clusterings.\n    scipy.spatial.distance.pdist : pairwise distance metrics\n    Examples\n    --------\n    &gt;&gt;&gt; from scipy.cluster.hierarchy import weighted, fcluster\n    &gt;&gt;&gt; from scipy.spatial.distance import pdist\n    First, we need a toy dataset to play with::\n        x x    x x\n        x        x\n        x        x\n        x x    x x\n    &gt;&gt;&gt; X = [[0, 0], [0, 1], [1, 0],\n    ...      [0, 4], [0, 3], [1, 4],\n    ...      [4, 0], [3, 0], [4, 1],\n    ...      [4, 4], [3, 4], [4, 3]]\n    Then, we get a condensed distance matrix from this dataset:\n    &gt;&gt;&gt; y = pdist(X)\n    Finally, we can perform the clustering:\n    &gt;&gt;&gt; Z = weighted(y)\n    &gt;&gt;&gt; Z\n    array([[ 0.        ,  1.        ,  1.        ,  2.        ],\n           [ 6.        ,  7.        ,  1.        ,  2.        ],\n           [ 3.        ,  4.        ,  1.        ,  2.        ],\n           [ 9.        , 11.        ,  1.        ,  2.        ],\n           [ 2.        , 12.        ,  1.20710678,  3.        ],\n           [ 8.        , 13.        ,  1.20710678,  3.        ],\n           [ 5.        , 14.        ,  1.20710678,  3.        ],\n           [10.        , 15.        ,  1.20710678,  3.        ],\n           [18.        , 19.        ,  3.05595762,  6.        ],\n           [16.        , 17.        ,  3.32379407,  6.        ],\n           [20.        , 21.        ,  4.06357713, 12.        ]])\n    The linkage matrix ``Z`` represents a dendrogram - see\n    `scipy.cluster.hierarchy.linkage` for a detailed explanation of its\n    contents.\n    We can use `scipy.cluster.hierarchy.fcluster` to see to which cluster\n    each initial point would belong given a distance threshold:\n    &gt;&gt;&gt; fcluster(Z, 0.9, criterion='distance')\n    array([ 7,  8,  9,  1,  2,  3, 10, 11, 12,  4,  6,  5], dtype=int32)\n    &gt;&gt;&gt; fcluster(Z, 1.5, criterion='distance')\n    array([3, 3, 3, 1, 1, 1, 4, 4, 4, 2, 2, 2], dtype=int32)\n    &gt;&gt;&gt; fcluster(Z, 4, criterion='distance')\n    array([2, 2, 2, 1, 1, 1, 2, 2, 2, 1, 1, 1], dtype=int32)\n    &gt;&gt;&gt; fcluster(Z, 6, criterion='distance')\n    array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)\n    Also, `scipy.cluster.hierarchy.dendrogram` can be used to generate a\n    plot of the dendrogram.\n    \"\"\"\n    return linkage(y, method='weighted', metric='euclidean')\n\n\ndef centroid(y):\n    \"\"\"\n    Perform centroid/UPGMC linkage.\n    See `linkage` for more information on the input matrix,\n    return structure, and algorithm.\n    The following are common calling conventions:\n    1. ``Z = centroid(y)``\n       Performs centroid/UPGMC linkage on the condensed distance\n       matrix ``y``.\n    2. ``Z = centroid(X)``\n       Performs centroid/UPGMC linkage on the observation matrix ``X``\n       using Euclidean distance as the distance metric.\n    Parameters\n    ----------\n    y : ndarray\n        A condensed distance matrix. A condensed\n        distance matrix is a flat array containing the upper\n        triangular of the distance matrix. This is the form that\n        ``pdist`` returns. Alternatively, a collection of\n        m observation vectors in n dimensions may be passed as\n        an m by n array.\n    Returns\n    -------\n    Z : ndarray\n        A linkage matrix containing the hierarchical clustering. See\n        the `linkage` function documentation for more information\n        on its structure.\n    See Also\n    --------\n    linkage: for advanced creation of hierarchical clusterings.\n    scipy.spatial.distance.pdist : pairwise distance metrics\n    Examples\n    --------\n    &gt;&gt;&gt; from scipy.cluster.hierarchy import centroid, fcluster\n    &gt;&gt;&gt; from scipy.spatial.distance import pdist\n    First, we need a toy dataset to play with::\n        x x    x x\n        x        x\n        x        x\n        x x    x x\n    &gt;&gt;&gt; X = [[0, 0], [0, 1], [1, 0],\n    ...      [0, 4], [0, 3], [1, 4],\n    ...      [4, 0], [3, 0], [4, 1],\n    ...      [4, 4], [3, 4], [4, 3]]\n    Then, we get a condensed distance matrix from this dataset:\n    &gt;&gt;&gt; y = pdist(X)\n    Finally, we can perform the clustering:\n    &gt;&gt;&gt; Z = centroid(y)\n    &gt;&gt;&gt; Z\n    array([[ 0.        ,  1.        ,  1.        ,  2.        ],\n           [ 3.        ,  4.        ,  1.        ,  2.        ],\n           [ 9.        , 10.        ,  1.        ,  2.        ],\n           [ 6.        ,  7.        ,  1.        ,  2.        ],\n           [ 2.        , 12.        ,  1.11803399,  3.        ],\n           [ 5.        , 13.        ,  1.11803399,  3.        ],\n           [ 8.        , 15.        ,  1.11803399,  3.        ],\n           [11.        , 14.        ,  1.11803399,  3.        ],\n           [18.        , 19.        ,  3.33333333,  6.        ],\n           [16.        , 17.        ,  3.33333333,  6.        ],\n           [20.        , 21.        ,  3.33333333, 12.        ]])\n    The linkage matrix ``Z`` represents a dendrogram - see\n    `scipy.cluster.hierarchy.linkage` for a detailed explanation of its\n    contents.\n    We can use `scipy.cluster.hierarchy.fcluster` to see to which cluster\n    each initial point would belong given a distance threshold:\n    &gt;&gt;&gt; fcluster(Z, 0.9, criterion='distance')\n    array([ 7,  8,  9, 10, 11, 12,  1,  2,  3,  4,  5,  6], dtype=int32)\n    &gt;&gt;&gt; fcluster(Z, 1.1, criterion='distance')\n    array([5, 5, 6, 7, 7, 8, 1, 1, 2, 3, 3, 4], dtype=int32)\n    &gt;&gt;&gt; fcluster(Z, 2, criterion='distance')\n    array([3, 3, 3, 4, 4, 4, 1, 1, 1, 2, 2, 2], dtype=int32)\n    &gt;&gt;&gt; fcluster(Z, 4, criterion='distance')\n    array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)\n    Also, `scipy.cluster.hierarchy.dendrogram` can be used to generate a\n    plot of the dendrogram.\n    \"\"\"\n    return linkage(y, method='centroid', metric='euclidean')\n\n\ndef median(y):\n    \"\"\"\n    Perform median/WPGMC linkage.\n    See `linkage` for more information on the return structure\n    and algorithm.\n     The following are common calling conventions:\n     1. ``Z = median(y)``\n        Performs median/WPGMC linkage on the condensed distance matrix\n        ``y``.  See ``linkage`` for more information on the return\n        structure and algorithm.\n     2. ``Z = median(X)``\n        Performs median/WPGMC linkage on the observation matrix ``X``\n        using Euclidean distance as the distance metric. See `linkage`\n        for more information on the return structure and algorithm.\n    Parameters\n    ----------\n    y : ndarray\n        A condensed distance matrix. A condensed\n        distance matrix is a flat array containing the upper\n        triangular of the distance matrix. This is the form that\n        ``pdist`` returns.  Alternatively, a collection of\n        m observation vectors in n dimensions may be passed as\n        an m by n array.\n    Returns\n    -------\n    Z : ndarray\n        The hierarchical clustering encoded as a linkage matrix.\n    See Also\n    --------\n    linkage: for advanced creation of hierarchical clusterings.\n    scipy.spatial.distance.pdist : pairwise distance metrics\n    Examples\n    --------\n    &gt;&gt;&gt; from scipy.cluster.hierarchy import median, fcluster\n    &gt;&gt;&gt; from scipy.spatial.distance import pdist\n    First, we need a toy dataset to play with::\n        x x    x x\n        x        x\n        x        x\n        x x    x x\n    &gt;&gt;&gt; X = [[0, 0], [0, 1], [1, 0],\n    ...      [0, 4], [0, 3], [1, 4],\n    ...      [4, 0], [3, 0], [4, 1],\n    ...      [4, 4], [3, 4], [4, 3]]\n    Then, we get a condensed distance matrix from this dataset:\n    &gt;&gt;&gt; y = pdist(X)\n    Finally, we can perform the clustering:\n    &gt;&gt;&gt; Z = median(y)\n    &gt;&gt;&gt; Z\n    array([[ 0.        ,  1.        ,  1.        ,  2.        ],\n           [ 3.        ,  4.        ,  1.        ,  2.        ],\n           [ 9.        , 10.        ,  1.        ,  2.        ],\n           [ 6.        ,  7.        ,  1.        ,  2.        ],\n           [ 2.        , 12.        ,  1.11803399,  3.        ],\n           [ 5.        , 13.        ,  1.11803399,  3.        ],\n           [ 8.        , 15.        ,  1.11803399,  3.        ],\n           [11.        , 14.        ,  1.11803399,  3.        ],\n           [18.        , 19.        ,  3.        ,  6.        ],\n           [16.        , 17.        ,  3.5       ,  6.        ],\n           [20.        , 21.        ,  3.25      , 12.        ]])\n    The linkage matrix ``Z`` represents a dendrogram - see\n    `scipy.cluster.hierarchy.linkage` for a detailed explanation of its\n    contents.\n    We can use `scipy.cluster.hierarchy.fcluster` to see to which cluster\n    each initial point would belong given a distance threshold:\n    &gt;&gt;&gt; fcluster(Z, 0.9, criterion='distance')\n    array([ 7,  8,  9, 10, 11, 12,  1,  2,  3,  4,  5,  6], dtype=int32)\n    &gt;&gt;&gt; fcluster(Z, 1.1, criterion='distance')\n    array([5, 5, 6, 7, 7, 8, 1, 1, 2, 3, 3, 4], dtype=int32)\n    &gt;&gt;&gt; fcluster(Z, 2, criterion='distance')\n    array([3, 3, 3, 4, 4, 4, 1, 1, 1, 2, 2, 2], dtype=int32)\n    &gt;&gt;&gt; fcluster(Z, 4, criterion='distance')\n    array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)\n    Also, `scipy.cluster.hierarchy.dendrogram` can be used to generate a\n    plot of the dendrogram.\n    \"\"\"\n    return linkage(y, method='median', metric='euclidean')\n\n\ndef ward(y):\n    \"\"\"\n    Perform Ward's linkage on a condensed distance matrix.\n    See `linkage` for more information on the return structure\n    and algorithm.\n    The following are common calling conventions:\n    1. ``Z = ward(y)``\n       Performs Ward's linkage on the condensed distance matrix ``y``.\n    2. ``Z = ward(X)``\n       Performs Ward's linkage on the observation matrix ``X`` using\n       Euclidean distance as the distance metric.\n    Parameters\n    ----------\n    y : ndarray\n        A condensed distance matrix. A condensed\n        distance matrix is a flat array containing the upper\n        triangular of the distance matrix. This is the form that\n        ``pdist`` returns.  Alternatively, a collection of\n        m observation vectors in n dimensions may be passed as\n        an m by n array.\n    Returns\n    -------\n    Z : ndarray\n        The hierarchical clustering encoded as a linkage matrix. See\n        `linkage` for more information on the return structure and\n        algorithm.\n    See Also\n    --------\n    linkage: for advanced creation of hierarchical clusterings.\n    scipy.spatial.distance.pdist : pairwise distance metrics\n    Examples\n    --------\n    &gt;&gt;&gt; from scipy.cluster.hierarchy import ward, fcluster\n    &gt;&gt;&gt; from scipy.spatial.distance import pdist\n    First, we need a toy dataset to play with::\n        x x    x x\n        x        x\n        x        x\n        x x    x x\n    &gt;&gt;&gt; X = [[0, 0], [0, 1], [1, 0],\n    ...      [0, 4], [0, 3], [1, 4],\n    ...      [4, 0], [3, 0], [4, 1],\n    ...      [4, 4], [3, 4], [4, 3]]\n    Then, we get a condensed distance matrix from this dataset:\n    &gt;&gt;&gt; y = pdist(X)\n    Finally, we can perform the clustering:\n    &gt;&gt;&gt; Z = ward(y)\n    &gt;&gt;&gt; Z\n    array([[ 0.        ,  1.        ,  1.        ,  2.        ],\n           [ 3.        ,  4.        ,  1.        ,  2.        ],\n           [ 6.        ,  7.        ,  1.        ,  2.        ],\n           [ 9.        , 10.        ,  1.        ,  2.        ],\n           [ 2.        , 12.        ,  1.29099445,  3.        ],\n           [ 5.        , 13.        ,  1.29099445,  3.        ],\n           [ 8.        , 14.        ,  1.29099445,  3.        ],\n           [11.        , 15.        ,  1.29099445,  3.        ],\n           [16.        , 17.        ,  5.77350269,  6.        ],\n           [18.        , 19.        ,  5.77350269,  6.        ],\n           [20.        , 21.        ,  8.16496581, 12.        ]])\n    The linkage matrix ``Z`` represents a dendrogram - see\n    `scipy.cluster.hierarchy.linkage` for a detailed explanation of its\n    contents.\n    We can use `scipy.cluster.hierarchy.fcluster` to see to which cluster\n    each initial point would belong given a distance threshold:\n    &gt;&gt;&gt; fcluster(Z, 0.9, criterion='distance')\n    array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12], dtype=int32)\n    &gt;&gt;&gt; fcluster(Z, 1.1, criterion='distance')\n    array([1, 1, 2, 3, 3, 4, 5, 5, 6, 7, 7, 8], dtype=int32)\n    &gt;&gt;&gt; fcluster(Z, 3, criterion='distance')\n    array([1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4], dtype=int32)\n    &gt;&gt;&gt; fcluster(Z, 9, criterion='distance')\n    array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)\n    Also, `scipy.cluster.hierarchy.dendrogram` can be used to generate a\n    plot of the dendrogram.\n    \"\"\"\n    return linkage(y, method='ward', metric='euclidean')\n\n\ndef linkage(y, method='single', metric='euclidean', optimal_ordering=False):\n    \"\"\"\n    Perform hierarchical/agglomerative clustering.\n    The input y may be either a 1-D condensed distance matrix\n    or a 2-D array of observation vectors.\n    If y is a 1-D condensed distance matrix,\n    then y must be a :math:`\\\\binom{n}{2}` sized\n    vector, where n is the number of original observations paired\n    in the distance matrix. The behavior of this function is very\n    similar to the MATLAB linkage function.\n    A :math:`(n-1)` by 4 matrix ``Z`` is returned. At the\n    :math:`i`-th iteration, clusters with indices ``Z[i, 0]`` and\n    ``Z[i, 1]`` are combined to form cluster :math:`n + i`. A\n    cluster with an index less than :math:`n` corresponds to one of\n    the :math:`n` original observations. The distance between\n    clusters ``Z[i, 0]`` and ``Z[i, 1]`` is given by ``Z[i, 2]``. The\n    fourth value ``Z[i, 3]`` represents the number of original\n    observations in the newly formed cluster.\n    The following linkage methods are used to compute the distance\n    :math:`d(s, t)` between two clusters :math:`s` and\n    :math:`t`. The algorithm begins with a forest of clusters that\n    have yet to be used in the hierarchy being formed. When two\n    clusters :math:`s` and :math:`t` from this forest are combined\n    into a single cluster :math:`u`, :math:`s` and :math:`t` are\n    removed from the forest, and :math:`u` is added to the\n    forest. When only one cluster remains in the forest, the algorithm\n    stops, and this cluster becomes the root.\n    A distance matrix is maintained at each iteration. The ``d[i,j]``\n    entry corresponds to the distance between cluster :math:`i` and\n    :math:`j` in the original forest.\n    At each iteration, the algorithm must update the distance matrix\n    to reflect the distance of the newly formed cluster u with the\n    remaining clusters in the forest.\n    Suppose there are :math:`|u|` original observations\n    :math:`u[0], \\\\ldots, u[|u|-1]` in cluster :math:`u` and\n    :math:`|v|` original objects :math:`v[0], \\\\ldots, v[|v|-1]` in\n    cluster :math:`v`. Recall, :math:`s` and :math:`t` are\n    combined to form cluster :math:`u`. Let :math:`v` be any\n    remaining cluster in the forest that is not :math:`u`.\n    The following are methods for calculating the distance between the\n    newly formed cluster :math:`u` and each :math:`v`.\n      * method='single' assigns\n        .. math::\n           d(u,v) = \\\\min(dist(u[i],v[j]))\n        for all points :math:`i` in cluster :math:`u` and\n        :math:`j` in cluster :math:`v`. This is also known as the\n        Nearest Point Algorithm.\n      * method='complete' assigns\n        .. math::\n           d(u, v) = \\\\max(dist(u[i],v[j]))\n        for all points :math:`i` in cluster u and :math:`j` in\n        cluster :math:`v`. This is also known by the Farthest Point\n        Algorithm or Voor Hees Algorithm.\n      * method='average' assigns\n        .. math::\n           d(u,v) = \\\\sum_{ij} \\\\frac{d(u[i], v[j])}\n                                   {(|u|*|v|)}\n        for all points :math:`i` and :math:`j` where :math:`|u|`\n        and :math:`|v|` are the cardinalities of clusters :math:`u`\n        and :math:`v`, respectively. This is also called the UPGMA\n        algorithm.\n      * method='weighted' assigns\n        .. math::\n           d(u,v) = (dist(s,v) + dist(t,v))/2\n        where cluster u was formed with cluster s and t and v\n        is a remaining cluster in the forest (also called WPGMA).\n      * method='centroid' assigns\n        .. math::\n           dist(s,t) = ||c_s-c_t||_2\n        where :math:`c_s` and :math:`c_t` are the centroids of\n        clusters :math:`s` and :math:`t`, respectively. When two\n        clusters :math:`s` and :math:`t` are combined into a new\n        cluster :math:`u`, the new centroid is computed over all the\n        original objects in clusters :math:`s` and :math:`t`. The\n        distance then becomes the Euclidean distance between the\n        centroid of :math:`u` and the centroid of a remaining cluster\n        :math:`v` in the forest. This is also known as the UPGMC\n        algorithm.\n      * method='median' assigns :math:`d(s,t)` like the ``centroid``\n        method. When two clusters :math:`s` and :math:`t` are combined\n        into a new cluster :math:`u`, the average of centroids s and t\n        give the new centroid :math:`u`. This is also known as the\n        WPGMC algorithm.\n      * method='ward' uses the Ward variance minimization algorithm.\n        The new entry :math:`d(u,v)` is computed as follows,\n        .. math::\n           d(u,v) = \\\\sqrt{\\\\frac{|v|+|s|}\n                               {T}d(v,s)^2\n                        + \\\\frac{|v|+|t|}\n                               {T}d(v,t)^2\n                        - \\\\frac{|v|}\n                               {T}d(s,t)^2}\n        where :math:`u` is the newly joined cluster consisting of\n        clusters :math:`s` and :math:`t`, :math:`v` is an unused\n        cluster in the forest, :math:`T=|v|+|s|+|t|`, and\n        :math:`|*|` is the cardinality of its argument. This is also\n        known as the incremental algorithm.\n    Warning: When the minimum distance pair in the forest is chosen, there\n    may be two or more pairs with the same minimum distance. This\n    implementation may choose a different minimum than the MATLAB\n    version.\n    Parameters\n    ----------\n    y : ndarray\n        A condensed distance matrix. A condensed distance matrix\n        is a flat array containing the upper triangular of the distance matrix.\n        This is the form that ``pdist`` returns. Alternatively, a collection of\n        :math:`m` observation vectors in :math:`n` dimensions may be passed as\n        an :math:`m` by :math:`n` array. All elements of the condensed distance\n        matrix must be finite, i.e., no NaNs or infs.\n    method : str, optional\n        The linkage algorithm to use. See the ``Linkage Methods`` section below\n        for full descriptions.\n    metric : str or function, optional\n        The distance metric to use in the case that y is a collection of\n        observation vectors; ignored otherwise. See the ``pdist``\n        function for a list of valid distance metrics. A custom distance\n        function can also be used.\n    optimal_ordering : bool, optional\n        If True, the linkage matrix will be reordered so that the distance\n        between successive leaves is minimal. This results in a more intuitive\n        tree structure when the data are visualized. defaults to False, because\n        this algorithm can be slow, particularly on large datasets [2]_. See\n        also the `optimal_leaf_ordering` function.\n        .. versionadded:: 1.0.0\n    Returns\n    -------\n    Z : ndarray\n        The hierarchical clustering encoded as a linkage matrix.\n    Notes\n    -----\n    1. For method 'single', an optimized algorithm based on minimum spanning\n       tree is implemented. It has time complexity :math:`O(n^2)`.\n       For methods 'complete', 'average', 'weighted' and 'ward', an algorithm\n       called nearest-neighbors chain is implemented. It also has time\n       complexity :math:`O(n^2)`.\n       For other methods, a naive algorithm is implemented with :math:`O(n^3)`\n       time complexity.\n       All algorithms use :math:`O(n^2)` memory.\n       Refer to [1]_ for details about the algorithms.\n    2. Methods 'centroid', 'median', and 'ward' are correctly defined only if\n       Euclidean pairwise metric is used. If `y` is passed as precomputed\n       pairwise distances, then it is the user's responsibility to assure that\n       these distances are in fact Euclidean, otherwise the produced result\n       will be incorrect.\n    See Also\n    --------\n    scipy.spatial.distance.pdist : pairwise distance metrics\n    References\n    ----------\n    .. [1] Daniel Mullner, \"Modern hierarchical, agglomerative clustering\n           algorithms\", :arXiv:`1109.2378v1`.\n    .. [2] Ziv Bar-Joseph, David K. Gifford, Tommi S. Jaakkola, \"Fast optimal\n           leaf ordering for hierarchical clustering\", 2001. Bioinformatics\n           :doi:`10.1093/bioinformatics/17.suppl_1.S22`\n    Examples\n    --------\n    &gt;&gt;&gt; from scipy.cluster.hierarchy import dendrogram, linkage\n    &gt;&gt;&gt; from matplotlib import pyplot as plt\n    &gt;&gt;&gt; X = [[i] for i in [2, 8, 0, 4, 1, 9, 9, 0]]\n    &gt;&gt;&gt; Z = linkage(X, 'ward')\n    &gt;&gt;&gt; fig = plt.figure(figsize=(25, 10))\n    &gt;&gt;&gt; dn = dendrogram(Z)\n    &gt;&gt;&gt; Z = linkage(X, 'single')\n    &gt;&gt;&gt; fig = plt.figure(figsize=(25, 10))\n    &gt;&gt;&gt; dn = dendrogram(Z)\n    &gt;&gt;&gt; plt.show()\n    \"\"\"\n    if method not in _LINKAGE_METHODS:\n        raise ValueError(\"Invalid method: {0}\".format(method))\n\n    y = _convert_to_double(np.asarray(y, order='c'))\n\n    if y.ndim == 1:\n        distance.is_valid_y(y, throw=True, name='y')\n        [y] = _copy_arrays_if_base_present([y])\n    elif y.ndim == 2:\n        if method in _EUCLIDEAN_METHODS and metric != 'euclidean':\n            raise ValueError(\"Method '{0}' requires the distance metric \"\n                             \"to be Euclidean\".format(method))\n        if y.shape[0] == y.shape[1] and np.allclose(np.diag(y), 0):\n            if np.all(y &gt;= 0) and np.allclose(y, y.T):\n                _warning('The symmetric non-negative hollow observation '\n                         'matrix looks suspiciously like an uncondensed '\n                         'distance matrix')\n        y = distance.pdist(y, metric)\n    else:\n        raise ValueError(\"`y` must be 1 or 2 dimensional.\")\n\n    if not np.all(np.isfinite(y)):\n        raise ValueError(\"The condensed distance matrix must contain only \"\n                         \"finite values.\")\n\n    n = int(distance.num_obs_y(y))\n    method_code = _LINKAGE_METHODS[method]\n\n    if method == 'single':\n        result = _hierarchy.mst_single_linkage(y, n)\n    elif method in ['complete', 'average', 'weighted', 'ward']:\n        result = _hierarchy.nn_chain(y, n, method_code)\n    else:\n        result = _hierarchy.fast_linkage(y, n, method_code)\n\n    if optimal_ordering:\n        return optimal_leaf_ordering(result, y)\n    else:\n        return result\n\n\nclass ClusterNode(object):\n    \"\"\"\n    A tree node class for representing a cluster.\n    Leaf nodes correspond to original observations, while non-leaf nodes\n    correspond to non-singleton clusters.\n    The `to_tree` function converts a matrix returned by the linkage\n    function into an easy-to-use tree representation.\n    All parameter names are also attributes.\n    Parameters\n    ----------\n    id : int\n        The node id.\n    left : ClusterNode instance, optional\n        The left child tree node.\n    right : ClusterNode instance, optional\n        The right child tree node.\n    dist : float, optional\n        Distance for this cluster in the linkage matrix.\n    count : int, optional\n        The number of samples in this cluster.\n    See Also\n    --------\n    to_tree : for converting a linkage matrix ``Z`` into a tree object.\n    \"\"\"\n\n    def __init__(self, id, left=None, right=None, dist=0, count=1):\n        if id &lt; 0:\n            raise ValueError('The id must be non-negative.')\n        if dist &lt; 0:\n            raise ValueError('The distance must be non-negative.')\n        if (left is None and right is not None) or \\\n                (left is not None and right is None):\n            raise ValueError('Only full or proper binary trees are permitted.'\n                             '  This node has one child.')\n        if count &lt; 1:\n            raise ValueError('A cluster must contain at least one original '\n                             'observation.')\n        self.id = id\n        self.left = left\n        self.right = right\n        self.dist = dist\n        if self.left is None:\n            self.count = count\n        else:\n            self.count = left.count + right.count\n\n    def __lt__(self, node):\n        if not isinstance(node, ClusterNode):\n            raise ValueError(\"Can't compare ClusterNode \"\n                             \"to type {}\".format(type(node)))\n        return self.dist &lt; node.dist\n\n    def __gt__(self, node):\n        if not isinstance(node, ClusterNode):\n            raise ValueError(\"Can't compare ClusterNode \"\n                             \"to type {}\".format(type(node)))\n        return self.dist &gt; node.dist\n\n    def __eq__(self, node):\n        if not isinstance(node, ClusterNode):\n            raise ValueError(\"Can't compare ClusterNode \"\n                             \"to type {}\".format(type(node)))\n        return self.dist == node.dist\n\n    def get_id(self):\n        \"\"\"\n        The identifier of the target node.\n        For ``0 &lt;= i &lt; n``, `i` corresponds to original observation i.\n        For ``n &lt;= i &lt; 2n-1``, `i` corresponds to non-singleton cluster formed\n        at iteration ``i-n``.\n        Returns\n        -------\n        id : int\n            The identifier of the target node.\n        \"\"\"\n        return self.id\n\n    def get_count(self):\n        \"\"\"\n        The number of leaf nodes (original observations) belonging to\n        the cluster node nd. If the target node is a leaf, 1 is\n        returned.\n        Returns\n        -------\n        get_count : int\n            The number of leaf nodes below the target node.\n        \"\"\"\n        return self.count\n\n    def get_left(self):\n        \"\"\"\n        Return a reference to the left child tree object.\n        Returns\n        -------\n        left : ClusterNode\n            The left child of the target node. If the node is a leaf,\n            None is returned.\n        \"\"\"\n        return self.left\n\n    def get_right(self):\n        \"\"\"\n        Return a reference to the right child tree object.\n        Returns\n        -------\n        right : ClusterNode\n            The left child of the target node. If the node is a leaf,\n            None is returned.\n        \"\"\"\n        return self.right\n\n    def is_leaf(self):\n        \"\"\"\n        Return True if the target node is a leaf.\n        Returns\n        -------\n        leafness : bool\n            True if the target node is a leaf node.\n        \"\"\"\n        return self.left is None\n\n    def pre_order(self, func=(lambda x: x.id)):\n        \"\"\"\n        Perform pre-order traversal without recursive function calls.\n        When a leaf node is first encountered, ``func`` is called with\n        the leaf node as its argument, and its result is appended to\n        the list.\n        For example, the statement::\n           ids = root.pre_order(lambda x: x.id)\n        returns a list of the node ids corresponding to the leaf nodes\n        of the tree as they appear from left to right.\n        Parameters\n        ----------\n        func : function\n            Applied to each leaf ClusterNode object in the pre-order traversal.\n            Given the ``i``-th leaf node in the pre-order traversal ``n[i]``,\n            the result of ``func(n[i])`` is stored in ``L[i]``. If not\n            provided, the index of the original observation to which the node\n            corresponds is used.\n        Returns\n        -------\n        L : list\n            The pre-order traversal.\n        \"\"\"\n        # Do a preorder traversal, caching the result. To avoid having to do\n        # recursion, we'll store the previous index we've visited in a vector.\n        n = self.count\n\n        curNode = [None] * (2 * n)\n        lvisited = set()\n        rvisited = set()\n        curNode[0] = self\n        k = 0\n        preorder = []\n        while k &gt;= 0:\n            nd = curNode[k]\n            ndid = nd.id\n            if nd.is_leaf():\n                preorder.append(func(nd))\n                k = k - 1\n            else:\n                if ndid not in lvisited:\n                    curNode[k + 1] = nd.left\n                    lvisited.add(ndid)\n                    k = k + 1\n                elif ndid not in rvisited:\n                    curNode[k + 1] = nd.right\n                    rvisited.add(ndid)\n                    k = k + 1\n                # If we've visited the left and right of this non-leaf\n                # node already, go up in the tree.\n                else:\n                    k = k - 1\n\n        return preorder\n\n\n_cnode_bare = ClusterNode(0)\n_cnode_type = type(ClusterNode)\n\n\ndef _order_cluster_tree(Z):\n    \"\"\"\n    Return clustering nodes in bottom-up order by distance.\n    Parameters\n    ----------\n    Z : scipy.cluster.linkage array\n        The linkage matrix.\n    Returns\n    -------\n    nodes : list\n        A list of ClusterNode objects.\n    \"\"\"\n    q = deque()\n    tree = to_tree(Z)\n    q.append(tree)\n    nodes = []\n\n    while q:\n        node = q.popleft()\n        if not node.is_leaf():\n            bisect.insort_left(nodes, node)\n            q.append(node.get_right())\n            q.append(node.get_left())\n    return nodes\n\n\ndef cut_tree(Z, n_clusters=None, height=None):\n    \"\"\"\n    Given a linkage matrix Z, return the cut tree.\n    Parameters\n    ----------\n    Z : scipy.cluster.linkage array\n        The linkage matrix.\n    n_clusters : array_like, optional\n        Number of clusters in the tree at the cut point.\n    height : array_like, optional\n        The height at which to cut the tree. Only possible for ultrametric\n        trees.\n    Returns\n    -------\n    cutree : array\n        An array indicating group membership at each agglomeration step. I.e.,\n        for a full cut tree, in the first column each data point is in its own\n        cluster. At the next step, two nodes are merged. Finally, all\n        singleton and non-singleton clusters are in one group. If `n_clusters`\n        or `height` are given, the columns correspond to the columns of\n        `n_clusters` or `height`.\n    Examples\n    --------\n    &gt;&gt;&gt; from scipy import cluster\n    &gt;&gt;&gt; np.random.seed(23)\n    &gt;&gt;&gt; X = np.random.randn(50, 4)\n    &gt;&gt;&gt; Z = cluster.hierarchy.ward(X)\n    &gt;&gt;&gt; cutree = cluster.hierarchy.cut_tree(Z, n_clusters=[5, 10])\n    &gt;&gt;&gt; cutree[:10]\n    array([[0, 0],\n           [1, 1],\n           [2, 2],\n           [3, 3],\n           [3, 4],\n           [2, 2],\n           [0, 0],\n           [1, 5],\n           [3, 6],\n           [4, 7]])\n    \"\"\"\n    nobs = num_obs_linkage(Z)\n    nodes = _order_cluster_tree(Z)\n\n    if height is not None and n_clusters is not None:\n        raise ValueError(\"At least one of either height or n_clusters \"\n                         \"must be None\")\n    elif height is None and n_clusters is None:  # return the full cut tree\n        cols_idx = np.arange(nobs)\n    elif height is not None:\n        heights = np.array([x.dist for x in nodes])\n        cols_idx = np.searchsorted(heights, height)\n    else:\n        cols_idx = nobs - np.searchsorted(np.arange(nobs), n_clusters)\n\n    try:\n        n_cols = len(cols_idx)\n    except TypeError:  # scalar\n        n_cols = 1\n        cols_idx = np.array([cols_idx])\n\n    groups = np.zeros((n_cols, nobs), dtype=int)\n    last_group = np.arange(nobs)\n    if 0 in cols_idx:\n        groups[0] = last_group\n\n    for i, node in enumerate(nodes):\n        idx = node.pre_order()\n        this_group = last_group.copy()\n        this_group[idx] = last_group[idx].min()\n        this_group[this_group &gt; last_group[idx].max()] -= 1\n        if i + 1 in cols_idx:\n            groups[np.nonzero(i + 1 == cols_idx)[0]] = this_group\n        last_group = this_group\n\n    return groups.T\n\n\ndef to_tree(Z, rd=False):\n    \"\"\"\n    Convert a linkage matrix into an easy-to-use tree object.\n    The reference to the root `ClusterNode` object is returned (by default).\n    Each `ClusterNode` object has a ``left``, ``right``, ``dist``, ``id``,\n    and ``count`` attribute. The left and right attributes point to\n    ClusterNode objects that were combined to generate the cluster.\n    If both are None then the `ClusterNode` object is a leaf node, its count\n    must be 1, and its distance is meaningless but set to 0.\n    *Note: This function is provided for the convenience of the library\n    user. ClusterNodes are not used as input to any of the functions in this\n    library.*\n    Parameters\n    ----------\n    Z : ndarray\n        The linkage matrix in proper form (see the `linkage`\n        function documentation).\n    rd : bool, optional\n        When False (default), a reference to the root `ClusterNode` object is\n        returned.  Otherwise, a tuple ``(r, d)`` is returned. ``r`` is a\n        reference to the root node while ``d`` is a list of `ClusterNode`\n        objects - one per original entry in the linkage matrix plus entries\n        for all clustering steps. If a cluster id is\n        less than the number of samples ``n`` in the data that the linkage\n        matrix describes, then it corresponds to a singleton cluster (leaf\n        node).\n        See `linkage` for more information on the assignment of cluster ids\n        to clusters.\n    Returns\n    -------\n    tree : ClusterNode or tuple (ClusterNode, list of ClusterNode)\n        If ``rd`` is False, a `ClusterNode`.\n        If ``rd`` is True, a list of length ``2*n - 1``, with ``n`` the number\n        of samples.  See the description of `rd` above for more details.\n    See Also\n    --------\n    linkage, is_valid_linkage, ClusterNode\n    Examples\n    --------\n    &gt;&gt;&gt; from scipy.cluster import hierarchy\n    &gt;&gt;&gt; x = np.random.rand(10).reshape(5, 2)\n    &gt;&gt;&gt; Z = hierarchy.linkage(x)\n    &gt;&gt;&gt; hierarchy.to_tree(Z)\n    &lt;scipy.cluster.hierarchy.ClusterNode object at ...\n    &gt;&gt;&gt; rootnode, nodelist = hierarchy.to_tree(Z, rd=True)\n    &gt;&gt;&gt; rootnode\n    &lt;scipy.cluster.hierarchy.ClusterNode object at ...\n    &gt;&gt;&gt; len(nodelist)\n    9\n    \"\"\"\n    Z = np.asarray(Z, order='c')\n    is_valid_linkage(Z, throw=True, name='Z')\n\n    # Number of original objects is equal to the number of rows minus 1.\n    n = Z.shape[0] + 1\n\n    # Create a list full of None's to store the node objects\n    d = [None] * (n * 2 - 1)\n\n    # Create the nodes corresponding to the n original objects.\n    for i in range(0, n):\n        d[i] = ClusterNode(i)\n\n    nd = None\n\n    for i in range(0, n - 1):\n        fi = int(Z[i, 0])\n        fj = int(Z[i, 1])\n        if fi &gt; i + n:\n            raise ValueError(('Corrupt matrix Z. Index to derivative cluster '\n                              'is used before it is formed. See row %d, '\n                              'column 0') % fi)\n        if fj &gt; i + n:\n            raise ValueError(('Corrupt matrix Z. Index to derivative cluster '\n                              'is used before it is formed. See row %d, '\n                              'column 1') % fj)\n        nd = ClusterNode(i + n, d[fi], d[fj], Z[i, 2])\n        #                 ^ id   ^ left ^ right ^ dist\n        if Z[i, 3] != nd.count:\n            raise ValueError(('Corrupt matrix Z. The count Z[%d,3] is '\n                              'incorrect.') % i)\n        d[n + i] = nd\n\n    if rd:\n        return (nd, d)\n    else:\n        return nd\n\n\ndef optimal_leaf_ordering(Z, y, metric='euclidean'):\n    \"\"\"\n    Given a linkage matrix Z and distance, reorder the cut tree.\n    Parameters\n    ----------\n    Z : ndarray\n        The hierarchical clustering encoded as a linkage matrix. See\n        `linkage` for more information on the return structure and\n        algorithm.\n    y : ndarray\n        The condensed distance matrix from which Z was generated.\n        Alternatively, a collection of m observation vectors in n\n        dimensions may be passed as an m by n array.\n    metric : str or function, optional\n        The distance metric to use in the case that y is a collection of\n        observation vectors; ignored otherwise. See the ``pdist``\n        function for a list of valid distance metrics. A custom distance\n        function can also be used.\n    Returns\n    -------\n    Z_ordered : ndarray\n        A copy of the linkage matrix Z, reordered to minimize the distance\n        between adjacent leaves.\n    Examples\n    --------\n    &gt;&gt;&gt; from scipy.cluster import hierarchy\n    &gt;&gt;&gt; np.random.seed(23)\n    &gt;&gt;&gt; X = np.random.randn(10,10)\n    &gt;&gt;&gt; Z = hierarchy.ward(X)\n    &gt;&gt;&gt; hierarchy.leaves_list(Z)\n    array([0, 5, 3, 9, 6, 8, 1, 4, 2, 7], dtype=int32)\n    &gt;&gt;&gt; hierarchy.leaves_list(hierarchy.optimal_leaf_ordering(Z, X))\n    array([3, 9, 0, 5, 8, 2, 7, 4, 1, 6], dtype=int32)\n    \"\"\"\n    Z = np.asarray(Z, order='c')\n    is_valid_linkage(Z, throw=True, name='Z')\n\n    y = _convert_to_double(np.asarray(y, order='c'))\n\n    if y.ndim == 1:\n        distance.is_valid_y(y, throw=True, name='y')\n        [y] = _copy_arrays_if_base_present([y])\n    elif y.ndim == 2:\n        if y.shape[0] == y.shape[1] and np.allclose(np.diag(y), 0):\n            if np.all(y &gt;= 0) and np.allclose(y, y.T):\n                _warning('The symmetric non-negative hollow observation '\n                         'matrix looks suspiciously like an uncondensed '\n                         'distance matrix')\n        y = distance.pdist(y, metric)\n    else:\n        raise ValueError(\"`y` must be 1 or 2 dimensional.\")\n\n    if not np.all(np.isfinite(y)):\n        raise ValueError(\"The condensed distance matrix must contain only \"\n                         \"finite values.\")\n\n    return _optimal_leaf_ordering.optimal_leaf_ordering(Z, y)\n\n\ndef _convert_to_bool(X):\n    if X.dtype != bool:\n        X = X.astype(bool)\n    if not X.flags.contiguous:\n        X = X.copy()\n    return X\n\n\ndef _convert_to_double(X):\n    if X.dtype != np.double:\n        X = X.astype(np.double)\n    if not X.flags.contiguous:\n        X = X.copy()\n    return X\n\n\ndef cophenet(Z, Y=None):\n    \"\"\"\n    Calculate the cophenetic distances between each observation in\n    the hierarchical clustering defined by the linkage ``Z``.\n    Suppose ``p`` and ``q`` are original observations in\n    disjoint clusters ``s`` and ``t``, respectively and\n    ``s`` and ``t`` are joined by a direct parent cluster\n    ``u``. The cophenetic distance between observations\n    ``i`` and ``j`` is simply the distance between\n    clusters ``s`` and ``t``.\n    Parameters\n    ----------\n    Z : ndarray\n        The hierarchical clustering encoded as an array\n        (see `linkage` function).\n    Y : ndarray (optional)\n        Calculates the cophenetic correlation coefficient ``c`` of a\n        hierarchical clustering defined by the linkage matrix `Z`\n        of a set of :math:`n` observations in :math:`m`\n        dimensions. `Y` is the condensed distance matrix from which\n        `Z` was generated.\n    Returns\n    -------\n    c : ndarray\n        The cophentic correlation distance (if ``Y`` is passed).\n    d : ndarray\n        The cophenetic distance matrix in condensed form. The\n        :math:`ij` th entry is the cophenetic distance between\n        original observations :math:`i` and :math:`j`.\n    See Also\n    --------\n    linkage: for a description of what a linkage matrix is.\n    scipy.spatial.distance.squareform: transforming condensed matrices into square ones.\n    Examples\n    --------\n    &gt;&gt;&gt; from scipy.cluster.hierarchy import single, cophenet\n    &gt;&gt;&gt; from scipy.spatial.distance import pdist, squareform\n    Given a dataset ``X`` and a linkage matrix ``Z``, the cophenetic distance\n    between two points of ``X`` is the distance between the largest two\n    distinct clusters that each of the points:\n    &gt;&gt;&gt; X = [[0, 0], [0, 1], [1, 0],\n    ...      [0, 4], [0, 3], [1, 4],\n    ...      [4, 0], [3, 0], [4, 1],\n    ...      [4, 4], [3, 4], [4, 3]]\n    ``X`` corresponds to this dataset ::\n        x x    x x\n        x        x\n        x        x\n        x x    x x\n    &gt;&gt;&gt; Z = single(pdist(X))\n    &gt;&gt;&gt; Z\n    array([[ 0.,  1.,  1.,  2.],\n           [ 2., 12.,  1.,  3.],\n           [ 3.,  4.,  1.,  2.],\n           [ 5., 14.,  1.,  3.],\n           [ 6.,  7.,  1.,  2.],\n           [ 8., 16.,  1.,  3.],\n           [ 9., 10.,  1.,  2.],\n           [11., 18.,  1.,  3.],\n           [13., 15.,  2.,  6.],\n           [17., 20.,  2.,  9.],\n           [19., 21.,  2., 12.]])\n    &gt;&gt;&gt; cophenet(Z)\n    array([1., 1., 2., 2., 2., 2., 2., 2., 2., 2., 2., 1., 2., 2., 2., 2., 2.,\n           2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 1., 1., 2., 2.,\n           2., 2., 2., 2., 1., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n           1., 1., 2., 2., 2., 1., 2., 2., 2., 2., 2., 2., 1., 1., 1.])\n    The output of the `scipy.cluster.hierarchy.cophenet` method is\n    represented in condensed form. We can use\n    `scipy.spatial.distance.squareform` to see the output as a\n    regular matrix (where each element ``ij`` denotes the cophenetic distance\n    between each ``i``, ``j`` pair of points in ``X``):\n    &gt;&gt;&gt; squareform(cophenet(Z))\n    array([[0., 1., 1., 2., 2., 2., 2., 2., 2., 2., 2., 2.],\n           [1., 0., 1., 2., 2., 2., 2., 2., 2., 2., 2., 2.],\n           [1., 1., 0., 2., 2., 2., 2., 2., 2., 2., 2., 2.],\n           [2., 2., 2., 0., 1., 1., 2., 2., 2., 2., 2., 2.],\n           [2., 2., 2., 1., 0., 1., 2., 2., 2., 2., 2., 2.],\n           [2., 2., 2., 1., 1., 0., 2., 2., 2., 2., 2., 2.],\n           [2., 2., 2., 2., 2., 2., 0., 1., 1., 2., 2., 2.],\n           [2., 2., 2., 2., 2., 2., 1., 0., 1., 2., 2., 2.],\n           [2., 2., 2., 2., 2., 2., 1., 1., 0., 2., 2., 2.],\n           [2., 2., 2., 2., 2., 2., 2., 2., 2., 0., 1., 1.],\n           [2., 2., 2., 2., 2., 2., 2., 2., 2., 1., 0., 1.],\n           [2., 2., 2., 2., 2., 2., 2., 2., 2., 1., 1., 0.]])\n    In this example, the cophenetic distance between points on ``X`` that are\n    very close (i.e., in the same corner) is 1. For other pairs of points is 2,\n    because the points will be located in clusters at different\n    corners - thus, the distance between these clusters will be larger.\n    \"\"\"\n    Z = np.asarray(Z, order='c')\n    is_valid_linkage(Z, throw=True, name='Z')\n    Zs = Z.shape\n    n = Zs[0] + 1\n\n    zz = np.zeros((n * (n-1)) // 2, dtype=np.double)\n    # Since the C code does not support striding using strides.\n    # The dimensions are used instead.\n    Z = _convert_to_double(Z)\n\n    _hierarchy.cophenetic_distances(Z, zz, int(n))\n    if Y is None:\n        return zz\n\n    Y = np.asarray(Y, order='c')\n    distance.is_valid_y(Y, throw=True, name='Y')\n\n    z = zz.mean()\n    y = Y.mean()\n    Yy = Y - y\n    Zz = zz - z\n    numerator = (Yy * Zz)\n    denomA = Yy**2\n    denomB = Zz**2\n    c = numerator.sum() / np.sqrt((denomA.sum() * denomB.sum()))\n    return (c, zz)\n\n\ndef inconsistent(Z, d=2):\n    r\"\"\"\n    Calculate inconsistency statistics on a linkage matrix.\n    Parameters\n    ----------\n    Z : ndarray\n        The :math:`(n-1)` by 4 matrix encoding the linkage (hierarchical\n        clustering).  See `linkage` documentation for more information on its\n        form.\n    d : int, optional\n        The number of links up to `d` levels below each non-singleton cluster.\n    Returns\n    -------\n    R : ndarray\n        A :math:`(n-1)` by 4 matrix where the ``i``'th row contains the link\n        statistics for the non-singleton cluster ``i``. The link statistics are\n        computed over the link heights for links :math:`d` levels below the\n        cluster ``i``. ``R[i,0]`` and ``R[i,1]`` are the mean and standard\n        deviation of the link heights, respectively; ``R[i,2]`` is the number\n        of links included in the calculation; and ``R[i,3]`` is the\n        inconsistency coefficient,\n        .. math:: \\frac{\\mathtt{Z[i,2]} - \\mathtt{R[i,0]}} {R[i,1]}\n    Notes\n    -----\n    This function behaves similarly to the MATLAB(TM) ``inconsistent``\n    function.\n    Examples\n    --------\n    &gt;&gt;&gt; from scipy.cluster.hierarchy import inconsistent, linkage\n    &gt;&gt;&gt; from matplotlib import pyplot as plt\n    &gt;&gt;&gt; X = [[i] for i in [2, 8, 0, 4, 1, 9, 9, 0]]\n    &gt;&gt;&gt; Z = linkage(X, 'ward')\n    &gt;&gt;&gt; print(Z)\n    [[ 5.          6.          0.          2.        ]\n     [ 2.          7.          0.          2.        ]\n     [ 0.          4.          1.          2.        ]\n     [ 1.          8.          1.15470054  3.        ]\n     [ 9.         10.          2.12132034  4.        ]\n     [ 3.         12.          4.11096096  5.        ]\n     [11.         13.         14.07183949  8.        ]]\n    &gt;&gt;&gt; inconsistent(Z)\n    array([[ 0.        ,  0.        ,  1.        ,  0.        ],\n           [ 0.        ,  0.        ,  1.        ,  0.        ],\n           [ 1.        ,  0.        ,  1.        ,  0.        ],\n           [ 0.57735027,  0.81649658,  2.        ,  0.70710678],\n           [ 1.04044011,  1.06123822,  3.        ,  1.01850858],\n           [ 3.11614065,  1.40688837,  2.        ,  0.70710678],\n           [ 6.44583366,  6.76770586,  3.        ,  1.12682288]])\n    \"\"\"\n    Z = np.asarray(Z, order='c')\n\n    Zs = Z.shape\n    is_valid_linkage(Z, throw=True, name='Z')\n    if (not d == np.floor(d)) or d &lt; 0:\n        raise ValueError('The second argument d must be a nonnegative '\n                         'integer value.')\n\n    # Since the C code does not support striding using strides.\n    # The dimensions are used instead.\n    [Z] = _copy_arrays_if_base_present([Z])\n\n    n = Zs[0] + 1\n    R = np.zeros((n - 1, 4), dtype=np.double)\n\n    _hierarchy.inconsistent(Z, R, int(n), int(d))\n    return R\n\n\ndef from_mlab_linkage(Z):\n    \"\"\"\n    Convert a linkage matrix generated by MATLAB(TM) to a new\n    linkage matrix compatible with this module.\n    The conversion does two things:\n     * the indices are converted from ``1..N`` to ``0..(N-1)`` form,\n       and\n     * a fourth column ``Z[:,3]`` is added where ``Z[i,3]`` represents the\n       number of original observations (leaves) in the non-singleton\n       cluster ``i``.\n    This function is useful when loading in linkages from legacy data\n    files generated by MATLAB.\n    Parameters\n    ----------\n    Z : ndarray\n        A linkage matrix generated by MATLAB(TM).\n    Returns\n    -------\n    ZS : ndarray\n        A linkage matrix compatible with ``scipy.cluster.hierarchy``.\n    See Also\n    --------\n    linkage: for a description of what a linkage matrix is.\n    to_mlab_linkage: transform from SciPy to MATLAB format.\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from scipy.cluster.hierarchy import ward, from_mlab_linkage\n    Given a linkage matrix in MATLAB format ``mZ``, we can use\n    `scipy.cluster.hierarchy.from_mlab_linkage` to import\n    it into SciPy format:\n    &gt;&gt;&gt; mZ = np.array([[1, 2, 1], [4, 5, 1], [7, 8, 1],\n    ...                [10, 11, 1], [3, 13, 1.29099445],\n    ...                [6, 14, 1.29099445],\n    ...                [9, 15, 1.29099445],\n    ...                [12, 16, 1.29099445],\n    ...                [17, 18, 5.77350269],\n    ...                [19, 20, 5.77350269],\n    ...                [21, 22,  8.16496581]])\n    &gt;&gt;&gt; Z = from_mlab_linkage(mZ)\n    &gt;&gt;&gt; Z\n    array([[  0.        ,   1.        ,   1.        ,   2.        ],\n           [  3.        ,   4.        ,   1.        ,   2.        ],\n           [  6.        ,   7.        ,   1.        ,   2.        ],\n           [  9.        ,  10.        ,   1.        ,   2.        ],\n           [  2.        ,  12.        ,   1.29099445,   3.        ],\n           [  5.        ,  13.        ,   1.29099445,   3.        ],\n           [  8.        ,  14.        ,   1.29099445,   3.        ],\n           [ 11.        ,  15.        ,   1.29099445,   3.        ],\n           [ 16.        ,  17.        ,   5.77350269,   6.        ],\n           [ 18.        ,  19.        ,   5.77350269,   6.        ],\n           [ 20.        ,  21.        ,   8.16496581,  12.        ]])\n    As expected, the linkage matrix ``Z`` returned includes an\n    additional column counting the number of original samples in\n    each cluster. Also, all cluster indices are reduced by 1\n    (MATLAB format uses 1-indexing, whereas SciPy uses 0-indexing).\n    \"\"\"\n    Z = np.asarray(Z, dtype=np.double, order='c')\n    Zs = Z.shape\n\n    # If it's empty, return it.\n    if len(Zs) == 0 or (len(Zs) == 1 and Zs[0] == 0):\n        return Z.copy()\n\n    if len(Zs) != 2:\n        raise ValueError(\"The linkage array must be rectangular.\")\n\n    # If it contains no rows, return it.\n    if Zs[0] == 0:\n        return Z.copy()\n\n    Zpart = Z.copy()\n    if Zpart[:, 0:2].min() != 1.0 and Zpart[:, 0:2].max() != 2 * Zs[0]:\n        raise ValueError('The format of the indices is not 1..N')\n\n    Zpart[:, 0:2] -= 1.0\n    CS = np.zeros((Zs[0],), dtype=np.double)\n    _hierarchy.calculate_cluster_sizes(Zpart, CS, int(Zs[0]) + 1)\n    return np.hstack([Zpart, CS.reshape(Zs[0], 1)])\n\n\ndef to_mlab_linkage(Z):\n    \"\"\"\n    Convert a linkage matrix to a MATLAB(TM) compatible one.\n    Converts a linkage matrix ``Z`` generated by the linkage function\n    of this module to a MATLAB(TM) compatible one. The return linkage\n    matrix has the last column removed and the cluster indices are\n    converted to ``1..N`` indexing.\n    Parameters\n    ----------\n    Z : ndarray\n        A linkage matrix generated by ``scipy.cluster.hierarchy``.\n    Returns\n    -------\n    to_mlab_linkage : ndarray\n        A linkage matrix compatible with MATLAB(TM)'s hierarchical\n        clustering functions.\n        The return linkage matrix has the last column removed\n        and the cluster indices are converted to ``1..N`` indexing.\n    See Also\n    --------\n    linkage: for a description of what a linkage matrix is.\n    from_mlab_linkage: transform from Matlab to SciPy format.\n    Examples\n    --------\n    &gt;&gt;&gt; from scipy.cluster.hierarchy import ward, to_mlab_linkage\n    &gt;&gt;&gt; from scipy.spatial.distance import pdist\n    &gt;&gt;&gt; X = [[0, 0], [0, 1], [1, 0],\n    ...      [0, 4], [0, 3], [1, 4],\n    ...      [4, 0], [3, 0], [4, 1],\n    ...      [4, 4], [3, 4], [4, 3]]\n    &gt;&gt;&gt; Z = ward(pdist(X))\n    &gt;&gt;&gt; Z\n    array([[ 0.        ,  1.        ,  1.        ,  2.        ],\n           [ 3.        ,  4.        ,  1.        ,  2.        ],\n           [ 6.        ,  7.        ,  1.        ,  2.        ],\n           [ 9.        , 10.        ,  1.        ,  2.        ],\n           [ 2.        , 12.        ,  1.29099445,  3.        ],\n           [ 5.        , 13.        ,  1.29099445,  3.        ],\n           [ 8.        , 14.        ,  1.29099445,  3.        ],\n           [11.        , 15.        ,  1.29099445,  3.        ],\n           [16.        , 17.        ,  5.77350269,  6.        ],\n           [18.        , 19.        ,  5.77350269,  6.        ],\n           [20.        , 21.        ,  8.16496581, 12.        ]])\n    After a linkage matrix ``Z`` has been created, we can use\n    `scipy.cluster.hierarchy.to_mlab_linkage` to convert it\n    into MATLAB format:\n    &gt;&gt;&gt; mZ = to_mlab_linkage(Z)\n    &gt;&gt;&gt; mZ\n    array([[  1.        ,   2.        ,   1.        ],\n           [  4.        ,   5.        ,   1.        ],\n           [  7.        ,   8.        ,   1.        ],\n           [ 10.        ,  11.        ,   1.        ],\n           [  3.        ,  13.        ,   1.29099445],\n           [  6.        ,  14.        ,   1.29099445],\n           [  9.        ,  15.        ,   1.29099445],\n           [ 12.        ,  16.        ,   1.29099445],\n           [ 17.        ,  18.        ,   5.77350269],\n           [ 19.        ,  20.        ,   5.77350269],\n           [ 21.        ,  22.        ,   8.16496581]])\n    The new linkage matrix ``mZ`` uses 1-indexing for all the\n    clusters (instead of 0-indexing). Also, the last column of\n    the original linkage matrix has been dropped.\n    \"\"\"\n    Z = np.asarray(Z, order='c', dtype=np.double)\n    Zs = Z.shape\n    if len(Zs) == 0 or (len(Zs) == 1 and Zs[0] == 0):\n        return Z.copy()\n    is_valid_linkage(Z, throw=True, name='Z')\n\n    ZP = Z[:, 0:3].copy()\n    ZP[:, 0:2] += 1.0\n\n    return ZP\n\n\ndef is_monotonic(Z):\n    \"\"\"\n    Return True if the linkage passed is monotonic.\n    The linkage is monotonic if for every cluster :math:`s` and :math:`t`\n    joined, the distance between them is no less than the distance\n    between any previously joined clusters.\n    Parameters\n    ----------\n    Z : ndarray\n        The linkage matrix to check for monotonicity.\n    Returns\n    -------\n    b : bool\n        A boolean indicating whether the linkage is monotonic.\n    See Also\n    --------\n    linkage: for a description of what a linkage matrix is.\n    Examples\n    --------\n    &gt;&gt;&gt; from scipy.cluster.hierarchy import median, ward, is_monotonic\n    &gt;&gt;&gt; from scipy.spatial.distance import pdist\n    By definition, some hierarchical clustering algorithms - such as\n    `scipy.cluster.hierarchy.ward` - produce monotonic assignments of\n    samples to clusters; however, this is not always true for other\n    hierarchical methods - e.g. `scipy.cluster.hierarchy.median`.\n    Given a linkage matrix ``Z`` (as the result of a hierarchical clustering\n    method) we can test programmatically whether it has the monotonicity\n    property or not, using `scipy.cluster.hierarchy.is_monotonic`:\n    &gt;&gt;&gt; X = [[0, 0], [0, 1], [1, 0],\n    ...      [0, 4], [0, 3], [1, 4],\n    ...      [4, 0], [3, 0], [4, 1],\n    ...      [4, 4], [3, 4], [4, 3]]\n    &gt;&gt;&gt; Z = ward(pdist(X))\n    &gt;&gt;&gt; Z\n    array([[ 0.        ,  1.        ,  1.        ,  2.        ],\n           [ 3.        ,  4.        ,  1.        ,  2.        ],\n           [ 6.        ,  7.        ,  1.        ,  2.        ],\n           [ 9.        , 10.        ,  1.        ,  2.        ],\n           [ 2.        , 12.        ,  1.29099445,  3.        ],\n           [ 5.        , 13.        ,  1.29099445,  3.        ],\n           [ 8.        , 14.        ,  1.29099445,  3.        ],\n           [11.        , 15.        ,  1.29099445,  3.        ],\n           [16.        , 17.        ,  5.77350269,  6.        ],\n           [18.        , 19.        ,  5.77350269,  6.        ],\n           [20.        , 21.        ,  8.16496581, 12.        ]])\n    &gt;&gt;&gt; is_monotonic(Z)\n    True\n    &gt;&gt;&gt; Z = median(pdist(X))\n    &gt;&gt;&gt; Z\n    array([[ 0.        ,  1.        ,  1.        ,  2.        ],\n           [ 3.        ,  4.        ,  1.        ,  2.        ],\n           [ 9.        , 10.        ,  1.        ,  2.        ],\n           [ 6.        ,  7.        ,  1.        ,  2.        ],\n           [ 2.        , 12.        ,  1.11803399,  3.        ],\n           [ 5.        , 13.        ,  1.11803399,  3.        ],\n           [ 8.        , 15.        ,  1.11803399,  3.        ],\n           [11.        , 14.        ,  1.11803399,  3.        ],\n           [18.        , 19.        ,  3.        ,  6.        ],\n           [16.        , 17.        ,  3.5       ,  6.        ],\n           [20.        , 21.        ,  3.25      , 12.        ]])\n    &gt;&gt;&gt; is_monotonic(Z)\n    False\n    Note that this method is equivalent to just verifying that the distances\n    in the third column of the linkage matrix appear in a monotonically\n    increasing order.\n    \"\"\"\n    Z = np.asarray(Z, order='c')\n    is_valid_linkage(Z, throw=True, name='Z')\n\n    # We expect the i'th value to be greater than its successor.\n    return (Z[1:, 2] &gt;= Z[:-1, 2]).all()\n\n\ndef is_valid_im(R, warning=False, throw=False, name=None):\n    \"\"\"Return True if the inconsistency matrix passed is valid.\n    It must be a :math:`n` by 4 array of doubles. The standard\n    deviations ``R[:,1]`` must be nonnegative. The link counts\n    ``R[:,2]`` must be positive and no greater than :math:`n-1`.\n    Parameters\n    ----------\n    R : ndarray\n        The inconsistency matrix to check for validity.\n    warning : bool, optional\n         When True, issues a Python warning if the linkage\n         matrix passed is invalid.\n    throw : bool, optional\n         When True, throws a Python exception if the linkage\n         matrix passed is invalid.\n    name : str, optional\n         This string refers to the variable name of the invalid\n         linkage matrix.\n    Returns\n    -------\n    b : bool\n        True if the inconsistency matrix is valid.\n    See Also\n    --------\n    linkage: for a description of what a linkage matrix is.\n    inconsistent: for the creation of a inconsistency matrix.\n    Examples\n    --------\n    &gt;&gt;&gt; from scipy.cluster.hierarchy import ward, inconsistent, is_valid_im\n    &gt;&gt;&gt; from scipy.spatial.distance import pdist\n    Given a data set ``X``, we can apply a clustering method to obtain a\n    linkage matrix ``Z``. `scipy.cluster.hierarchy.inconsistent` can\n    be also used to obtain the inconsistency matrix ``R`` associated to\n    this clustering process:\n    &gt;&gt;&gt; X = [[0, 0], [0, 1], [1, 0],\n    ...      [0, 4], [0, 3], [1, 4],\n    ...      [4, 0], [3, 0], [4, 1],\n    ...      [4, 4], [3, 4], [4, 3]]\n    &gt;&gt;&gt; Z = ward(pdist(X))\n    &gt;&gt;&gt; R = inconsistent(Z)\n    &gt;&gt;&gt; Z\n    array([[ 0.        ,  1.        ,  1.        ,  2.        ],\n           [ 3.        ,  4.        ,  1.        ,  2.        ],\n           [ 6.        ,  7.        ,  1.        ,  2.        ],\n           [ 9.        , 10.        ,  1.        ,  2.        ],\n           [ 2.        , 12.        ,  1.29099445,  3.        ],\n           [ 5.        , 13.        ,  1.29099445,  3.        ],\n           [ 8.        , 14.        ,  1.29099445,  3.        ],\n           [11.        , 15.        ,  1.29099445,  3.        ],\n           [16.        , 17.        ,  5.77350269,  6.        ],\n           [18.        , 19.        ,  5.77350269,  6.        ],\n           [20.        , 21.        ,  8.16496581, 12.        ]])\n    &gt;&gt;&gt; R\n    array([[1.        , 0.        , 1.        , 0.        ],\n           [1.        , 0.        , 1.        , 0.        ],\n           [1.        , 0.        , 1.        , 0.        ],\n           [1.        , 0.        , 1.        , 0.        ],\n           [1.14549722, 0.20576415, 2.        , 0.70710678],\n           [1.14549722, 0.20576415, 2.        , 0.70710678],\n           [1.14549722, 0.20576415, 2.        , 0.70710678],\n           [1.14549722, 0.20576415, 2.        , 0.70710678],\n           [2.78516386, 2.58797734, 3.        , 1.15470054],\n           [2.78516386, 2.58797734, 3.        , 1.15470054],\n           [6.57065706, 1.38071187, 3.        , 1.15470054]])\n    Now we can use `scipy.cluster.hierarchy.is_valid_im` to verify that\n    ``R`` is correct:\n    &gt;&gt;&gt; is_valid_im(R)\n    True\n    However, if ``R`` is wrongly constructed (e.g., one of the standard\n    deviations is set to a negative value), then the check will fail:\n    &gt;&gt;&gt; R[-1,1] = R[-1,1] * -1\n    &gt;&gt;&gt; is_valid_im(R)\n    False\n    \"\"\"\n    R = np.asarray(R, order='c')\n    valid = True\n    name_str = \"%r \" % name if name else ''\n    try:\n        if type(R) != np.ndarray:\n            raise TypeError('Variable %spassed as inconsistency matrix is not '\n                            'a numpy array.' % name_str)\n        if R.dtype != np.double:\n            raise TypeError('Inconsistency matrix %smust contain doubles '\n                            '(double).' % name_str)\n        if len(R.shape) != 2:\n            raise ValueError('Inconsistency matrix %smust have shape=2 (i.e. '\n                             'be two-dimensional).' % name_str)\n        if R.shape[1] != 4:\n            raise ValueError('Inconsistency matrix %smust have 4 columns.' %\n                             name_str)\n        if R.shape[0] &lt; 1:\n            raise ValueError('Inconsistency matrix %smust have at least one '\n                             'row.' % name_str)\n        if (R[:, 0] &lt; 0).any():\n            raise ValueError('Inconsistency matrix %scontains negative link '\n                             'height means.' % name_str)\n        if (R[:, 1] &lt; 0).any():\n            raise ValueError('Inconsistency matrix %scontains negative link '\n                             'height standard deviations.' % name_str)\n        if (R[:, 2] &lt; 0).any():\n            raise ValueError('Inconsistency matrix %scontains negative link '\n                             'counts.' % name_str)\n    except Exception as e:\n        if throw:\n            raise\n        if warning:\n            _warning(str(e))\n        valid = False\n\n    return valid\n\n\ndef is_valid_linkage(Z, warning=False, throw=False, name=None):\n    \"\"\"\n    Check the validity of a linkage matrix.\n    A linkage matrix is valid if it is a 2-D array (type double)\n    with :math:`n` rows and 4 columns. The first two columns must contain\n    indices between 0 and :math:`2n-1`. For a given row ``i``, the following\n    two expressions have to hold:\n    .. math::\n        0 \\\\leq \\\\mathtt{Z[i,0]} \\\\leq i+n-1\n        0 \\\\leq Z[i,1] \\\\leq i+n-1\n    I.e., a cluster cannot join another cluster unless the cluster being joined\n    has been generated.\n    Parameters\n    ----------\n    Z : array_like\n        Linkage matrix.\n    warning : bool, optional\n        When True, issues a Python warning if the linkage\n        matrix passed is invalid.\n    throw : bool, optional\n        When True, throws a Python exception if the linkage\n        matrix passed is invalid.\n    name : str, optional\n        This string refers to the variable name of the invalid\n        linkage matrix.\n    Returns\n    -------\n    b : bool\n        True if the inconsistency matrix is valid.\n    See Also\n    --------\n    linkage: for a description of what a linkage matrix is.\n    Examples\n    --------\n    &gt;&gt;&gt; from scipy.cluster.hierarchy import ward, is_valid_linkage\n    &gt;&gt;&gt; from scipy.spatial.distance import pdist\n    All linkage matrices generated by the clustering methods in this module\n    will be valid (i.e., they will have the appropriate dimensions and the two\n    required expressions will hold for all the rows).\n    We can check this using `scipy.cluster.hierarchy.is_valid_linkage`:\n    &gt;&gt;&gt; X = [[0, 0], [0, 1], [1, 0],\n    ...      [0, 4], [0, 3], [1, 4],\n    ...      [4, 0], [3, 0], [4, 1],\n    ...      [4, 4], [3, 4], [4, 3]]\n    &gt;&gt;&gt; Z = ward(pdist(X))\n    &gt;&gt;&gt; Z\n    array([[ 0.        ,  1.        ,  1.        ,  2.        ],\n           [ 3.        ,  4.        ,  1.        ,  2.        ],\n           [ 6.        ,  7.        ,  1.        ,  2.        ],\n           [ 9.        , 10.        ,  1.        ,  2.        ],\n           [ 2.        , 12.        ,  1.29099445,  3.        ],\n           [ 5.        , 13.        ,  1.29099445,  3.        ],\n           [ 8.        , 14.        ,  1.29099445,  3.        ],\n           [11.        , 15.        ,  1.29099445,  3.        ],\n           [16.        , 17.        ,  5.77350269,  6.        ],\n           [18.        , 19.        ,  5.77350269,  6.        ],\n           [20.        , 21.        ,  8.16496581, 12.        ]])\n    &gt;&gt;&gt; is_valid_linkage(Z)\n    True\n    However, if we create a linkage matrix in a wrong way - or if we modify\n    a valid one in a way that any of the required expressions don't hold\n    anymore, then the check will fail:\n    &gt;&gt;&gt; Z[3][1] = 20    # the cluster number 20 is not defined at this point\n    &gt;&gt;&gt; is_valid_linkage(Z)\n    False\n    \"\"\"\n    Z = np.asarray(Z, order='c')\n    valid = True\n    name_str = \"%r \" % name if name else ''\n    try:\n        if type(Z) != np.ndarray:\n            raise TypeError('Passed linkage argument %sis not a valid array.' %\n                            name_str)\n        if Z.dtype != np.double:\n            raise TypeError('Linkage matrix %smust contain doubles.' % name_str)\n        if len(Z.shape) != 2:\n            raise ValueError('Linkage matrix %smust have shape=2 (i.e. be '\n                             'two-dimensional).' % name_str)\n        if Z.shape[1] != 4:\n            raise ValueError('Linkage matrix %smust have 4 columns.' % name_str)\n        if Z.shape[0] == 0:\n            raise ValueError('Linkage must be computed on at least two '\n                             'observations.')\n        n = Z.shape[0]\n        if n &gt; 1:\n            if ((Z[:, 0] &lt; 0).any() or (Z[:, 1] &lt; 0).any()):\n                raise ValueError('Linkage %scontains negative indices.' %\n                                 name_str)\n            if (Z[:, 2] &lt; 0).any():\n                raise ValueError('Linkage %scontains negative distances.' %\n                                 name_str)\n            if (Z[:, 3] &lt; 0).any():\n                raise ValueError('Linkage %scontains negative counts.' %\n                                 name_str)\n        if _check_hierarchy_uses_cluster_before_formed(Z):\n            raise ValueError('Linkage %suses non-singleton cluster before '\n                             'it is formed.' % name_str)\n        if _check_hierarchy_uses_cluster_more_than_once(Z):\n            raise ValueError('Linkage %suses the same cluster more than once.'\n                             % name_str)\n    except Exception as e:\n        if throw:\n            raise\n        if warning:\n            _warning(str(e))\n        valid = False\n\n    return valid\n\n\ndef _check_hierarchy_uses_cluster_before_formed(Z):\n    n = Z.shape[0] + 1\n    for i in range(0, n - 1):\n        if Z[i, 0] &gt;= n + i or Z[i, 1] &gt;= n + i:\n            return True\n    return False\n\n\ndef _check_hierarchy_uses_cluster_more_than_once(Z):\n    n = Z.shape[0] + 1\n    chosen = set([])\n    for i in range(0, n - 1):\n        if (Z[i, 0] in chosen) or (Z[i, 1] in chosen) or Z[i, 0] == Z[i, 1]:\n            return True\n        chosen.add(Z[i, 0])\n        chosen.add(Z[i, 1])\n    return False\n\n\ndef _check_hierarchy_not_all_clusters_used(Z):\n    n = Z.shape[0] + 1\n    chosen = set([])\n    for i in range(0, n - 1):\n        chosen.add(int(Z[i, 0]))\n        chosen.add(int(Z[i, 1]))\n    must_chosen = set(range(0, 2 * n - 2))\n    return len(must_chosen.difference(chosen)) &gt; 0\n\n\ndef num_obs_linkage(Z):\n    \"\"\"\n    Return the number of original observations of the linkage matrix passed.\n    Parameters\n    ----------\n    Z : ndarray\n        The linkage matrix on which to perform the operation.\n    Returns\n    -------\n    n : int\n        The number of original observations in the linkage.\n    Examples\n    --------\n    &gt;&gt;&gt; from scipy.cluster.hierarchy import ward, num_obs_linkage\n    &gt;&gt;&gt; from scipy.spatial.distance import pdist\n    &gt;&gt;&gt; X = [[0, 0], [0, 1], [1, 0],\n    ...      [0, 4], [0, 3], [1, 4],\n    ...      [4, 0], [3, 0], [4, 1],\n    ...      [4, 4], [3, 4], [4, 3]]\n    &gt;&gt;&gt; Z = ward(pdist(X))\n    ``Z`` is a linkage matrix obtained after using the Ward clustering method\n    with ``X``, a dataset with 12 data points.\n    &gt;&gt;&gt; num_obs_linkage(Z)\n    12\n    \"\"\"\n    Z = np.asarray(Z, order='c')\n    is_valid_linkage(Z, throw=True, name='Z')\n    return (Z.shape[0] + 1)\n\n\ndef correspond(Z, Y):\n    \"\"\"\n    Check for correspondence between linkage and condensed distance matrices.\n    They must have the same number of original observations for\n    the check to succeed.\n    This function is useful as a sanity check in algorithms that make\n    extensive use of linkage and distance matrices that must\n    correspond to the same set of original observations.\n    Parameters\n    ----------\n    Z : array_like\n        The linkage matrix to check for correspondence.\n    Y : array_like\n        The condensed distance matrix to check for correspondence.\n    Returns\n    -------\n    b : bool\n        A boolean indicating whether the linkage matrix and distance\n        matrix could possibly correspond to one another.\n    See Also\n    --------\n    linkage: for a description of what a linkage matrix is.\n    Examples\n    --------\n    &gt;&gt;&gt; from scipy.cluster.hierarchy import ward, correspond\n    &gt;&gt;&gt; from scipy.spatial.distance import pdist\n    This method can be used to check if a given linkage matrix ``Z`` has been\n    obtained from the application of a cluster method over a dataset ``X``:\n    &gt;&gt;&gt; X = [[0, 0], [0, 1], [1, 0],\n    ...      [0, 4], [0, 3], [1, 4],\n    ...      [4, 0], [3, 0], [4, 1],\n    ...      [4, 4], [3, 4], [4, 3]]\n    &gt;&gt;&gt; X_condensed = pdist(X)\n    &gt;&gt;&gt; Z = ward(X_condensed)\n    Here, we can compare ``Z`` and ``X`` (in condensed form):\n    &gt;&gt;&gt; correspond(Z, X_condensed)\n    True\n    \"\"\"\n    is_valid_linkage(Z, throw=True)\n    distance.is_valid_y(Y, throw=True)\n    Z = np.asarray(Z, order='c')\n    Y = np.asarray(Y, order='c')\n    return distance.num_obs_y(Y) == num_obs_linkage(Z)\n\n\ndef fcluster(Z, t, criterion='inconsistent', depth=2, R=None, monocrit=None):\n    \"\"\"\n    Form flat clusters from the hierarchical clustering defined by\n    the given linkage matrix.\n    Parameters\n    ----------\n    Z : ndarray\n        The hierarchical clustering encoded with the matrix returned\n        by the `linkage` function.\n    t : scalar\n        For criteria 'inconsistent', 'distance' or 'monocrit',\n         this is the threshold to apply when forming flat clusters.\n        For 'maxclust' or 'maxclust_monocrit' criteria,\n         this would be max number of clusters requested.\n    criterion : str, optional\n        The criterion to use in forming flat clusters. This can\n        be any of the following values:\n          ``inconsistent`` :\n              If a cluster node and all its\n              descendants have an inconsistent value less than or equal\n              to `t`, then all its leaf descendants belong to the\n              same flat cluster. When no non-singleton cluster meets\n              this criterion, every node is assigned to its own\n              cluster. (Default)\n          ``distance`` :\n              Forms flat clusters so that the original\n              observations in each flat cluster have no greater a\n              cophenetic distance than `t`.\n          ``maxclust`` :\n              Finds a minimum threshold ``r`` so that\n              the cophenetic distance between any two original\n              observations in the same flat cluster is no more than\n              ``r`` and no more than `t` flat clusters are formed.\n          ``monocrit`` :\n              Forms a flat cluster from a cluster node c\n              with index i when ``monocrit[j] &lt;= t``.\n              For example, to threshold on the maximum mean distance\n              as computed in the inconsistency matrix R with a\n              threshold of 0.8 do::\n                  MR = maxRstat(Z, R, 3)\n                  fcluster(Z, t=0.8, criterion='monocrit', monocrit=MR)\n          ``maxclust_monocrit`` :\n              Forms a flat cluster from a\n              non-singleton cluster node ``c`` when ``monocrit[i] &lt;=\n              r`` for all cluster indices ``i`` below and including\n              ``c``. ``r`` is minimized such that no more than ``t``\n              flat clusters are formed. monocrit must be\n              monotonic. For example, to minimize the threshold t on\n              maximum inconsistency values so that no more than 3 flat\n              clusters are formed, do::\n                  MI = maxinconsts(Z, R)\n                  fcluster(Z, t=3, criterion='maxclust_monocrit', monocrit=MI)\n    depth : int, optional\n        The maximum depth to perform the inconsistency calculation.\n        It has no meaning for the other criteria. Default is 2.\n    R : ndarray, optional\n        The inconsistency matrix to use for the 'inconsistent'\n        criterion. This matrix is computed if not provided.\n    monocrit : ndarray, optional\n        An array of length n-1. `monocrit[i]` is the\n        statistics upon which non-singleton i is thresholded. The\n        monocrit vector must be monotonic, i.e., given a node c with\n        index i, for all node indices j corresponding to nodes\n        below c, ``monocrit[i] &gt;= monocrit[j]``.\n    Returns\n    -------\n    fcluster : ndarray\n        An array of length ``n``. ``T[i]`` is the flat cluster number to\n        which original observation ``i`` belongs.\n    See Also\n    --------\n    linkage : for information about hierarchical clustering methods work.\n    Examples\n    --------\n    &gt;&gt;&gt; from scipy.cluster.hierarchy import ward, fcluster\n    &gt;&gt;&gt; from scipy.spatial.distance import pdist\n    All cluster linkage methods - e.g., `scipy.cluster.hierarchy.ward`\n    generate a linkage matrix ``Z`` as their output:\n    &gt;&gt;&gt; X = [[0, 0], [0, 1], [1, 0],\n    ...      [0, 4], [0, 3], [1, 4],\n    ...      [4, 0], [3, 0], [4, 1],\n    ...      [4, 4], [3, 4], [4, 3]]\n    &gt;&gt;&gt; Z = ward(pdist(X))\n    &gt;&gt;&gt; Z\n    array([[ 0.        ,  1.        ,  1.        ,  2.        ],\n           [ 3.        ,  4.        ,  1.        ,  2.        ],\n           [ 6.        ,  7.        ,  1.        ,  2.        ],\n           [ 9.        , 10.        ,  1.        ,  2.        ],\n           [ 2.        , 12.        ,  1.29099445,  3.        ],\n           [ 5.        , 13.        ,  1.29099445,  3.        ],\n           [ 8.        , 14.        ,  1.29099445,  3.        ],\n           [11.        , 15.        ,  1.29099445,  3.        ],\n           [16.        , 17.        ,  5.77350269,  6.        ],\n           [18.        , 19.        ,  5.77350269,  6.        ],\n           [20.        , 21.        ,  8.16496581, 12.        ]])\n    This matrix represents a dendrogram, where the first and second elements\n    are the two clusters merged at each step, the third element is the\n    distance between these clusters, and the fourth element is the size of\n    the new cluster - the number of original data points included.\n    `scipy.cluster.hierarchy.fcluster` can be used to flatten the\n    dendrogram, obtaining as a result an assignation of the original data\n    points to single clusters.\n    This assignation mostly depends on a distance threshold ``t`` - the maximum\n    inter-cluster distance allowed:\n    &gt;&gt;&gt; fcluster(Z, t=0.9, criterion='distance')\n    array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12], dtype=int32)\n    &gt;&gt;&gt; fcluster(Z, t=1.1, criterion='distance')\n    array([1, 1, 2, 3, 3, 4, 5, 5, 6, 7, 7, 8], dtype=int32)\n    &gt;&gt;&gt; fcluster(Z, t=3, criterion='distance')\n    array([1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4], dtype=int32)\n    &gt;&gt;&gt; fcluster(Z, t=9, criterion='distance')\n    array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)\n    In the first case, the threshold ``t`` is too small to allow any two\n    samples in the data to form a cluster, so 12 different clusters are\n    returned.\n    In the second case, the threshold is large enough to allow the first\n    4 points to be merged with their nearest neighbors. So, here, only 8\n    clusters are returned.\n    The third case, with a much higher threshold, allows for up to 8 data\n    points to be connected - so 4 clusters are returned here.\n    Lastly, the threshold of the fourth case is large enough to allow for\n    all data points to be merged together - so a single cluster is returned.\n    \"\"\"\n    Z = np.asarray(Z, order='c')\n    is_valid_linkage(Z, throw=True, name='Z')\n\n    n = Z.shape[0] + 1\n    T = np.zeros((n,), dtype='i')\n\n    # Since the C code does not support striding using strides.\n    # The dimensions are used instead.\n    [Z] = _copy_arrays_if_base_present([Z])\n\n    if criterion == 'inconsistent':\n        if R is None:\n            R = inconsistent(Z, depth)\n        else:\n            R = np.asarray(R, order='c')\n            is_valid_im(R, throw=True, name='R')\n            # Since the C code does not support striding using strides.\n            # The dimensions are used instead.\n            [R] = _copy_arrays_if_base_present([R])\n        _hierarchy.cluster_in(Z, R, T, float(t), int(n))\n    elif criterion == 'distance':\n        _hierarchy.cluster_dist(Z, T, float(t), int(n))\n    elif criterion == 'maxclust':\n        _hierarchy.cluster_maxclust_dist(Z, T, int(n), int(t))\n    elif criterion == 'monocrit':\n        [monocrit] = _copy_arrays_if_base_present([monocrit])\n        _hierarchy.cluster_monocrit(Z, monocrit, T, float(t), int(n))\n    elif criterion == 'maxclust_monocrit':\n        [monocrit] = _copy_arrays_if_base_present([monocrit])\n        _hierarchy.cluster_maxclust_monocrit(Z, monocrit, T, int(n), int(t))\n    else:\n        raise ValueError('Invalid cluster formation criterion: %s'\n                         % str(criterion))\n    return T\n\n\ndef fclusterdata(X, t, criterion='inconsistent',\n                 metric='euclidean', depth=2, method='single', R=None):\n    \"\"\"\n    Cluster observation data using a given metric.\n    Clusters the original observations in the n-by-m data\n    matrix X (n observations in m dimensions), using the euclidean\n    distance metric to calculate distances between original observations,\n    performs hierarchical clustering using the single linkage algorithm,\n    and forms flat clusters using the inconsistency method with `t` as the\n    cut-off threshold.\n    A 1-D array ``T`` of length ``n`` is returned. ``T[i]`` is\n    the index of the flat cluster to which the original observation ``i``\n    belongs.\n    Parameters\n    ----------\n    X : (N, M) ndarray\n        N by M data matrix with N observations in M dimensions.\n    t : scalar\n        For criteria 'inconsistent', 'distance' or 'monocrit',\n         this is the threshold to apply when forming flat clusters.\n        For 'maxclust' or 'maxclust_monocrit' criteria,\n         this would be max number of clusters requested.\n    criterion : str, optional\n        Specifies the criterion for forming flat clusters. Valid\n        values are 'inconsistent' (default), 'distance', or 'maxclust'\n        cluster formation algorithms. See `fcluster` for descriptions.\n    metric : str or function, optional\n        The distance metric for calculating pairwise distances. See\n        ``distance.pdist`` for descriptions and linkage to verify\n        compatibility with the linkage method.\n    depth : int, optional\n        The maximum depth for the inconsistency calculation. See\n        `inconsistent` for more information.\n    method : str, optional\n        The linkage method to use (single, complete, average,\n        weighted, median centroid, ward). See `linkage` for more\n        information. Default is \"single\".\n    R : ndarray, optional\n        The inconsistency matrix. It will be computed if necessary\n        if it is not passed.\n    Returns\n    -------\n    fclusterdata : ndarray\n        A vector of length n. T[i] is the flat cluster number to\n        which original observation i belongs.\n    See Also\n    --------\n    scipy.spatial.distance.pdist : pairwise distance metrics\n    Notes\n    -----\n    This function is similar to the MATLAB function ``clusterdata``.\n    Examples\n    --------\n    &gt;&gt;&gt; from scipy.cluster.hierarchy import fclusterdata\n    This is a convenience method that abstracts all the steps to perform in a\n    typical SciPy's hierarchical clustering workflow.\n    * Transform the input data into a condensed matrix with `scipy.spatial.distance.pdist`.\n    * Apply a clustering method.\n    * Obtain flat clusters at a user defined distance threshold ``t`` using `scipy.cluster.hierarchy.fcluster`.\n    &gt;&gt;&gt; X = [[0, 0], [0, 1], [1, 0],\n    ...      [0, 4], [0, 3], [1, 4],\n    ...      [4, 0], [3, 0], [4, 1],\n    ...      [4, 4], [3, 4], [4, 3]]\n    &gt;&gt;&gt; fclusterdata(X, t=1)\n    array([3, 3, 3, 4, 4, 4, 2, 2, 2, 1, 1, 1], dtype=int32)\n    The output here (for the dataset ``X``, distance threshold ``t``, and the\n    default settings) is four clusters with three data points each.\n    \"\"\"\n    X = np.asarray(X, order='c', dtype=np.double)\n\n    if type(X) != np.ndarray or len(X.shape) != 2:\n        raise TypeError('The observation matrix X must be an n by m numpy '\n                        'array.')\n\n    Y = distance.pdist(X, metric=metric)\n    Z = linkage(Y, method=method)\n    if R is None:\n        R = inconsistent(Z, d=depth)\n    else:\n        R = np.asarray(R, order='c')\n    T = fcluster(Z, criterion=criterion, depth=depth, R=R, t=t)\n    return T\n\n\ndef leaves_list(Z):\n    \"\"\"\n    Return a list of leaf node ids.\n    The return corresponds to the observation vector index as it appears\n    in the tree from left to right. Z is a linkage matrix.\n    Parameters\n    ----------\n    Z : ndarray\n        The hierarchical clustering encoded as a matrix.  `Z` is\n        a linkage matrix.  See `linkage` for more information.\n    Returns\n    -------\n    leaves_list : ndarray\n        The list of leaf node ids.\n    See Also\n    --------\n    dendrogram: for information about dendrogram structure.\n    Examples\n    --------\n    &gt;&gt;&gt; from scipy.cluster.hierarchy import ward, dendrogram, leaves_list\n    &gt;&gt;&gt; from scipy.spatial.distance import pdist\n    &gt;&gt;&gt; from matplotlib import pyplot as plt\n    &gt;&gt;&gt; X = [[0, 0], [0, 1], [1, 0],\n    ...      [0, 4], [0, 3], [1, 4],\n    ...      [4, 0], [3, 0], [4, 1],\n    ...      [4, 4], [3, 4], [4, 3]]\n    &gt;&gt;&gt; Z = ward(pdist(X))\n    The linkage matrix ``Z`` represents a dendrogram, that is, a tree that\n    encodes the structure of the clustering performed.\n    `scipy.cluster.hierarchy.leaves_list` shows the mapping between\n    indices in the ``X`` dataset and leaves in the dendrogram:\n    &gt;&gt;&gt; leaves_list(Z)\n    array([ 2,  0,  1,  5,  3,  4,  8,  6,  7, 11,  9, 10], dtype=int32)\n    &gt;&gt;&gt; fig = plt.figure(figsize=(25, 10))\n    &gt;&gt;&gt; dn = dendrogram(Z)\n    &gt;&gt;&gt; plt.show()\n    \"\"\"\n    Z = np.asarray(Z, order='c')\n    is_valid_linkage(Z, throw=True, name='Z')\n    n = Z.shape[0] + 1\n    ML = np.zeros((n,), dtype='i')\n    [Z] = _copy_arrays_if_base_present([Z])\n    _hierarchy.prelist(Z, ML, int(n))\n    return ML\n\n\n# Maps number of leaves to text size.\n#\n# p &lt;= 20, size=\"12\"\n# 20 &lt; p &lt;= 30, size=\"10\"\n# 30 &lt; p &lt;= 50, size=\"8\"\n# 50 &lt; p &lt;= np.inf, size=\"6\"\n\n_dtextsizes = {20: 12, 30: 10, 50: 8, 85: 6, np.inf: 5}\n_drotation = {20: 0, 40: 45, np.inf: 90}\n_dtextsortedkeys = list(_dtextsizes.keys())\n_dtextsortedkeys.sort()\n_drotationsortedkeys = list(_drotation.keys())\n_drotationsortedkeys.sort()\n\n\ndef _remove_dups(L):\n    \"\"\"\n    Remove duplicates AND preserve the original order of the elements.\n    The set class is not guaranteed to do this.\n    \"\"\"\n    seen_before = set([])\n    L2 = []\n    for i in L:\n        if i not in seen_before:\n            seen_before.add(i)\n            L2.append(i)\n    return L2\n\n\ndef _get_tick_text_size(p):\n    for k in _dtextsortedkeys:\n        if p &lt;= k:\n            return _dtextsizes[k]\n\n\ndef _get_tick_rotation(p):\n    for k in _drotationsortedkeys:\n        if p &lt;= k:\n            return _drotation[k]\n\n\ndef _plot_dendrogram(icoords, dcoords, ivl, p, n, mh, orientation,\n                     no_labels, color_list, leaf_font_size=None,\n                     leaf_rotation=None, contraction_marks=None,\n                     ax=None, above_threshold_color='C0'):\n    # Import matplotlib here so that it's not imported unless dendrograms\n    # are plotted. Raise an informative error if importing fails.\n    try:\n        # if an axis is provided, don't use pylab at all\n        if ax is None:\n            import matplotlib.pylab\n        import matplotlib.patches\n        import matplotlib.collections\n    except ImportError as e:\n        raise ImportError(\"You must install the matplotlib library to plot \"\n                          \"the dendrogram. Use no_plot=True to calculate the \"\n                          \"dendrogram without plotting.\") from e\n\n    if ax is None:\n        ax = matplotlib.pylab.gca()\n        # if we're using pylab, we want to trigger a draw at the end\n        trigger_redraw = True\n    else:\n        trigger_redraw = False\n\n    # Independent variable plot width\n    ivw = len(ivl) * 10\n    # Dependent variable plot height\n    dvw = mh + mh * 0.05\n\n    iv_ticks = np.arange(5, len(ivl) * 10 + 5, 10)\n    if orientation in ('top', 'bottom'):\n        if orientation == 'top':\n            ax.set_ylim([0, dvw])\n            ax.set_xlim([0, ivw])\n        else:\n            ax.set_ylim([dvw, 0])\n            ax.set_xlim([0, ivw])\n\n        xlines = icoords\n        ylines = dcoords\n        if no_labels:\n            ax.set_xticks([])\n            ax.set_xticklabels([])\n        else:\n            ax.set_xticks(iv_ticks)\n\n            if orientation == 'top':\n                ax.xaxis.set_ticks_position('bottom')\n            else:\n                ax.xaxis.set_ticks_position('top')\n\n            # Make the tick marks invisible because they cover up the links\n            for line in ax.get_xticklines():\n                line.set_visible(False)\n\n            leaf_rot = (float(_get_tick_rotation(len(ivl)))\n                        if (leaf_rotation is None) else leaf_rotation)\n            leaf_font = (float(_get_tick_text_size(len(ivl)))\n                         if (leaf_font_size is None) else leaf_font_size)\n            ax.set_xticklabels(ivl, rotation=leaf_rot, size=leaf_font)\n\n    elif orientation in ('left', 'right'):\n        if orientation == 'left':\n            ax.set_xlim([dvw, 0])\n            ax.set_ylim([0, ivw])\n        else:\n            ax.set_xlim([0, dvw])\n            ax.set_ylim([0, ivw])\n\n        xlines = dcoords\n        ylines = icoords\n        if no_labels:\n            ax.set_yticks([])\n            ax.set_yticklabels([])\n        else:\n            ax.set_yticks(iv_ticks)\n\n            if orientation == 'left':\n                ax.yaxis.set_ticks_position('right')\n            else:\n                ax.yaxis.set_ticks_position('left')\n\n            # Make the tick marks invisible because they cover up the links\n            for line in ax.get_yticklines():\n                line.set_visible(False)\n\n            leaf_font = (float(_get_tick_text_size(len(ivl)))\n                         if (leaf_font_size is None) else leaf_font_size)\n\n            if leaf_rotation is not None:\n                ax.set_yticklabels(ivl, rotation=leaf_rotation, size=leaf_font)\n            else:\n                ax.set_yticklabels(ivl, size=leaf_font)\n\n    # Let's use collections instead. This way there is a separate legend item\n    # for each tree grouping, rather than stupidly one for each line segment.\n    colors_used = _remove_dups(color_list)\n    color_to_lines = {}\n    for color in colors_used:\n        color_to_lines[color] = []\n    for (xline, yline, color) in zip(xlines, ylines, color_list):\n        color_to_lines[color].append(list(zip(xline, yline)))\n\n    colors_to_collections = {}\n    # Construct the collections.\n    for color in colors_used:\n        coll = matplotlib.collections.LineCollection(color_to_lines[color],\n                                                     colors=(color,))\n        colors_to_collections[color] = coll\n\n    # Add all the groupings below the color threshold.\n    for color in colors_used:\n        if color != above_threshold_color:\n            ax.add_collection(colors_to_collections[color])\n    # If there's a grouping of links above the color threshold, it goes last.\n    if above_threshold_color in colors_to_collections:\n        ax.add_collection(colors_to_collections[above_threshold_color])\n\n    if contraction_marks is not None:\n        Ellipse = matplotlib.patches.Ellipse\n        for (x, y) in contraction_marks:\n            if orientation in ('left', 'right'):\n                e = Ellipse((y, x), width=dvw / 100, height=1.0)\n            else:\n                e = Ellipse((x, y), width=1.0, height=dvw / 100)\n            ax.add_artist(e)\n            e.set_clip_box(ax.bbox)\n            e.set_alpha(0.5)\n            e.set_facecolor('k')\n\n    if trigger_redraw:\n        matplotlib.pylab.draw_if_interactive()\n\n\n# C0  is used for above threshhold color\n_link_line_colors_default = ('C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9')\n_link_line_colors = list(_link_line_colors_default)\n\n\ndef set_link_color_palette(palette):\n    \"\"\"\n    Set list of matplotlib color codes for use by dendrogram.\n    Note that this palette is global (i.e., setting it once changes the colors\n    for all subsequent calls to `dendrogram`) and that it affects only the\n    the colors below ``color_threshold``.\n    Note that `dendrogram` also accepts a custom coloring function through its\n    ``link_color_func`` keyword, which is more flexible and non-global.\n    Parameters\n    ----------\n    palette : list of str or None\n        A list of matplotlib color codes.  The order of the color codes is the\n        order in which the colors are cycled through when color thresholding in\n        the dendrogram.\n        If ``None``, resets the palette to its default (which are matplotlib\n        default colors C1 to C9).\n    Returns\n    -------\n    None\n    See Also\n    --------\n    dendrogram\n    Notes\n    -----\n    Ability to reset the palette with ``None`` added in SciPy 0.17.0.\n    Examples\n    --------\n    &gt;&gt;&gt; from scipy.cluster import hierarchy\n    &gt;&gt;&gt; ytdist = np.array([662., 877., 255., 412., 996., 295., 468., 268.,\n    ...                    400., 754., 564., 138., 219., 869., 669.])\n    &gt;&gt;&gt; Z = hierarchy.linkage(ytdist, 'single')\n    &gt;&gt;&gt; dn = hierarchy.dendrogram(Z, no_plot=True)\n    &gt;&gt;&gt; dn['color_list']\n    ['C1', 'C0', 'C0', 'C0', 'C0']\n    &gt;&gt;&gt; hierarchy.set_link_color_palette(['c', 'm', 'y', 'k'])\n    &gt;&gt;&gt; dn = hierarchy.dendrogram(Z, no_plot=True, above_threshold_color='b')\n    &gt;&gt;&gt; dn['color_list']\n    ['c', 'b', 'b', 'b', 'b']\n    &gt;&gt;&gt; dn = hierarchy.dendrogram(Z, no_plot=True, color_threshold=267,\n    ...                           above_threshold_color='k')\n    &gt;&gt;&gt; dn['color_list']\n    ['c', 'm', 'm', 'k', 'k']\n    Now, reset the color palette to its default:\n    &gt;&gt;&gt; hierarchy.set_link_color_palette(None)\n    \"\"\"\n    if palette is None:\n        # reset to its default\n        palette = _link_line_colors_default\n    elif type(palette) not in (list, tuple):\n        raise TypeError(\"palette must be a list or tuple\")\n    _ptypes = [isinstance(p, str) for p in palette]\n\n    if False in _ptypes:\n        raise TypeError(\"all palette list elements must be color strings\")\n\n    global _link_line_colors\n    _link_line_colors = palette\n\n\ndef dendrogram(Z, p=30, truncate_mode=None, color_threshold=None,\n               get_leaves=True, orientation='top', labels=None,\n               count_sort=False, distance_sort=False, show_leaf_counts=True,\n               no_plot=False, no_labels=False, leaf_font_size=None,\n               leaf_rotation=None, leaf_label_func=None,\n               show_contracted=False, link_color_func=None, ax=None,\n               above_threshold_color='C0'):\n    \"\"\"\n    Plot the hierarchical clustering as a dendrogram.\n    The dendrogram illustrates how each cluster is\n    composed by drawing a U-shaped link between a non-singleton\n    cluster and its children. The top of the U-link indicates a\n    cluster merge. The two legs of the U-link indicate which clusters\n    were merged. The length of the two legs of the U-link represents\n    the distance between the child clusters. It is also the\n    cophenetic distance between original observations in the two\n    children clusters. Customized to track cluster index traversal in\n    construction of dendrogram for labeling purposes.\n\n    Parameters\n    ----------\n    Z : ndarray\n        The linkage matrix encoding the hierarchical clustering to\n        render as a dendrogram. See the ``linkage`` function for more\n        information on the format of ``Z``.\n    p : int, optional\n        The ``p`` parameter for ``truncate_mode``.\n    truncate_mode : str, optional\n        The dendrogram can be hard to read when the original\n        observation matrix from which the linkage is derived is\n        large. Truncation is used to condense the dendrogram. There\n        are several modes:\n        ``None``\n          No truncation is performed (default).\n          Note: ``'none'`` is an alias for ``None`` that's kept for\n          backward compatibility.\n        ``'lastp'``\n          The last ``p`` non-singleton clusters formed in the linkage are the\n          only non-leaf nodes in the linkage; they correspond to rows\n          ``Z[n-p-2:end]`` in ``Z``. All other non-singleton clusters are\n          contracted into leaf nodes.\n        ``'level'``\n          No more than ``p`` levels of the dendrogram tree are displayed.\n          A \"level\" includes all nodes with ``p`` merges from the last merge.\n          Note: ``'mtica'`` is an alias for ``'level'`` that's kept for\n          backward compatibility.\n    color_threshold : double, optional\n        For brevity, let :math:`t` be the ``color_threshold``.\n        Colors all the descendent links below a cluster node\n        :math:`k` the same color if :math:`k` is the first node below\n        the cut threshold :math:`t`. All links connecting nodes with\n        distances greater than or equal to the threshold are colored\n        with de default matplotlib color ``'C0'``. If :math:`t` is less\n        than or equal to zero, all nodes are colored ``'C0'``.\n        If ``color_threshold`` is None or 'default',\n        corresponding with MATLAB(TM) behavior, the threshold is set to\n        ``0.7*max(Z[:,2])``.\n    get_leaves : bool, optional\n        Includes a list ``R['leaves']=H`` in the result\n        dictionary. For each :math:`i`, ``H[i] == j``, cluster node\n        ``j`` appears in position ``i`` in the left-to-right traversal\n        of the leaves, where :math:`j &lt; 2n-1` and :math:`i &lt; n`.\n    orientation : str, optional\n        The direction to plot the dendrogram, which can be any\n        of the following strings:\n        ``'top'``\n          Plots the root at the top, and plot descendent links going downwards.\n          (default).\n        ``'bottom'``\n          Plots the root at the bottom, and plot descendent links going\n          upwards.\n        ``'left'``\n          Plots the root at the left, and plot descendent links going right.\n        ``'right'``\n          Plots the root at the right, and plot descendent links going left.\n    labels : ndarray, optional\n        By default, ``labels`` is None so the index of the original observation\n        is used to label the leaf nodes.  Otherwise, this is an :math:`n`-sized\n        sequence, with ``n == Z.shape[0] + 1``. The ``labels[i]`` value is the\n        text to put under the :math:`i` th leaf node only if it corresponds to\n        an original observation and not a non-singleton cluster.\n    count_sort : str or bool, optional\n        For each node n, the order (visually, from left-to-right) n's\n        two descendent links are plotted is determined by this\n        parameter, which can be any of the following values:\n        ``False``\n          Nothing is done.\n        ``'ascending'`` or ``True``\n          The child with the minimum number of original objects in its cluster\n          is plotted first.\n        ``'descending'``\n          The child with the maximum number of original objects in its cluster\n          is plotted first.\n        Note, ``distance_sort`` and ``count_sort`` cannot both be True.\n    distance_sort : str or bool, optional\n        For each node n, the order (visually, from left-to-right) n's\n        two descendent links are plotted is determined by this\n        parameter, which can be any of the following values:\n        ``False``\n          Nothing is done.\n        ``'ascending'`` or ``True``\n          The child with the minimum distance between its direct descendents is\n          plotted first.\n        ``'descending'``\n          The child with the maximum distance between its direct descendents is\n          plotted first.\n        Note ``distance_sort`` and ``count_sort`` cannot both be True.\n    show_leaf_counts : bool, optional\n         When True, leaf nodes representing :math:`k&gt;1` original\n         observation are labeled with the number of observations they\n         contain in parentheses.\n    no_plot : bool, optional\n        When True, the final rendering is not performed. This is\n        useful if only the data structures computed for the rendering\n        are needed or if matplotlib is not available.\n    no_labels : bool, optional\n        When True, no labels appear next to the leaf nodes in the\n        rendering of the dendrogram.\n    leaf_rotation : double, optional\n        Specifies the angle (in degrees) to rotate the leaf\n        labels. When unspecified, the rotation is based on the number of\n        nodes in the dendrogram (default is 0).\n    leaf_font_size : int, optional\n        Specifies the font size (in points) of the leaf labels. When\n        unspecified, the size based on the number of nodes in the\n        dendrogram.\n    leaf_label_func : lambda or function, optional\n        When leaf_label_func is a callable function, for each\n        leaf with cluster index :math:`k &lt; 2n-1`. The function\n        is expected to return a string with the label for the\n        leaf.\n        Indices :math:`k &lt; n` correspond to original observations\n        while indices :math:`k \\\\geq n` correspond to non-singleton\n        clusters.\n        For example, to label singletons with their node id and\n        non-singletons with their id, count, and inconsistency\n        coefficient, simply do::\n            # First define the leaf label function.\n            def llf(id):\n                if id &lt; n:\n                    return str(id)\n                else:\n                    return '[%d %d %1.2f]' % (id, count, R[n-id,3])\n            # The text for the leaf nodes is going to be big so force\n            # a rotation of 90 degrees.\n            dendrogram(Z, leaf_label_func=llf, leaf_rotation=90)\n    show_contracted : bool, optional\n        When True the heights of non-singleton nodes contracted\n        into a leaf node are plotted as crosses along the link\n        connecting that leaf node.  This really is only useful when\n        truncation is used (see ``truncate_mode`` parameter).\n    link_color_func : callable, optional\n        If given, `link_color_function` is called with each non-singleton id\n        corresponding to each U-shaped link it will paint. The function is\n        expected to return the color to paint the link, encoded as a matplotlib\n        color string code. For example::\n            dendrogram(Z, link_color_func=lambda k: colors[k])\n        colors the direct links below each untruncated non-singleton node\n        ``k`` using ``colors[k]``.\n    ax : matplotlib Axes instance, optional\n        If None and `no_plot` is not True, the dendrogram will be plotted\n        on the current axes.  Otherwise if `no_plot` is not True the\n        dendrogram will be plotted on the given ``Axes`` instance. This can be\n        useful if the dendrogram is part of a more complex figure.\n    above_threshold_color : str, optional\n        This matplotlib color string sets the color of the links above the\n        color_threshold. The default is ``'C0'``.\n    Returns\n    -------\n    R : dict\n        A dictionary of data structures computed to render the\n        dendrogram. Its has the following keys:\n        ``'color_list'``\n          A list of color names. The k'th element represents the color of the\n          k'th link.\n        ``'icoord'`` and ``'dcoord'``\n          Each of them is a list of lists. Let ``icoord = [I1, I2, ..., Ip]``\n          where ``Ik = [xk1, xk2, xk3, xk4]`` and ``dcoord = [D1, D2, ..., Dp]``\n          where ``Dk = [yk1, yk2, yk3, yk4]``, then the k'th link painted is\n          ``(xk1, yk1)`` - ``(xk2, yk2)`` - ``(xk3, yk3)`` - ``(xk4, yk4)``.\n        ``'ivl'``\n          A list of labels corresponding to the leaf nodes.\n        ``'leaves'``\n          For each i, ``H[i] == j``, cluster node ``j`` appears in position\n          ``i`` in the left-to-right traversal of the leaves, where\n          :math:`j &lt; 2n-1` and :math:`i &lt; n`. If ``j`` is less than ``n``, the\n          ``i``-th leaf node corresponds to an original observation.\n          Otherwise, it corresponds to a non-singleton cluster.\n        ``'leaves_color_list'``\n          A list of color names. The k'th element represents the color of the\n          k'th leaf.\n    See Also\n    --------\n    linkage, set_link_color_palette\n    Notes\n    -----\n    It is expected that the distances in ``Z[:,2]`` be monotonic, otherwise\n    crossings appear in the dendrogram.\n    Examples\n    --------\n    &gt;&gt;&gt; from scipy.cluster import hierarchy\n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n    A very basic example:\n    &gt;&gt;&gt; ytdist = np.array([662., 877., 255., 412., 996., 295., 468., 268.,\n    ...                    400., 754., 564., 138., 219., 869., 669.])\n    &gt;&gt;&gt; Z = hierarchy.linkage(ytdist, 'single')\n    &gt;&gt;&gt; plt.figure()\n    &gt;&gt;&gt; dn = hierarchy.dendrogram(Z)\n    Now, plot in given axes, improve the color scheme and use both vertical and\n    horizontal orientations:\n    &gt;&gt;&gt; hierarchy.set_link_color_palette(['m', 'c', 'y', 'k'])\n    &gt;&gt;&gt; fig, axes = plt.subplots(1, 2, figsize=(8, 3))\n    &gt;&gt;&gt; dn1 = hierarchy.dendrogram(Z, ax=axes[0], above_threshold_color='y',\n    ...                            orientation='top')\n    &gt;&gt;&gt; dn2 = hierarchy.dendrogram(Z, ax=axes[1],\n    ...                            above_threshold_color='#bcbddc',\n    ...                            orientation='right')\n    &gt;&gt;&gt; hierarchy.set_link_color_palette(None)  # reset to default after use\n    &gt;&gt;&gt; plt.show()\n    \"\"\"\n    # This feature was thought about but never implemented (still useful?):\n    #\n    #         ... = dendrogram(..., leaves_order=None)\n    #\n    #         Plots the leaves in the order specified by a vector of\n    #         original observation indices. If the vector contains duplicates\n    #         or results in a crossing, an exception will be thrown. Passing\n    #         None orders leaf nodes based on the order they appear in the\n    #         pre-order traversal.\n    Z = np.asarray(Z, order='c')\n\n    if orientation not in [\"top\", \"left\", \"bottom\", \"right\"]:\n        raise ValueError(\"orientation must be one of 'top', 'left', \"\n                         \"'bottom', or 'right'\")\n\n    if labels is not None and Z.shape[0] + 1 != len(labels):\n        raise ValueError(\"Dimensions of Z and labels must be consistent.\")\n\n    is_valid_linkage(Z, throw=True, name='Z')\n    Zs = Z.shape\n    n = Zs[0] + 1\n    if type(p) in (int, float):\n        p = int(p)\n    else:\n        raise TypeError('The second argument must be a number')\n\n    if truncate_mode not in ('lastp', 'mlab', 'mtica', 'level', 'none', None):\n        # 'mlab' and 'mtica' are kept working for backwards compat.\n        raise ValueError('Invalid truncation mode.')\n\n    if truncate_mode == 'lastp' or truncate_mode == 'mlab':\n        if p &gt; n or p == 0:\n            p = n\n\n    if truncate_mode == 'mtica':\n        # 'mtica' is an alias\n        truncate_mode = 'level'\n\n    if truncate_mode == 'level':\n        if p &lt;= 0:\n            p = np.inf\n\n    if get_leaves:\n        lvs = []\n    else:\n        lvs = None\n\n    icoord_list = []\n    dcoord_list = []\n    color_list = []\n    current_color = [0]\n    currently_below_threshold = [False]\n    ivl = []  # list of leaves\n\n    if color_threshold is None or (isinstance(color_threshold, str) and\n                                   color_threshold == 'default'):\n        color_threshold = max(Z[:, 2]) * 0.7\n\n    R = {'icoord': icoord_list, 'dcoord': dcoord_list, 'ivl': ivl,\n         'leaves': lvs, 'color_list': color_list}\n\n    # Empty list will be filled in _dendrogram_calculate_info\n    contraction_marks = [] if show_contracted else None\n\n    traversal = []\n    _dendrogram_calculate_info(\n        Z=Z, p=p,\n        truncate_mode=truncate_mode,\n        color_threshold=color_threshold,\n        get_leaves=get_leaves,\n        orientation=orientation,\n        labels=labels,\n        count_sort=count_sort,\n        distance_sort=distance_sort,\n        show_leaf_counts=show_leaf_counts,\n        i=2*n - 2,\n        iv=0.0,\n        ivl=ivl,\n        n=n,\n        icoord_list=icoord_list,\n        dcoord_list=dcoord_list,\n        lvs=lvs,\n        current_color=current_color,\n        color_list=color_list,\n        currently_below_threshold=currently_below_threshold,\n        leaf_label_func=leaf_label_func,\n        contraction_marks=contraction_marks,\n        link_color_func=link_color_func,\n        above_threshold_color=above_threshold_color,\n        traversal=traversal)\n\n    if not no_plot:\n        mh = max(Z[:, 2])\n        _plot_dendrogram(icoord_list, dcoord_list, ivl, p, n, mh, orientation,\n                         no_labels, color_list,\n                         leaf_font_size=leaf_font_size,\n                         leaf_rotation=leaf_rotation,\n                         contraction_marks=contraction_marks,\n                         ax=ax,\n                         above_threshold_color=above_threshold_color)\n\n    R[\"leaves_color_list\"] = _get_leaves_color_list(R)\n    R['traversal'] = traversal\n    return R\n\n\ndef _get_leaves_color_list(R):\n    leaves_color_list = [None] * len(R['leaves'])\n    for link_x, link_y, link_color in zip(R['icoord'],\n                                          R['dcoord'],\n                                          R['color_list']):\n        for (xi, yi) in zip(link_x, link_y):\n            if yi == 0.0:  # if yi is 0.0, the point is a leaf\n                # xi of leaves are      5, 15, 25, 35, ... (see `iv_ticks`)\n                # index of leaves are   0,  1,  2,  3, ... as below\n                leaf_index = (int(xi) - 5) // 10\n                # each leaf has a same color of its link.\n                leaves_color_list[leaf_index] = link_color\n    return leaves_color_list\n\n\ndef _append_singleton_leaf_node(Z, p, n, level, lvs, ivl, leaf_label_func,\n                                i, labels):\n    # If the leaf id structure is not None and is a list then the caller\n    # to dendrogram has indicated that cluster id's corresponding to the\n    # leaf nodes should be recorded.\n\n    if lvs is not None:\n        lvs.append(int(i))\n\n    # If leaf node labels are to be displayed...\n    if ivl is not None:\n        # If a leaf_label_func has been provided, the label comes from the\n        # string returned from the leaf_label_func, which is a function\n        # passed to dendrogram.\n        if leaf_label_func:\n            ivl.append(leaf_label_func(int(i)))\n        else:\n            # Otherwise, if the dendrogram caller has passed a labels list\n            # for the leaf nodes, use it.\n            if labels is not None:\n                ivl.append(labels[int(i - n)])\n            else:\n                # Otherwise, use the id as the label for the leaf.x\n                ivl.append(str(int(i)))\n\n\ndef _append_nonsingleton_leaf_node(Z, p, n, level, lvs, ivl, leaf_label_func,\n                                   i, labels, show_leaf_counts):\n    # If the leaf id structure is not None and is a list then the caller\n    # to dendrogram has indicated that cluster id's corresponding to the\n    # leaf nodes should be recorded.\n\n    if lvs is not None:\n        lvs.append(int(i))\n    if ivl is not None:\n        if leaf_label_func:\n            ivl.append(leaf_label_func(int(i)))\n        else:\n            if show_leaf_counts:\n                ivl.append(\"(\" + str(int(Z[i - n, 3])) + \")\")\n            else:\n                ivl.append(\"\")\n\n\ndef _append_contraction_marks(Z, iv, i, n, contraction_marks):\n    _append_contraction_marks_sub(Z, iv, int(Z[i - n, 0]), n, contraction_marks)\n    _append_contraction_marks_sub(Z, iv, int(Z[i - n, 1]), n, contraction_marks)\n\n\ndef _append_contraction_marks_sub(Z, iv, i, n, contraction_marks):\n    if i &gt;= n:\n        contraction_marks.append((iv, Z[i - n, 2]))\n        _append_contraction_marks_sub(Z, iv, int(Z[i - n, 0]), n, contraction_marks)\n        _append_contraction_marks_sub(Z, iv, int(Z[i - n, 1]), n, contraction_marks)\n\n\ndef _dendrogram_calculate_info(Z, p, truncate_mode,\n                               color_threshold=np.inf, get_leaves=True,\n                               orientation='top', labels=None,\n                               count_sort=False, distance_sort=False,\n                               show_leaf_counts=False, i=-1, iv=0.0,\n                               ivl=[], n=0, icoord_list=[], dcoord_list=[],\n                               lvs=None, mhr=False,\n                               current_color=[], color_list=[],\n                               currently_below_threshold=[],\n                               leaf_label_func=None, level=0,\n                               contraction_marks=None,\n                               link_color_func=None,\n                               above_threshold_color='C0',\n                               traversal=[]):\n    \"\"\"\n    Calculate the endpoints of the links as well as the labels for the\n    the dendrogram rooted at the node with index i. iv is the independent\n    variable value to plot the left-most leaf node below the root node i\n    (if orientation='top', this would be the left-most x value where the\n    plotting of this root node i and its descendents should begin).\n    ivl is a list to store the labels of the leaf nodes. The leaf_label_func\n    is called whenever ivl != None, labels == None, and\n    leaf_label_func != None. When ivl != None and labels != None, the\n    labels list is used only for labeling the leaf nodes. When\n    ivl == None, no labels are generated for leaf nodes.\n    When get_leaves==True, a list of leaves is built as they are visited\n    in the dendrogram.\n    Returns a tuple with l being the independent variable coordinate that\n    corresponds to the midpoint of cluster to the left of cluster i if\n    i is non-singleton, otherwise the independent coordinate of the leaf\n    node if i is a leaf node.\n    Returns\n    -------\n    A tuple (left, w, h, md), where:\n      * left is the independent variable coordinate of the center of the\n        the U of the subtree\n      * w is the amount of space used for the subtree (in independent\n        variable units)\n      * h is the height of the subtree in dependent variable units\n      * md is the ``max(Z[*,2]``) for all nodes ``*`` below and including\n        the target node.\n    \"\"\"\n    traversal.append(i)\n    \n    if n == 0:\n        raise ValueError(\"Invalid singleton cluster count n.\")\n\n    if i == -1:\n        raise ValueError(\"Invalid root cluster index i.\")\n\n    if truncate_mode == 'lastp':\n        # If the node is a leaf node but corresponds to a non-singleton\n        # cluster, its label is either the empty string or the number of\n        # original observations belonging to cluster i.\n        if 2*n - p &gt; i &gt;= n:\n            d = Z[i - n, 2]\n            _append_nonsingleton_leaf_node(Z, p, n, level, lvs, ivl,\n                                           leaf_label_func, i, labels,\n                                           show_leaf_counts)\n            if contraction_marks is not None:\n                _append_contraction_marks(Z, iv + 5.0, i, n, contraction_marks)\n            return (iv + 5.0, 10.0, 0.0, d)\n        elif i &lt; n:\n            _append_singleton_leaf_node(Z, p, n, level, lvs, ivl,\n                                        leaf_label_func, i, labels)\n            return (iv + 5.0, 10.0, 0.0, 0.0)\n    elif truncate_mode == 'level':\n        if i &gt; n and level &gt; p:\n            d = Z[i - n, 2]\n            _append_nonsingleton_leaf_node(Z, p, n, level, lvs, ivl,\n                                           leaf_label_func, i, labels,\n                                           show_leaf_counts)\n            if contraction_marks is not None:\n                _append_contraction_marks(Z, iv + 5.0, i, n, contraction_marks)\n            return (iv + 5.0, 10.0, 0.0, d)\n        elif i &lt; n:\n            _append_singleton_leaf_node(Z, p, n, level, lvs, ivl,\n                                        leaf_label_func, i, labels)\n            return (iv + 5.0, 10.0, 0.0, 0.0)\n    elif truncate_mode in ('mlab',):\n        msg = \"Mode 'mlab' is deprecated in scipy 0.19.0 (it never worked).\"\n        warnings.warn(msg, DeprecationWarning)\n\n    # Otherwise, only truncate if we have a leaf node.\n    #\n    # Only place leaves if they correspond to original observations.\n    if i &lt; n:\n        _append_singleton_leaf_node(Z, p, n, level, lvs, ivl,\n                                    leaf_label_func, i, labels)\n        return (iv + 5.0, 10.0, 0.0, 0.0)\n\n    # !!! Otherwise, we don't have a leaf node, so work on plotting a\n    # non-leaf node.\n    # Actual indices of a and b\n    aa = int(Z[i - n, 0])\n    ab = int(Z[i - n, 1])\n    if aa &gt;= n:\n        # The number of singletons below cluster a\n        na = Z[aa - n, 3]\n        # The distance between a's two direct children.\n        da = Z[aa - n, 2]\n    else:\n        na = 1\n        da = 0.0\n    if ab &gt;= n:\n        nb = Z[ab - n, 3]\n        db = Z[ab - n, 2]\n    else:\n        nb = 1\n        db = 0.0\n\n    if count_sort == 'ascending' or count_sort == True:\n        # If a has a count greater than b, it and its descendents should\n        # be drawn to the right. Otherwise, to the left.\n        if na &gt; nb:\n            # The cluster index to draw to the left (ua) will be ab\n            # and the one to draw to the right (ub) will be aa\n            ua = ab\n            ub = aa\n        else:\n            ua = aa\n            ub = ab\n    elif count_sort == 'descending':\n        # If a has a count less than or equal to b, it and its\n        # descendents should be drawn to the left. Otherwise, to\n        # the right.\n        if na &gt; nb:\n            ua = aa\n            ub = ab\n        else:\n            ua = ab\n            ub = aa\n    elif distance_sort == 'ascending' or distance_sort == True:\n        # If a has a distance greater than b, it and its descendents should\n        # be drawn to the right. Otherwise, to the left.\n        if da &gt; db:\n            ua = ab\n            ub = aa\n        else:\n            ua = aa\n            ub = ab\n    elif distance_sort == 'descending':\n        # If a has a distance less than or equal to b, it and its\n        # descendents should be drawn to the left. Otherwise, to\n        # the right.\n        if da &gt; db:\n            ua = aa\n            ub = ab\n        else:\n            ua = ab\n            ub = aa\n    else:\n        ua = aa\n        ub = ab\n\n    # Updated iv variable and the amount of space used.\n    (uiva, uwa, uah, uamd) = \\\n        _dendrogram_calculate_info(\n            Z=Z, p=p,\n            truncate_mode=truncate_mode,\n            color_threshold=color_threshold,\n            get_leaves=get_leaves,\n            orientation=orientation,\n            labels=labels,\n            count_sort=count_sort,\n            distance_sort=distance_sort,\n            show_leaf_counts=show_leaf_counts,\n            i=ua, iv=iv, ivl=ivl, n=n,\n            icoord_list=icoord_list,\n            dcoord_list=dcoord_list, lvs=lvs,\n            current_color=current_color,\n            color_list=color_list,\n            currently_below_threshold=currently_below_threshold,\n            leaf_label_func=leaf_label_func,\n            level=level + 1, contraction_marks=contraction_marks,\n            link_color_func=link_color_func,\n            above_threshold_color=above_threshold_color,\n            traversal=traversal)\n\n    h = Z[i - n, 2]\n    if h &gt;= color_threshold or color_threshold &lt;= 0:\n        c = above_threshold_color\n\n        if currently_below_threshold[0]:\n            current_color[0] = (current_color[0] + 1) % len(_link_line_colors)\n        currently_below_threshold[0] = False\n    else:\n        currently_below_threshold[0] = True\n        c = _link_line_colors[current_color[0]]\n\n    (uivb, uwb, ubh, ubmd) = \\\n        _dendrogram_calculate_info(\n            Z=Z, p=p,\n            truncate_mode=truncate_mode,\n            color_threshold=color_threshold,\n            get_leaves=get_leaves,\n            orientation=orientation,\n            labels=labels,\n            count_sort=count_sort,\n            distance_sort=distance_sort,\n            show_leaf_counts=show_leaf_counts,\n            i=ub, iv=iv + uwa, ivl=ivl, n=n,\n            icoord_list=icoord_list,\n            dcoord_list=dcoord_list, lvs=lvs,\n            current_color=current_color,\n            color_list=color_list,\n            currently_below_threshold=currently_below_threshold,\n            leaf_label_func=leaf_label_func,\n            level=level + 1, contraction_marks=contraction_marks,\n            link_color_func=link_color_func,\n            above_threshold_color=above_threshold_color,\n            traversal=traversal)\n\n    max_dist = max(uamd, ubmd, h)\n\n    icoord_list.append([uiva, uiva, uivb, uivb])\n    dcoord_list.append([uah, h, h, ubh])\n    if link_color_func is not None:\n        v = link_color_func(int(i))\n        if not isinstance(v, str):\n            raise TypeError(\"link_color_func must return a matplotlib \"\n                            \"color string!\")\n        color_list.append(v)\n    else:\n        color_list.append(c)\n\n    return (((uiva + uivb) / 2), uwa + uwb, h, max_dist)\n\n\ndef is_isomorphic(T1, T2):\n    \"\"\"\n    Determine if two different cluster assignments are equivalent.\n    Parameters\n    ----------\n    T1 : array_like\n        An assignment of singleton cluster ids to flat cluster ids.\n    T2 : array_like\n        An assignment of singleton cluster ids to flat cluster ids.\n    Returns\n    -------\n    b : bool\n        Whether the flat cluster assignments `T1` and `T2` are\n        equivalent.\n    See Also\n    --------\n    linkage: for a description of what a linkage matrix is.\n    fcluster: for the creation of flat cluster assignments.\n    Examples\n    --------\n    &gt;&gt;&gt; from scipy.cluster.hierarchy import fcluster, is_isomorphic\n    &gt;&gt;&gt; from scipy.cluster.hierarchy import single, complete\n    &gt;&gt;&gt; from scipy.spatial.distance import pdist\n    Two flat cluster assignments can be isomorphic if they represent the same\n    cluster assignment, with different labels.\n    For example, we can use the `scipy.cluster.hierarchy.single`: method\n    and flatten the output to four clusters:\n    &gt;&gt;&gt; X = [[0, 0], [0, 1], [1, 0],\n    ...      [0, 4], [0, 3], [1, 4],\n    ...      [4, 0], [3, 0], [4, 1],\n    ...      [4, 4], [3, 4], [4, 3]]\n    &gt;&gt;&gt; Z = single(pdist(X))\n    &gt;&gt;&gt; T = fcluster(Z, 1, criterion='distance')\n    &gt;&gt;&gt; T\n    array([3, 3, 3, 4, 4, 4, 2, 2, 2, 1, 1, 1], dtype=int32)\n    We can then do the same using the\n    `scipy.cluster.hierarchy.complete`: method:\n    &gt;&gt;&gt; Z = complete(pdist(X))\n    &gt;&gt;&gt; T_ = fcluster(Z, 1.5, criterion='distance')\n    &gt;&gt;&gt; T_\n    array([1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4], dtype=int32)\n    As we can see, in both cases we obtain four clusters and all the data\n    points are distributed in the same way - the only thing that changes\n    are the flat cluster labels (3 =&gt; 1, 4 =&gt;2, 2 =&gt;3 and 4 =&gt;1), so both\n    cluster assignments are isomorphic:\n    &gt;&gt;&gt; is_isomorphic(T, T_)\n    True\n    \"\"\"\n    T1 = np.asarray(T1, order='c')\n    T2 = np.asarray(T2, order='c')\n\n    if type(T1) != np.ndarray:\n        raise TypeError('T1 must be a numpy array.')\n    if type(T2) != np.ndarray:\n        raise TypeError('T2 must be a numpy array.')\n\n    T1S = T1.shape\n    T2S = T2.shape\n\n    if len(T1S) != 1:\n        raise ValueError('T1 must be one-dimensional.')\n    if len(T2S) != 1:\n        raise ValueError('T2 must be one-dimensional.')\n    if T1S[0] != T2S[0]:\n        raise ValueError('T1 and T2 must have the same number of elements.')\n    n = T1S[0]\n    d1 = {}\n    d2 = {}\n    for i in range(0, n):\n        if T1[i] in d1:\n            if not T2[i] in d2:\n                return False\n            if d1[T1[i]] != T2[i] or d2[T2[i]] != T1[i]:\n                return False\n        elif T2[i] in d2:\n            return False\n        else:\n            d1[T1[i]] = T2[i]\n            d2[T2[i]] = T1[i]\n    return True\n\n\ndef maxdists(Z):\n    \"\"\"\n    Return the maximum distance between any non-singleton cluster.\n    Parameters\n    ----------\n    Z : ndarray\n        The hierarchical clustering encoded as a matrix. See\n        ``linkage`` for more information.\n    Returns\n    -------\n    maxdists : ndarray\n        A ``(n-1)`` sized numpy array of doubles; ``MD[i]`` represents\n        the maximum distance between any cluster (including\n        singletons) below and including the node with index i. More\n        specifically, ``MD[i] = Z[Q(i)-n, 2].max()`` where ``Q(i)`` is the\n        set of all node indices below and including node i.\n    See Also\n    --------\n    linkage: for a description of what a linkage matrix is.\n    is_monotonic: for testing for monotonicity of a linkage matrix.\n    Examples\n    --------\n    &gt;&gt;&gt; from scipy.cluster.hierarchy import median, maxdists\n    &gt;&gt;&gt; from scipy.spatial.distance import pdist\n    Given a linkage matrix ``Z``, `scipy.cluster.hierarchy.maxdists`\n    computes for each new cluster generated (i.e., for each row of the linkage\n    matrix) what is the maximum distance between any two child clusters.\n    Due to the nature of hierarchical clustering, in many cases this is going\n    to be just the distance between the two child clusters that were merged\n    to form the current one - that is, Z[:,2].\n    However, for non-monotonic cluster assignments such as\n    `scipy.cluster.hierarchy.median` clustering this is not always the\n    case: There may be cluster formations were the distance between the two\n    clusters merged is smaller than the distance between their children.\n    We can see this in an example:\n    &gt;&gt;&gt; X = [[0, 0], [0, 1], [1, 0],\n    ...      [0, 4], [0, 3], [1, 4],\n    ...      [4, 0], [3, 0], [4, 1],\n    ...      [4, 4], [3, 4], [4, 3]]\n    &gt;&gt;&gt; Z = median(pdist(X))\n    &gt;&gt;&gt; Z\n    array([[ 0.        ,  1.        ,  1.        ,  2.        ],\n           [ 3.        ,  4.        ,  1.        ,  2.        ],\n           [ 9.        , 10.        ,  1.        ,  2.        ],\n           [ 6.        ,  7.        ,  1.        ,  2.        ],\n           [ 2.        , 12.        ,  1.11803399,  3.        ],\n           [ 5.        , 13.        ,  1.11803399,  3.        ],\n           [ 8.        , 15.        ,  1.11803399,  3.        ],\n           [11.        , 14.        ,  1.11803399,  3.        ],\n           [18.        , 19.        ,  3.        ,  6.        ],\n           [16.        , 17.        ,  3.5       ,  6.        ],\n           [20.        , 21.        ,  3.25      , 12.        ]])\n    &gt;&gt;&gt; maxdists(Z)\n    array([1.        , 1.        , 1.        , 1.        , 1.11803399,\n           1.11803399, 1.11803399, 1.11803399, 3.        , 3.5       ,\n           3.5       ])\n    Note that while the distance between the two clusters merged when creating the\n    last cluster is 3.25, there are two children (clusters 16 and 17) whose distance\n    is larger (3.5). Thus, `scipy.cluster.hierarchy.maxdists` returns 3.5 in\n    this case.\n    \"\"\"\n    Z = np.asarray(Z, order='c', dtype=np.double)\n    is_valid_linkage(Z, throw=True, name='Z')\n\n    n = Z.shape[0] + 1\n    MD = np.zeros((n - 1,))\n    [Z] = _copy_arrays_if_base_present([Z])\n    _hierarchy.get_max_dist_for_each_cluster(Z, MD, int(n))\n    return MD\n\n\ndef maxinconsts(Z, R):\n    \"\"\"\n    Return the maximum inconsistency coefficient for each\n    non-singleton cluster and its children.\n    Parameters\n    ----------\n    Z : ndarray\n        The hierarchical clustering encoded as a matrix. See\n        `linkage` for more information.\n    R : ndarray\n        The inconsistency matrix.\n    Returns\n    -------\n    MI : ndarray\n        A monotonic ``(n-1)``-sized numpy array of doubles.\n    See Also\n    --------\n    linkage: for a description of what a linkage matrix is.\n    inconsistent: for the creation of a inconsistency matrix.\n    Examples\n    --------\n    &gt;&gt;&gt; from scipy.cluster.hierarchy import median, inconsistent, maxinconsts\n    &gt;&gt;&gt; from scipy.spatial.distance import pdist\n    Given a data set ``X``, we can apply a clustering method to obtain a\n    linkage matrix ``Z``. `scipy.cluster.hierarchy.inconsistent` can\n    be also used to obtain the inconsistency matrix ``R`` associated to\n    this clustering process:\n    &gt;&gt;&gt; X = [[0, 0], [0, 1], [1, 0],\n    ...      [0, 4], [0, 3], [1, 4],\n    ...      [4, 0], [3, 0], [4, 1],\n    ...      [4, 4], [3, 4], [4, 3]]\n    &gt;&gt;&gt; Z = median(pdist(X))\n    &gt;&gt;&gt; R = inconsistent(Z)\n    &gt;&gt;&gt; Z\n    array([[ 0.        ,  1.        ,  1.        ,  2.        ],\n           [ 3.        ,  4.        ,  1.        ,  2.        ],\n           [ 9.        , 10.        ,  1.        ,  2.        ],\n           [ 6.        ,  7.        ,  1.        ,  2.        ],\n           [ 2.        , 12.        ,  1.11803399,  3.        ],\n           [ 5.        , 13.        ,  1.11803399,  3.        ],\n           [ 8.        , 15.        ,  1.11803399,  3.        ],\n           [11.        , 14.        ,  1.11803399,  3.        ],\n           [18.        , 19.        ,  3.        ,  6.        ],\n           [16.        , 17.        ,  3.5       ,  6.        ],\n           [20.        , 21.        ,  3.25      , 12.        ]])\n    &gt;&gt;&gt; R\n    array([[1.        , 0.        , 1.        , 0.        ],\n           [1.        , 0.        , 1.        , 0.        ],\n           [1.        , 0.        , 1.        , 0.        ],\n           [1.        , 0.        , 1.        , 0.        ],\n           [1.05901699, 0.08346263, 2.        , 0.70710678],\n           [1.05901699, 0.08346263, 2.        , 0.70710678],\n           [1.05901699, 0.08346263, 2.        , 0.70710678],\n           [1.05901699, 0.08346263, 2.        , 0.70710678],\n           [1.74535599, 1.08655358, 3.        , 1.15470054],\n           [1.91202266, 1.37522872, 3.        , 1.15470054],\n           [3.25      , 0.25      , 3.        , 0.        ]])\n    Here, `scipy.cluster.hierarchy.maxinconsts` can be used to compute\n    the maximum value of the inconsistency statistic (the last column of\n    ``R``) for each non-singleton cluster and its children:\n    &gt;&gt;&gt; maxinconsts(Z, R)\n    array([0.        , 0.        , 0.        , 0.        , 0.70710678,\n           0.70710678, 0.70710678, 0.70710678, 1.15470054, 1.15470054,\n           1.15470054])\n    \"\"\"\n    Z = np.asarray(Z, order='c')\n    R = np.asarray(R, order='c')\n    is_valid_linkage(Z, throw=True, name='Z')\n    is_valid_im(R, throw=True, name='R')\n\n    n = Z.shape[0] + 1\n    if Z.shape[0] != R.shape[0]:\n        raise ValueError(\"The inconsistency matrix and linkage matrix each \"\n                         \"have a different number of rows.\")\n    MI = np.zeros((n - 1,))\n    [Z, R] = _copy_arrays_if_base_present([Z, R])\n    _hierarchy.get_max_Rfield_for_each_cluster(Z, R, MI, int(n), 3)\n    return MI\n\n\ndef maxRstat(Z, R, i):\n    \"\"\"\n    Return the maximum statistic for each non-singleton cluster and its\n    children.\n    Parameters\n    ----------\n    Z : array_like\n        The hierarchical clustering encoded as a matrix. See `linkage` for more\n        information.\n    R : array_like\n        The inconsistency matrix.\n    i : int\n        The column of `R` to use as the statistic.\n    Returns\n    -------\n    MR : ndarray\n        Calculates the maximum statistic for the i'th column of the\n        inconsistency matrix `R` for each non-singleton cluster\n        node. ``MR[j]`` is the maximum over ``R[Q(j)-n, i]``, where\n        ``Q(j)`` the set of all node ids corresponding to nodes below\n        and including ``j``.\n    See Also\n    --------\n    linkage: for a description of what a linkage matrix is.\n    inconsistent: for the creation of a inconsistency matrix.\n    Examples\n    --------\n    &gt;&gt;&gt; from scipy.cluster.hierarchy import median, inconsistent, maxRstat\n    &gt;&gt;&gt; from scipy.spatial.distance import pdist\n    Given a data set ``X``, we can apply a clustering method to obtain a\n    linkage matrix ``Z``. `scipy.cluster.hierarchy.inconsistent` can\n    be also used to obtain the inconsistency matrix ``R`` associated to\n    this clustering process:\n    &gt;&gt;&gt; X = [[0, 0], [0, 1], [1, 0],\n    ...      [0, 4], [0, 3], [1, 4],\n    ...      [4, 0], [3, 0], [4, 1],\n    ...      [4, 4], [3, 4], [4, 3]]\n    &gt;&gt;&gt; Z = median(pdist(X))\n    &gt;&gt;&gt; R = inconsistent(Z)\n    &gt;&gt;&gt; R\n    array([[1.        , 0.        , 1.        , 0.        ],\n           [1.        , 0.        , 1.        , 0.        ],\n           [1.        , 0.        , 1.        , 0.        ],\n           [1.        , 0.        , 1.        , 0.        ],\n           [1.05901699, 0.08346263, 2.        , 0.70710678],\n           [1.05901699, 0.08346263, 2.        , 0.70710678],\n           [1.05901699, 0.08346263, 2.        , 0.70710678],\n           [1.05901699, 0.08346263, 2.        , 0.70710678],\n           [1.74535599, 1.08655358, 3.        , 1.15470054],\n           [1.91202266, 1.37522872, 3.        , 1.15470054],\n           [3.25      , 0.25      , 3.        , 0.        ]])\n    `scipy.cluster.hierarchy.maxRstat` can be used to compute\n    the maximum value of each column of ``R``, for each non-singleton\n    cluster and its children:\n    &gt;&gt;&gt; maxRstat(Z, R, 0)\n    array([1.        , 1.        , 1.        , 1.        , 1.05901699,\n           1.05901699, 1.05901699, 1.05901699, 1.74535599, 1.91202266,\n           3.25      ])\n    &gt;&gt;&gt; maxRstat(Z, R, 1)\n    array([0.        , 0.        , 0.        , 0.        , 0.08346263,\n           0.08346263, 0.08346263, 0.08346263, 1.08655358, 1.37522872,\n           1.37522872])\n    &gt;&gt;&gt; maxRstat(Z, R, 3)\n    array([0.        , 0.        , 0.        , 0.        , 0.70710678,\n           0.70710678, 0.70710678, 0.70710678, 1.15470054, 1.15470054,\n           1.15470054])\n    \"\"\"\n    Z = np.asarray(Z, order='c')\n    R = np.asarray(R, order='c')\n    is_valid_linkage(Z, throw=True, name='Z')\n    is_valid_im(R, throw=True, name='R')\n    if type(i) is not int:\n        raise TypeError('The third argument must be an integer.')\n    if i &lt; 0 or i &gt; 3:\n        raise ValueError('i must be an integer between 0 and 3 inclusive.')\n\n    if Z.shape[0] != R.shape[0]:\n        raise ValueError(\"The inconsistency matrix and linkage matrix each \"\n                         \"have a different number of rows.\")\n\n    n = Z.shape[0] + 1\n    MR = np.zeros((n - 1,))\n    [Z, R] = _copy_arrays_if_base_present([Z, R])\n    _hierarchy.get_max_Rfield_for_each_cluster(Z, R, MR, int(n), i)\n    return MR\n\n\ndef leaders(Z, T):\n    \"\"\"\n    Return the root nodes in a hierarchical clustering.\n    Returns the root nodes in a hierarchical clustering corresponding\n    to a cut defined by a flat cluster assignment vector ``T``. See\n    the ``fcluster`` function for more information on the format of ``T``.\n    For each flat cluster :math:`j` of the :math:`k` flat clusters\n    represented in the n-sized flat cluster assignment vector ``T``,\n    this function finds the lowest cluster node :math:`i` in the linkage\n    tree Z, such that:\n      * leaf descendants belong only to flat cluster j\n        (i.e., ``T[p]==j`` for all :math:`p` in :math:`S(i)`, where\n        :math:`S(i)` is the set of leaf ids of descendant leaf nodes\n        with cluster node :math:`i`)\n      * there does not exist a leaf that is not a descendant with\n        :math:`i` that also belongs to cluster :math:`j`\n        (i.e., ``T[q]!=j`` for all :math:`q` not in :math:`S(i)`). If\n        this condition is violated, ``T`` is not a valid cluster\n        assignment vector, and an exception will be thrown.\n    Parameters\n    ----------\n    Z : ndarray\n        The hierarchical clustering encoded as a matrix. See\n        `linkage` for more information.\n    T : ndarray\n        The flat cluster assignment vector.\n    Returns\n    -------\n    L : ndarray\n        The leader linkage node id's stored as a k-element 1-D array,\n        where ``k`` is the number of flat clusters found in ``T``.\n        ``L[j]=i`` is the linkage cluster node id that is the\n        leader of flat cluster with id M[j]. If ``i &lt; n``, ``i``\n        corresponds to an original observation, otherwise it\n        corresponds to a non-singleton cluster.\n    M : ndarray\n        The leader linkage node id's stored as a k-element 1-D array, where\n        ``k`` is the number of flat clusters found in ``T``. This allows the\n        set of flat cluster ids to be any arbitrary set of ``k`` integers.\n        For example: if ``L[3]=2`` and ``M[3]=8``, the flat cluster with\n        id 8's leader is linkage node 2.\n    See Also\n    --------\n    fcluster: for the creation of flat cluster assignments.\n    Examples\n    --------\n    &gt;&gt;&gt; from scipy.cluster.hierarchy import ward, fcluster, leaders\n    &gt;&gt;&gt; from scipy.spatial.distance import pdist\n    Given a linkage matrix ``Z`` - obtained after apply a clustering method\n    to a dataset ``X`` - and a flat cluster assignment array ``T``:\n    &gt;&gt;&gt; X = [[0, 0], [0, 1], [1, 0],\n    ...      [0, 4], [0, 3], [1, 4],\n    ...      [4, 0], [3, 0], [4, 1],\n    ...      [4, 4], [3, 4], [4, 3]]\n    &gt;&gt;&gt; Z = ward(pdist(X))\n    &gt;&gt;&gt; Z\n    array([[ 0.        ,  1.        ,  1.        ,  2.        ],\n           [ 3.        ,  4.        ,  1.        ,  2.        ],\n           [ 6.        ,  7.        ,  1.        ,  2.        ],\n           [ 9.        , 10.        ,  1.        ,  2.        ],\n           [ 2.        , 12.        ,  1.29099445,  3.        ],\n           [ 5.        , 13.        ,  1.29099445,  3.        ],\n           [ 8.        , 14.        ,  1.29099445,  3.        ],\n           [11.        , 15.        ,  1.29099445,  3.        ],\n           [16.        , 17.        ,  5.77350269,  6.        ],\n           [18.        , 19.        ,  5.77350269,  6.        ],\n           [20.        , 21.        ,  8.16496581, 12.        ]])\n    &gt;&gt;&gt; T = fcluster(Z, 3, criterion='distance')\n    &gt;&gt;&gt; T\n    array([1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4], dtype=int32)\n    `scipy.cluster.hierarchy.leaders` returns the indices of the nodes\n    in the dendrogram that are the leaders of each flat cluster:\n    &gt;&gt;&gt; L, M = leaders(Z, T)\n    &gt;&gt;&gt; L\n    array([16, 17, 18, 19], dtype=int32)\n    (remember that indices 0-11 point to the 12 data points in ``X``,\n    whereas indices 12-22 point to the 11 rows of ``Z``)\n    `scipy.cluster.hierarchy.leaders` also returns the indices of\n    the flat clusters in ``T``:\n    &gt;&gt;&gt; M\n    array([1, 2, 3, 4], dtype=int32)\n    \"\"\"\n    Z = np.asarray(Z, order='c')\n    T = np.asarray(T, order='c')\n    if type(T) != np.ndarray or T.dtype != 'i':\n        raise TypeError('T must be a one-dimensional numpy array of integers.')\n    is_valid_linkage(Z, throw=True, name='Z')\n    if len(T) != Z.shape[0] + 1:\n        raise ValueError('Mismatch: len(T)!=Z.shape[0] + 1.')\n\n    Cl = np.unique(T)\n    kk = len(Cl)\n    L = np.zeros((kk,), dtype='i')\n    M = np.zeros((kk,), dtype='i')\n    n = Z.shape[0] + 1\n    [Z, T] = _copy_arrays_if_base_present([Z, T])\n    s = _hierarchy.leaders(Z, T, L, M, int(kk), int(n))\n    if s &gt;= 0:\n        raise ValueError(('T is not a valid assignment vector. Error found '\n                          'when examining linkage node %d (&lt; 2n-1).') % s)\n    return (L, M)"
  },
  {
    "objectID": "notes/ClusteringNotes.html#implementing-dendrogram-cuts-in-plotly",
    "href": "notes/ClusteringNotes.html#implementing-dendrogram-cuts-in-plotly",
    "title": "Clustering Notes",
    "section": "",
    "text": "To implement cuts, the basic modification to be made is including truncation parameters such as truncate_mode and p to all wrappers around the scipy’s dendrogram function. The following changes are made:\n\ncreate_dendrogram now includes p and truncate_mode as parameters\nthe plotly class _Dendrogram now has both p and truncate_mode as object parameters\nthe get_dendrogram_traces method now passes on these parameters to its call to scipy’s dendogram function\n\n\nfrom __future__ import absolute_import\n\nfrom collections import OrderedDict\n\nfrom plotly import exceptions, optional_imports\nfrom plotly.graph_objs import graph_objs\n\n# Optional imports, may be None for users that only use our core functionality.\nnp = optional_imports.get_module(\"numpy\")\nscp = optional_imports.get_module(\"scipy\")\nsch = optional_imports.get_module(\"scipy.cluster.hierarchy\")\nscs = optional_imports.get_module(\"scipy.spatial\")\n\n\ndef create_dendrogram(\n    X,\n    p=30,\n    truncate_mode=None,\n    orientation=\"bottom\",\n    labels=None,\n    colorscale=None,\n    distfun=None,\n    linkagefun=lambda x: sch.linkage(x, \"complete\"),\n    hovertext=None,\n    color_threshold=None,\n    leaf_label_func=None,\n    \n):\n    \"\"\"\n    Function that returns a dendrogram Plotly figure object. This is a thin, modified\n    wrapper around scipy.cluster.hierarchy.dendrogram that includes truncation parameters.\n    \n    See also https://dash.plot.ly/dash-bio/clustergram.\n    :param (ndarray) X: Matrix of observations as array of arrays\n    :param (str) orientation: 'top', 'right', 'bottom', or 'left'\n    :param (list) labels: List of axis category labels(observation labels)\n    :param (list) colorscale: Optional colorscale for the dendrogram tree.\n                              Requires 8 colors to be specified, the 7th of\n                              which is ignored.  With scipy&gt;=1.5.0, the 2nd, 3rd\n                              and 6th are used twice as often as the others.\n                              Given a shorter list, the missing values are\n                              replaced with defaults and with a longer list the\n                              extra values are ignored.\n    :param (function) distfun: Function to compute the pairwise distance from\n                               the observations\n    :param (function) linkagefun: Function to compute the linkage matrix from\n                               the pairwise distances\n    :param (list[list]) hovertext: List of hovertext for constituent traces of dendrogram\n                               clusters\n    :param (double) color_threshold: Value at which the separation of clusters will be made\n    Example 1: Simple bottom oriented dendrogram\n    &gt;&gt;&gt; from plotly.figure_factory import create_dendrogram\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; X = np.random.rand(10,10)\n    &gt;&gt;&gt; fig = create_dendrogram(X)\n    &gt;&gt;&gt; fig.show()\n    Example 2: Dendrogram to put on the left of the heatmap\n    \n    &gt;&gt;&gt; from plotly.figure_factory import create_dendrogram\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; X = np.random.rand(5,5)\n    &gt;&gt;&gt; names = ['Jack', 'Oxana', 'John', 'Chelsea', 'Mark']\n    &gt;&gt;&gt; dendro = create_dendrogram(X, orientation='right', labels=names)\n    &gt;&gt;&gt; dendro.update_layout({'width':700, 'height':500}) # doctest: +SKIP\n    &gt;&gt;&gt; dendro.show()\n    Example 3: Dendrogram with Pandas\n    \n    &gt;&gt;&gt; from plotly.figure_factory import create_dendrogram\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; import pandas as pd\n    &gt;&gt;&gt; Index= ['A','B','C','D','E','F','G','H','I','J']\n    &gt;&gt;&gt; df = pd.DataFrame(abs(np.random.randn(10, 10)), index=Index)\n    &gt;&gt;&gt; fig = create_dendrogram(df, labels=Index)\n    &gt;&gt;&gt; fig.show()\n    \"\"\"\n    if not scp or not scs or not sch:\n        raise ImportError(\n            \"FigureFactory.create_dendrogram requires scipy, \\\n                            scipy.spatial and scipy.hierarchy\"\n        )\n\n    s = X.shape\n    if len(s) != 2:\n        exceptions.PlotlyError(\"X should be 2-dimensional array.\")\n\n    if distfun is None:\n        distfun = scs.distance.pdist\n\n    dendrogram = _Dendrogram(\n        X=X,\n        p=p,\n        truncate_mode=truncate_mode,\n        orientation=orientation,\n        labels=labels,\n        colorscale=colorscale,\n        distfun=distfun,\n        linkagefun=linkagefun,\n        hovertext=hovertext,\n        color_threshold=color_threshold,\n        leaf_label_func=leaf_label_func,\n    )\n\n    return graph_objs.Figure(data=dendrogram.data, layout=dendrogram.layout)\n\n\nclass _Dendrogram(object):\n    \"\"\"Refer to FigureFactory.create_dendrogram() for docstring.\"\"\"\n\n    def __init__(\n        self,\n        X,\n        p=30,\n        truncate_mode=None,\n        orientation=\"bottom\",\n        labels=None,\n        colorscale=None,\n        width=np.inf,\n        height=np.inf,\n        xaxis=\"xaxis\",\n        yaxis=\"yaxis\",\n        distfun=None,\n        linkagefun=lambda x: sch.linkage(x, \"complete\"),\n        hovertext=None,\n        color_threshold=None,\n        leaf_label_func=None,\n    ):\n        self.p = p\n        self.truncate_mode=truncate_mode\n        self.orientation=orientation\n        self.labels = labels\n        self.xaxis = xaxis\n        self.yaxis = yaxis\n        self.data = []\n        self.leaves = []\n        self.sign = {self.xaxis: 1, self.yaxis: 1}\n        self.layout = {self.xaxis: {}, self.yaxis: {}}\n        self.leaf_label_func = leaf_label_func\n\n        if self.orientation in [\"left\", \"bottom\"]:\n            self.sign[self.xaxis] = 1\n        else:\n            self.sign[self.xaxis] = -1\n\n        if self.orientation in [\"right\", \"bottom\"]:\n            self.sign[self.yaxis] = 1\n        else:\n            self.sign[self.yaxis] = -1\n\n        if distfun is None:\n            distfun = scs.distance.pdist\n\n        (dd_traces, xvals, yvals, ordered_labels, leaves) = self.get_dendrogram_traces(\n            X, colorscale, distfun, linkagefun, hovertext, color_threshold, \n        )\n\n        self.labels = ordered_labels\n        self.leaves = leaves\n        yvals_flat = yvals.flatten()\n        xvals_flat = xvals.flatten()\n\n        self.zero_vals = []\n\n        for i in range(len(yvals_flat)):\n            if yvals_flat[i] == 0.0 and xvals_flat[i] not in self.zero_vals:\n                self.zero_vals.append(xvals_flat[i])\n\n        if len(self.zero_vals) &gt; len(yvals) + 1:\n            # If the length of zero_vals is larger than the length of yvals,\n            # it means that there are wrong vals because of the identicial samples.\n            # Three and more identicial samples will make the yvals of spliting\n            # center into 0 and it will accidentally take it as leaves.\n            l_border = int(min(self.zero_vals))\n            r_border = int(max(self.zero_vals))\n            correct_leaves_pos = range(\n                l_border, r_border + 1, int((r_border - l_border) / len(yvals))\n            )\n            # Regenerating the leaves pos from the self.zero_vals with equally intervals.\n            self.zero_vals = [v for v in correct_leaves_pos]\n\n        self.zero_vals.sort()\n        self.layout = self.set_figure_layout(width, height)\n        self.data = dd_traces\n\n    def get_color_dict(self, colorscale):\n        \"\"\"\n        Returns colorscale used for dendrogram tree clusters.\n        :param (list) colorscale: Colors to use for the plot in rgb format.\n        :rtype (dict): A dict of default colors mapped to the user colorscale.\n        \"\"\"\n\n        # These are the color codes returned for dendrograms\n        # We're replacing them with nicer colors\n        # This list is the colors that can be used by dendrogram, which were\n        # determined as the combination of the default above_threshold_color and\n        # the default color palette (see scipy/cluster/hierarchy.py)\n        d = {\n            \"r\": \"red\",\n            \"g\": \"green\",\n            \"b\": \"blue\",\n            \"c\": \"cyan\",\n            \"m\": \"magenta\",\n            \"y\": \"yellow\",\n            \"k\": \"black\",\n            # TODO: 'w' doesn't seem to be in the default color\n            # palette in scipy/cluster/hierarchy.py\n            \"w\": \"white\",\n        }\n        default_colors = OrderedDict(sorted(d.items(), key=lambda t: t[0]))\n\n        if colorscale is None:\n            rgb_colorscale = [\n                \"rgb(0,116,217)\",  # blue\n                \"rgb(35,205,205)\",  # cyan\n                \"rgb(61,153,112)\",  # green\n                \"rgb(40,35,35)\",  # black\n                \"rgb(133,20,75)\",  # magenta\n                \"rgb(255,65,54)\",  # red\n                \"rgb(255,255,255)\",  # white\n                \"rgb(255,220,0)\",  # yellow\n            ]\n        else:\n            rgb_colorscale = colorscale\n\n        for i in range(len(default_colors.keys())):\n            k = list(default_colors.keys())[i]  # PY3 won't index keys\n            if i &lt; len(rgb_colorscale):\n                default_colors[k] = rgb_colorscale[i]\n\n        # add support for cyclic format colors as introduced in scipy===1.5.0\n        # before this, the colors were named 'r', 'b', 'y' etc., now they are\n        # named 'C0', 'C1', etc. To keep the colors consistent regardless of the\n        # scipy version, we try as much as possible to map the new colors to the\n        # old colors\n        # this mapping was found by inpecting scipy/cluster/hierarchy.py (see\n        # comment above).\n        new_old_color_map = [\n            (\"C0\", \"b\"),\n            (\"C1\", \"g\"),\n            (\"C2\", \"r\"),\n            (\"C3\", \"c\"),\n            (\"C4\", \"m\"),\n            (\"C5\", \"y\"),\n            (\"C6\", \"k\"),\n            (\"C7\", \"g\"),\n            (\"C8\", \"r\"),\n            (\"C9\", \"c\"),\n        ]\n        for nc, oc in new_old_color_map:\n            try:\n                default_colors[nc] = default_colors[oc]\n            except KeyError:\n                # it could happen that the old color isn't found (if a custom\n                # colorscale was specified), in this case we set it to an\n                # arbitrary default.\n                default_colors[n] = \"rgb(0,116,217)\"\n\n        return default_colors\n\n    def set_axis_layout(self, axis_key):\n        \"\"\"\n        Sets and returns default axis object for dendrogram figure.\n        :param (str) axis_key: E.g., 'xaxis', 'xaxis1', 'yaxis', yaxis1', etc.\n        :rtype (dict): An axis_key dictionary with set parameters.\n        \"\"\"\n        axis_defaults = {\n            \"type\": \"linear\",\n            \"ticks\": \"outside\",\n            \"mirror\": \"allticks\",\n            \"rangemode\": \"tozero\",\n            \"showticklabels\": True,\n            \"zeroline\": False,\n            \"showgrid\": False,\n            \"showline\": True,\n        }\n\n        if len(self.labels) != 0:\n            axis_key_labels = self.xaxis\n            if self.orientation in [\"left\", \"right\"]:\n                axis_key_labels = self.yaxis\n            if axis_key_labels not in self.layout:\n                self.layout[axis_key_labels] = {}\n            self.layout[axis_key_labels][\"tickvals\"] = [\n                zv * self.sign[axis_key] for zv in self.zero_vals\n            ]\n            self.layout[axis_key_labels][\"ticktext\"] = self.labels\n            self.layout[axis_key_labels][\"tickmode\"] = \"array\"\n\n        self.layout[axis_key].update(axis_defaults)\n\n        return self.layout[axis_key]\n\n    def set_figure_layout(self, width, height):\n        \"\"\"\n        Sets and returns default layout object for dendrogram figure.\n        \"\"\"\n        self.layout.update(\n            {\n                \"showlegend\": False,\n                \"autosize\": False,\n                \"hovermode\": \"closest\",\n                \"width\": width,\n                \"height\": height,\n            }\n        )\n\n        self.set_axis_layout(self.xaxis)\n        self.set_axis_layout(self.yaxis)\n\n        return self.layout\n\n    def get_dendrogram_traces(\n        self, X, colorscale, distfun, linkagefun, hovertext, color_threshold\n    ):\n        \"\"\"\n        Calculates all the elements needed for plotting a dendrogram.\n        :param (ndarray) X: Matrix of observations as array of arrays\n        :param (list) colorscale: Color scale for dendrogram tree clusters\n        :param (function) distfun: Function to compute the pairwise distance\n                                   from the observations\n        :param (function) linkagefun: Function to compute the linkage matrix\n                                      from the pairwise distances\n        :param (list) hovertext: List of hovertext for constituent traces of dendrogram\n        :rtype (tuple): Contains all the traces in the following order:\n            (a) trace_list: List of Plotly trace objects for dendrogram tree\n            (b) icoord: All X points of the dendrogram tree as array of arrays\n                with length 4\n            (c) dcoord: All Y points of the dendrogram tree as array of arrays\n                with length 4\n            (d) ordered_labels: leaf labels in the order they are going to\n                appear on the plot\n            (e) P['leaves']: left-to-right traversal of the leaves\n        \"\"\"\n        d = distfun(X)\n        Z = linkagefun(d)\n        P = sch.dendrogram(\n            Z,\n            p=self.p,\n            truncate_mode=self.truncate_mode,\n            orientation=self.orientation,\n            labels=self.labels,\n            no_plot=True,\n            color_threshold=color_threshold,\n            leaf_label_func=self.leaf_label_func\n        )\n\n        icoord = np.array(P[\"icoord\"])\n        dcoord = np.array(P[\"dcoord\"])\n        ordered_labels = np.array(P[\"ivl\"])\n        color_list = np.array(P[\"color_list\"])\n        colors = self.get_color_dict(colorscale)\n\n        trace_list = []\n\n        for i in range(len(icoord)):\n            # xs and ys are arrays of 4 points that make up the '∩' shapes\n            # of the dendrogram tree\n            if self.orientation in [\"top\", \"bottom\"]:\n                xs = icoord[i]\n            else:\n                xs = dcoord[i]\n\n            if self.orientation in [\"top\", \"bottom\"]:\n                ys = dcoord[i]\n            else:\n                ys = icoord[i]\n            color_key = color_list[i]\n            hovertext_label = None\n            if hovertext:\n                hovertext_label = hovertext[i]\n            trace = dict(\n                type=\"scatter\",\n                x=np.multiply(self.sign[self.xaxis], xs), \n                y=np.multiply(self.sign[self.yaxis], ys),\n                mode=\"lines\",\n                marker=dict(color=colors[color_key]),\n                text=hovertext_label,\n                hoverinfo=\"text\",\n            )\n\n            try:\n                x_index = int(self.xaxis[-1])\n            except ValueError:\n                x_index = \"\"\n\n            try:\n                y_index = int(self.yaxis[-1])\n            except ValueError:\n                y_index = \"\"\n\n            trace[\"xaxis\"] = \"x\" + x_index\n            trace[\"yaxis\"] = \"y\" + y_index\n\n            trace_list.append(trace)\n\n        return trace_list, icoord, dcoord, ordered_labels, P[\"leaves\"]\n\n\n\n\n\nIn Dendrograms with hovertext ranging a few hundred nodes, truncation is necessary\nThe Z matrix contains contains \\(n-1\\) rows each representing new clusters, where \\(n\\) is then number of data points.\n\nhigher rows signify larger clusters, ordered by the distance metric of their child clusters.\nthe row indices also correspond to the ClusterNode instances in the node list returned by scipy.cluster.hierarchy.to_tree\ndefault indexing will have the root cluster as \\(n-2\\)\n\nTruncation when truncate_mode is set to ‘lastp’ is governed by parameter p\n\np becomes the number of leaves to be displayed\nthere will be a total of \\(p - 1\\) new clusters displayed\nthe node indices will then be in range [2n-2-p, 2n-2], or range(2*n - 2 - p, 2*n - 1)\n\nIf indices are shown\nFor UI considerations, a shift in p Simpler indices, \\(j\\) can be transformations of default indices \\(i\\) for UI considerations can be given by: \\[j = 2n-2 - i\\]"
  },
  {
    "objectID": "notes/ClusteringNotes.html#t-stochastic-neighbor-embedding-tsne",
    "href": "notes/ClusteringNotes.html#t-stochastic-neighbor-embedding-tsne",
    "title": "Clustering Notes",
    "section": "t-Stochastic Neighbor Embedding (TSNE)",
    "text": "t-Stochastic Neighbor Embedding (TSNE)\nThis is a technique that maps a higher dimensional space to a lower dimensional space while preserving probabilities of choosing analogous neighboring points between the two difference spaces.\nThe original paper can be read here as well as a post about effective usage."
  },
  {
    "objectID": "notes/ClusteringNotes.html#uniform-manifold-approximation-umap",
    "href": "notes/ClusteringNotes.html#uniform-manifold-approximation-umap",
    "title": "Clustering Notes",
    "section": "Uniform Manifold Approximation (UMAP)",
    "text": "Uniform Manifold Approximation (UMAP)\nThis is another method of dimensional reduction but works well for non-linear mappings."
  },
  {
    "objectID": "notes/ClusteringNotes.html#implementation-with-plotly",
    "href": "notes/ClusteringNotes.html#implementation-with-plotly",
    "title": "Clustering Notes",
    "section": "Implementation with Plotly",
    "text": "Implementation with Plotly\nHere is an example of using Plotly to visualize data sets using both techniques, straight from their documentation\n\nimport plotly.express as px\n\ndf = px.data.iris()\nfeatures = [\"sepal_width\", \"sepal_length\", \"petal_width\", \"petal_length\"]\nfig = px.scatter_matrix(df, dimensions=features, color=\"species\")\nfig.show()\n\n\n\n\n\n\n\n\n\nfrom sklearn.manifold import TSNE, trustworthiness\nimport plotly.express as px\n\ndf = px.data.iris()\n\nfeatures = df.loc[:, :'petal_width']\n\ntsne = TSNE(n_components=2, random_state=0)\nprojections = tsne.fit_transform(features)\n\nfig = px.scatter(\n    projections, x=0, y=1,\n    color=df.species, labels={'color': 'species'}\n)\nfig.show()\n\n\n\n\n\n\n\n\n\nscore = trustworthiness(features.to_numpy(), projections, n_neighbors=5)\nprint(f'Trustworthiness score is: {score}')\n\nTrustworthiness score is: 0.9880657276995305\n\n\n\nfrom umap import UMAP\nimport plotly.express as px\n\ndf = px.data.iris()\n\nfeatures = df.loc[:, :'petal_width']\numap_2d = UMAP(random_state=0, n_components=2)\numap_3d = UMAP(random_state=0, n_components=3, n_neighbors=5)\nproj_2d = umap_2d.fit_transform(features)\nfig_2d = px.scatter(proj_2d, x=0, y=1,\n                 color=df.species, labels={'color': 'species'}\n                )\nfig_2d.show()\n\nC:\\Users\\Jonathan\\anaconda3\\envs\\py313\\Lib\\site-packages\\umap\\umap_.py:1952: UserWarning:\n\nn_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n\n\n\n\n\n\n\n\n\n\n\nscore = trustworthiness(features.to_numpy(), projections, n_neighbors=5)\nprint(f'Trustworthiness score is: {score}')\n\nTrustworthiness score is: 0.9880657276995305"
  },
  {
    "objectID": "notes/Closures.html",
    "href": "notes/Closures.html",
    "title": "Closure Notes",
    "section": "",
    "text": "These are notes based on Real Python’s article with motivation to better understand Python decorators.\nA closure also known as a function closure, is a technique to store a function with an environment. The function has local variables mapped to the enclosing scope.\nIn Python, a closure is a nested function where the inner function is returned by the outer function, along with parameter bindings defined in the outer function.\n\n\n\n\nThis is simply a nested function. Just like in the format of many Elements of Programming Interview Python solutions, nesting functions can control scope:\n\ndef outer_func():\n    name = 'Jonathan'\n    def inner_func():\n        print(f'Hello, {name}!')\n    inner_func()\n\nouter_func()\n\nHello, Jonathan!\n\n\n\n\n\nThe defining feature of a function closure separating them from other inner functions is that it returns the inner function. The anatomy of a function closure is the following: - outer function defining the scope - variables local to the outer function - inner function defined inside the outer function\n\ndef outer_func():\n    name = 'Jonathan'\n    def inner_func():\n        print(f'Hello, {name}')\n    return inner_func\n\nouter_func()\n\n&lt;function __main__.outer_func.&lt;locals&gt;.inner_func()&gt;\n\n\n\ngreeter = outer_func()\ngreeter()\n\nHello, Jonathan\n\n\nLambda functions can be used to build closures:\n\ndef outer_func():\n    name = 'Jonathan'\n    return lambda: print(f'Hello, {name}!')\n\ngreeter = outer_func()\ngreeter()\n\nHello, Jonathan!\n\n\n\n\n\nIn this example, the inner function has access to variables that are defined even after itself, demonstrating the subtleties of scope.\n\ndef outer_func(outer_arg):\n    local_var = 'Outer local variable'\n    def closure():\n        print(outer_arg)\n        print(local_var)\n        print(another_local_var)\n    another_local_var = 'Another local variable'\n    return closure\n\nclosure = outer_func('Outer argument')\nclosure()\n\nOuter argument\nOuter local variable\nAnother local variable\n\n\n\n\n\ndef make_counter():\n    count = 0\n    def counter():\n        nonlocal count # look for a predefined variable instead of a new one\n        count += 1\n        return count\n    return counter\n\ncounter = make_counter()\ncounter()\n\n1\n\n\n\ncounter()\n\n2\n\n\n\ncounter()\n\n3\n\n\n\n\n\n\ndef make_appender():\n    items = []\n    def appender(new_item):\n        items.append(new_item)\n        return items\n    return appender\n\nappender = make_appender()\nappender('First item')\n\n['First item']\n\n\n\nappender('Second item')\n\n['First item', 'Second item']\n\n\n\nappender('Third item')\n\n['First item', 'Second item', 'Third item']\n\n\n\n\n\n\n\n\n\n\ndef make_root_calculator(root_degree, precision=2):\n    def root_calculator(number):\n        return round(pow(number, 1 / root_degree), precision)\n    return root_calculator\n\nsquare_root = make_root_calculator(2, 4)\n\nsquare_root(42)\n\n6.4807\n\n\n\ncubic_root = make_root_calculator(3)\ncubic_root(42)\n\n3.48\n\n\n\n\n\n\ndef cumulative_average():\n    data = []\n    def average(value):\n        data.append(value)\n        return sum(data) / len(data)\n    return average\n\n# more efficient implementation\ndef cumulative_average():\n    total = 0\n    n = 0\n    def average(value):\n        nonlocal total\n        nonlocal n\n        n += 1\n        total += value\n        return total / n\n    return average\n        \n\n\nstream_average = cumulative_average()\n\nstream_average(12)\n\n12.0\n\n\n\nstream_average(13)\n\n12.5\n\n\n\nstream_average(11)\n\n12.0\n\n\n\nstream_average(10)\n\n11.5\n\n\n\n\n\nThe callback() function returns a closure object that is passed onto the command argument, which takes callable objects without arguments. This is a workaround to pass parameters.\n\nimport tkinter as tk\n\napp = tk.Tk()\napp.title('GUI App')\napp.geometry('320x240')\n\nlabel = tk.Label(\n    app,\n    font=('Helvetica', 16, 'bold'),\n)\nlabel.pack()\n\ndef callback(text):\n    def closure():\n        label.config(text=text)\n        \n    return closure\n\nbutton = tk.Button(\n    app,\n    text='Greet',\n    command=callback('Hello, World!'),\n)\n\nbutton.pack()\n\napp.mainloop()\n    \n\n\n\n\n\nThere are two types of decorators in Python: - function based - class based\nHere is a simple function based implementation with and without the decorator, and using the syntax:\n\ndef greet():\n    print('Hello, World!')\n\ngreet()\n\nHello, World!\n\n\n\ndef decorator(function):\n    def closure():\n        print('Do something before calling the function.')\n        function()\n        print('Do something after calling the function.')\n    return closure\n\ndef greet():\n    print('Hello, World!')\n\ngreet = decorator(greet)\ngreet()\n\nDo something before calling the function.\nHello, World!\nDo something after calling the function.\n\n\n\n@decorator\ndef greet():\n    print('Hello, World!')\n\ngreet()\n\nDo something before calling the function.\nHello, World!\nDo something after calling the function.\n\n\n\n\n\n\ndef memoize(function):\n    cache = {}\n    def closure(number):\n        if number not in cache:\n            cache[number] = function(number)\n        return cache[number]\n    return closure\n\n\nfrom time import sleep\nfrom timeit import timeit\n\ndef slow_operation(number):\n    sleep(0.5)\n\ntimeit(\"[slow_operation(number) for number in [2, 3, 4, 2, 3, 4]]\",\n       globals=globals(),\n       number=1,\n      )\n\n3.0021948000066914\n\n\n\n@memoize\ndef slow_operation(number):\n    sleep(0.5)\n\ntimeit(\"[slow_operation(number) for number in [2, 3, 4, 2, 3, 4]]\",\n       globals=globals(),\n       number=1,\n      )\n\n1.501583400007803\n\n\n\n\n\n\n\n\nclass Stack:\n    def __init__(self):\n        self._items = []\n\n    def push(self, item):\n        self._items.append(item)\n\n    def pop(self):\n        return self._items.pop()\n\nstack = Stack()\nstack.push(1)\nstack.push(2)\nstack.push(3)\n\nstack.pop()\n\n3\n\n\n\nstack._items\n\n[1, 2]\n\n\n\n\n\n\ndef Stack():\n    _items = []\n\n    def push(item):\n        _items.append(item)\n\n    def pop():\n        return _items.pop()\n\n    def closure():\n        pass\n\n    closure.push = push\n    closure.pop = pop\n    return closure\n\nstack = Stack()\nstack.push(1)\nstack.push(2)\nstack.push(3)\n\nstack.pop()\n\n3\n\n\n\nstack._items\n\nAttributeError: 'function' object has no attribute '_items'\n\n\n\nstack\n\n&lt;function __main__.Stack.&lt;locals&gt;.closure()&gt;\n\n\n\n\n\n\nInstead of closures, which can be difficult to reason about, we can use classes with the __call__() method:\n\ndef make_root_calculator(root_degree, precision=2):\n    def root_calculator(number):\n        return round(pow(number, 1 / root_degree), precision)\n    return root_calculator\n\n\nsquare_root = make_root_calculator(2, 4)\nsquare_root(42)\n\n6.4807\n\n\n\nclass RootCalculator:\n    def __init__(self, root_degree, precision=2):\n        self.root_degree = root_degree\n        self.precision = precision\n\n    def __call__(self, number):\n        return round(pow(number, 1 / self.root_degree), self.precision)\n\nsquare_root = RootCalculator(2, 4)\nsquare_root(42)\n\n6.4807\n\n\n\ncubic_root = RootCalculator(3)\ncubic_root(42)\n\n3.48"
  },
  {
    "objectID": "notes/Closures.html#background",
    "href": "notes/Closures.html#background",
    "title": "Closure Notes",
    "section": "",
    "text": "This is simply a nested function. Just like in the format of many Elements of Programming Interview Python solutions, nesting functions can control scope:\n\ndef outer_func():\n    name = 'Jonathan'\n    def inner_func():\n        print(f'Hello, {name}!')\n    inner_func()\n\nouter_func()\n\nHello, Jonathan!\n\n\n\n\n\nThe defining feature of a function closure separating them from other inner functions is that it returns the inner function. The anatomy of a function closure is the following: - outer function defining the scope - variables local to the outer function - inner function defined inside the outer function\n\ndef outer_func():\n    name = 'Jonathan'\n    def inner_func():\n        print(f'Hello, {name}')\n    return inner_func\n\nouter_func()\n\n&lt;function __main__.outer_func.&lt;locals&gt;.inner_func()&gt;\n\n\n\ngreeter = outer_func()\ngreeter()\n\nHello, Jonathan\n\n\nLambda functions can be used to build closures:\n\ndef outer_func():\n    name = 'Jonathan'\n    return lambda: print(f'Hello, {name}!')\n\ngreeter = outer_func()\ngreeter()\n\nHello, Jonathan!\n\n\n\n\n\nIn this example, the inner function has access to variables that are defined even after itself, demonstrating the subtleties of scope.\n\ndef outer_func(outer_arg):\n    local_var = 'Outer local variable'\n    def closure():\n        print(outer_arg)\n        print(local_var)\n        print(another_local_var)\n    another_local_var = 'Another local variable'\n    return closure\n\nclosure = outer_func('Outer argument')\nclosure()\n\nOuter argument\nOuter local variable\nAnother local variable\n\n\n\n\n\ndef make_counter():\n    count = 0\n    def counter():\n        nonlocal count # look for a predefined variable instead of a new one\n        count += 1\n        return count\n    return counter\n\ncounter = make_counter()\ncounter()\n\n1\n\n\n\ncounter()\n\n2\n\n\n\ncounter()\n\n3\n\n\n\n\n\n\ndef make_appender():\n    items = []\n    def appender(new_item):\n        items.append(new_item)\n        return items\n    return appender\n\nappender = make_appender()\nappender('First item')\n\n['First item']\n\n\n\nappender('Second item')\n\n['First item', 'Second item']\n\n\n\nappender('Third item')\n\n['First item', 'Second item', 'Third item']"
  },
  {
    "objectID": "notes/Closures.html#creating-closures-to-retain-state",
    "href": "notes/Closures.html#creating-closures-to-retain-state",
    "title": "Closure Notes",
    "section": "",
    "text": "def make_root_calculator(root_degree, precision=2):\n    def root_calculator(number):\n        return round(pow(number, 1 / root_degree), precision)\n    return root_calculator\n\nsquare_root = make_root_calculator(2, 4)\n\nsquare_root(42)\n\n6.4807\n\n\n\ncubic_root = make_root_calculator(3)\ncubic_root(42)\n\n3.48\n\n\n\n\n\n\ndef cumulative_average():\n    data = []\n    def average(value):\n        data.append(value)\n        return sum(data) / len(data)\n    return average\n\n# more efficient implementation\ndef cumulative_average():\n    total = 0\n    n = 0\n    def average(value):\n        nonlocal total\n        nonlocal n\n        n += 1\n        total += value\n        return total / n\n    return average\n        \n\n\nstream_average = cumulative_average()\n\nstream_average(12)\n\n12.0\n\n\n\nstream_average(13)\n\n12.5\n\n\n\nstream_average(11)\n\n12.0\n\n\n\nstream_average(10)\n\n11.5\n\n\n\n\n\nThe callback() function returns a closure object that is passed onto the command argument, which takes callable objects without arguments. This is a workaround to pass parameters.\n\nimport tkinter as tk\n\napp = tk.Tk()\napp.title('GUI App')\napp.geometry('320x240')\n\nlabel = tk.Label(\n    app,\n    font=('Helvetica', 16, 'bold'),\n)\nlabel.pack()\n\ndef callback(text):\n    def closure():\n        label.config(text=text)\n        \n    return closure\n\nbutton = tk.Button(\n    app,\n    text='Greet',\n    command=callback('Hello, World!'),\n)\n\nbutton.pack()\n\napp.mainloop()"
  },
  {
    "objectID": "notes/Closures.html#writing-decorators-with-closures",
    "href": "notes/Closures.html#writing-decorators-with-closures",
    "title": "Closure Notes",
    "section": "",
    "text": "There are two types of decorators in Python: - function based - class based\nHere is a simple function based implementation with and without the decorator, and using the syntax:\n\ndef greet():\n    print('Hello, World!')\n\ngreet()\n\nHello, World!\n\n\n\ndef decorator(function):\n    def closure():\n        print('Do something before calling the function.')\n        function()\n        print('Do something after calling the function.')\n    return closure\n\ndef greet():\n    print('Hello, World!')\n\ngreet = decorator(greet)\ngreet()\n\nDo something before calling the function.\nHello, World!\nDo something after calling the function.\n\n\n\n@decorator\ndef greet():\n    print('Hello, World!')\n\ngreet()\n\nDo something before calling the function.\nHello, World!\nDo something after calling the function."
  },
  {
    "objectID": "notes/Closures.html#impelementing-memoization-with-closures",
    "href": "notes/Closures.html#impelementing-memoization-with-closures",
    "title": "Closure Notes",
    "section": "",
    "text": "def memoize(function):\n    cache = {}\n    def closure(number):\n        if number not in cache:\n            cache[number] = function(number)\n        return cache[number]\n    return closure\n\n\nfrom time import sleep\nfrom timeit import timeit\n\ndef slow_operation(number):\n    sleep(0.5)\n\ntimeit(\"[slow_operation(number) for number in [2, 3, 4, 2, 3, 4]]\",\n       globals=globals(),\n       number=1,\n      )\n\n3.0021948000066914\n\n\n\n@memoize\ndef slow_operation(number):\n    sleep(0.5)\n\ntimeit(\"[slow_operation(number) for number in [2, 3, 4, 2, 3, 4]]\",\n       globals=globals(),\n       number=1,\n      )\n\n1.501583400007803"
  },
  {
    "objectID": "notes/Closures.html#achieving-encapsulation-with-closures",
    "href": "notes/Closures.html#achieving-encapsulation-with-closures",
    "title": "Closure Notes",
    "section": "",
    "text": "class Stack:\n    def __init__(self):\n        self._items = []\n\n    def push(self, item):\n        self._items.append(item)\n\n    def pop(self):\n        return self._items.pop()\n\nstack = Stack()\nstack.push(1)\nstack.push(2)\nstack.push(3)\n\nstack.pop()\n\n3\n\n\n\nstack._items\n\n[1, 2]\n\n\n\n\n\n\ndef Stack():\n    _items = []\n\n    def push(item):\n        _items.append(item)\n\n    def pop():\n        return _items.pop()\n\n    def closure():\n        pass\n\n    closure.push = push\n    closure.pop = pop\n    return closure\n\nstack = Stack()\nstack.push(1)\nstack.push(2)\nstack.push(3)\n\nstack.pop()\n\n3\n\n\n\nstack._items\n\nAttributeError: 'function' object has no attribute '_items'\n\n\n\nstack\n\n&lt;function __main__.Stack.&lt;locals&gt;.closure()&gt;"
  },
  {
    "objectID": "notes/Closures.html#exploring-alternatives-to-closures",
    "href": "notes/Closures.html#exploring-alternatives-to-closures",
    "title": "Closure Notes",
    "section": "",
    "text": "Instead of closures, which can be difficult to reason about, we can use classes with the __call__() method:\n\ndef make_root_calculator(root_degree, precision=2):\n    def root_calculator(number):\n        return round(pow(number, 1 / root_degree), precision)\n    return root_calculator\n\n\nsquare_root = make_root_calculator(2, 4)\nsquare_root(42)\n\n6.4807\n\n\n\nclass RootCalculator:\n    def __init__(self, root_degree, precision=2):\n        self.root_degree = root_degree\n        self.precision = precision\n\n    def __call__(self, number):\n        return round(pow(number, 1 / self.root_degree), self.precision)\n\nsquare_root = RootCalculator(2, 4)\nsquare_root(42)\n\n6.4807\n\n\n\ncubic_root = RootCalculator(3)\ncubic_root(42)\n\n3.48"
  }
]