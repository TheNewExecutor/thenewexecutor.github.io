<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.550">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Jonathan Sarker - Entropy Notes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Jonathan Sarker</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../projects.html"> 
<span class="menu-text">Projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../notes.html"> 
<span class="menu-text">Notes</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#entropy-notes" id="toc-entropy-notes" class="nav-link active" data-scroll-target="#entropy-notes">Entropy Notes</a>
  <ul class="collapse">
  <li><a href="#probability-notation" id="toc-probability-notation" class="nav-link" data-scroll-target="#probability-notation">Probability Notation</a></li>
  <li><a href="#shannon-information-of-measurement-x_i" id="toc-shannon-information-of-measurement-x_i" class="nav-link" data-scroll-target="#shannon-information-of-measurement-x_i">Shannon Information of Measurement <span class="math inline">\(x_i\)</span></a></li>
  <li><a href="#shannon-entropy-of-distribution-x" id="toc-shannon-entropy-of-distribution-x" class="nav-link" data-scroll-target="#shannon-entropy-of-distribution-x">Shannon Entropy of Distribution X</a></li>
  <li><a href="#joint-entropy-of-distributions-x-y" id="toc-joint-entropy-of-distributions-x-y" class="nav-link" data-scroll-target="#joint-entropy-of-distributions-x-y">Joint Entropy of Distributions X, Y:</a></li>
  <li><a href="#conditional-entropies" id="toc-conditional-entropies" class="nav-link" data-scroll-target="#conditional-entropies">Conditional Entropies:</a></li>
  <li><a href="#joint-entropy-and-conditional-entropy" id="toc-joint-entropy-and-conditional-entropy" class="nav-link" data-scroll-target="#joint-entropy-and-conditional-entropy">Joint Entropy and Conditional Entropy</a></li>
  <li><a href="#mutual-information-between-x-and-y" id="toc-mutual-information-between-x-and-y" class="nav-link" data-scroll-target="#mutual-information-between-x-and-y">Mutual Information Between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span></a></li>
  <li><a href="#relative-entropy-kullback-leibler-divergence-and-cross-entropy" id="toc-relative-entropy-kullback-leibler-divergence-and-cross-entropy" class="nav-link" data-scroll-target="#relative-entropy-kullback-leibler-divergence-and-cross-entropy">Relative Entropy, Kullback-Leibler Divergence and Cross-Entropy</a></li>
  <li><a href="#jensen-shannon-divergence" id="toc-jensen-shannon-divergence" class="nav-link" data-scroll-target="#jensen-shannon-divergence">Jensen-Shannon Divergence</a></li>
  <li><a href="#why-shannon-entropy" id="toc-why-shannon-entropy" class="nav-link" data-scroll-target="#why-shannon-entropy">Why Shannon Entropy?</a>
  <ul class="collapse">
  <li><a href="#additive-property-of-independent-variables" id="toc-additive-property-of-independent-variables" class="nav-link" data-scroll-target="#additive-property-of-independent-variables">Additive Property of Independent Variables</a></li>
  <li><a href="#entropy-is-maximized-with-uniform-probability-distributions" id="toc-entropy-is-maximized-with-uniform-probability-distributions" class="nav-link" data-scroll-target="#entropy-is-maximized-with-uniform-probability-distributions">Entropy is Maximized with Uniform Probability Distributions</a></li>
  </ul></li>
  <li><a href="#relation-to-log-likelihood" id="toc-relation-to-log-likelihood" class="nav-link" data-scroll-target="#relation-to-log-likelihood">Relation to Log-Likelihood</a>
  <ul class="collapse">
  <li><a href="#variation-of-information" id="toc-variation-of-information" class="nav-link" data-scroll-target="#variation-of-information">Variation of Information</a></li>
  <li><a href="#practical-implementations" id="toc-practical-implementations" class="nav-link" data-scroll-target="#practical-implementations">Practical Implementations</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-page-left" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Entropy Notes</h1>
</div>



<div class="quarto-title-meta column-page-left">

    
  
    
  </div>
  


</header>


<section id="entropy-notes" class="level1">
<h1>Entropy Notes</h1>
<section id="probability-notation" class="level3">
<h3 class="anchored" data-anchor-id="probability-notation">Probability Notation</h3>
<p>From David McKay’s <a href="https://www.inference.org.uk/itprnn/book.pdf"><em>Information Theory, Inference, and Learning Algorithms</em></a>, we acknowledge the following notation:</p>
<ul>
<li><strong>ensemble or random variable X</strong>: consists of the triple <span class="math inline">\((x, A_x, P_x)\)</span>
<ul>
<li><span class="math inline">\(x\)</span> is the <em>outcome</em> of the random variable <span class="math inline">\(X\)</span></li>
<li><span class="math inline">\(A_x\)</span> is the <em>alphabet</em> or set of possible values: <span class="math inline">\(\{a_1, a_2, ..., a_i, ..., a_I\}\)</span>
<ul>
<li>this is sometimes noted as <span class="math inline">\(\{x_1, x_2, ..., x_i, ..., x_I\}\)</span></li>
</ul></li>
<li><span class="math inline">\(P_x\)</span> are the associated probabilities, <span class="math inline">\(P(x=a_i) = p_i\)</span> of these outcomes: <span class="math inline">\(\{p_1, p_2, ..., p_i, ..., p_I\}\)</span></li>
</ul></li>
</ul>
</section>
<section id="shannon-information-of-measurement-x_i" class="level3">
<h3 class="anchored" data-anchor-id="shannon-information-of-measurement-x_i">Shannon Information of Measurement <span class="math inline">\(x_i\)</span></h3>
<p><span class="math display">\[h(x_i) = \log\Big(\frac{1}{P(x_i)}\Big)\tag{1}\]</span></p>
<p>where <span class="math inline">\(P(x_i)\)</span> is probability of outcome <span class="math inline">\(x_i\)</span>.</p>
</section>
<section id="shannon-entropy-of-distribution-x" class="level3">
<h3 class="anchored" data-anchor-id="shannon-entropy-of-distribution-x">Shannon Entropy of Distribution X</h3>
<p><span class="math display">\[H(X) = E\big[h(x_i)\big] = \sum_{x \in A_X} P(x)\log\Big(\frac{1}{P(x)}\Big)\tag{2}\]</span></p>
<p>where <span class="math inline">\(A_X\)</span> is the <strong>alphabet</strong> or set of outcomes for random variable <span class="math inline">\(X\)</span>. It’s also known as the <strong>marginal entropy</strong>.</p>
</section>
<section id="joint-entropy-of-distributions-x-y" class="level3">
<h3 class="anchored" data-anchor-id="joint-entropy-of-distributions-x-y">Joint Entropy of Distributions X, Y:</h3>
<p><span class="math display">\[H(X,Y) = \sum_{x, y \in A_XA_Y} P(x,y)\log\Big(\frac{1}{P(x, y)}\Big)\tag{3}\]</span></p>
<section id="independent-distributions" class="level4">
<h4 class="anchored" data-anchor-id="independent-distributions">Independent Distributions</h4>
<p><span class="math display">\[\begin{align}
H(X,Y) &amp;= \sum_{x, y \in A_XA_Y} P(x,y)\log\Big(\frac{1}{P(x, y)}\Big)\\
       &amp;= \sum_{x \in A_X}\sum_{y \in A_Y} P(x)P(y)\log\Big(\frac{1}{P(x)P(y)}\Big)\\
       &amp;= \sum_{x \in A_X}\sum_{y \in A_Y} P(x)P(y)\log\Big(\frac{1}{P(x)}\Big) + \sum_{x \in A_X}\sum_{y \in A_Y} P(x)P(y)\log\Big(\frac{1}{P(y)}\Big)\\
       &amp;= \sum_{x \in A_X} P(x)\log\Big(\frac{1}{P(x)}\Big) + \sum_{y \in A_Y} P(y)\log\Big(\frac{1}{P(y)}\Big)\\
       &amp;= H(X) + H(Y)
\end{align}\]</span></p>
</section>
</section>
<section id="conditional-entropies" class="level3">
<h3 class="anchored" data-anchor-id="conditional-entropies">Conditional Entropies:</h3>
<section id="conditional-entropy-of-x-given-yb_k" class="level4">
<h4 class="anchored" data-anchor-id="conditional-entropy-of-x-given-yb_k">Conditional Entropy of <span class="math inline">\(X\)</span> given <span class="math inline">\(y=b_k\)</span>:</h4>
<p><span class="math display">\[H(X|y=b_k) = \sum_i P(x_i|y=b_k)\log\Big(\frac{1}{P(x_i|y=b_k)}\Big)\tag{4}\]</span></p>
<p>which is simply the entropy of distribution <span class="math inline">\(P(X|y=b_k)\)</span>.</p>
</section>
<section id="conditional-entropy-of-x-given-y" class="level4">
<h4 class="anchored" data-anchor-id="conditional-entropy-of-x-given-y">Conditional Entropy of <span class="math inline">\(X\)</span> given <span class="math inline">\(Y\)</span></h4>
<p>is the average of (4) over <span class="math inline">\(y\)</span>:</p>
<p><span class="math display">\[\begin{align}
H(X|Y) &amp;= \sum_{y \in A_y}P(y)\Bigg[\sum_{x \in A_x} P(x|y)\log\Big(\frac{1}{P(x|y)}\Big)\Bigg]\\ \tag{5}
&amp;= \sum_{x,y \in A_XA_Y} P(x, y)\log\Big(\frac{1}{P(x|y)}\Big)
\end{align}\]</span></p>
<p>which measures the uncertainty that remains about <span class="math inline">\(x\)</span> when <span class="math inline">\(y\)</span> is known.</p>
</section>
</section>
<section id="joint-entropy-and-conditional-entropy" class="level3">
<h3 class="anchored" data-anchor-id="joint-entropy-and-conditional-entropy">Joint Entropy and Conditional Entropy</h3>
<p><span class="math display">\[\begin{align}H(X,Y) &amp;= \sum_{x, y \in A_XA_Y} P(x,y)\log\Big(\frac{1}{P(x, y)}\Big) \\
&amp;=\sum_{x, y \in A_XA_Y} P(x, y)\log\Big(\frac{1}{P(x|y)P(y)}\Big) \\
&amp;= \sum_{x, y \in A_XA_Y} P(x, y)\log\Big(\frac{1}{P(x|y)}\Big) +\sum_{x, y \in A_XA_Y} P(x, y)\log\Big(\frac{1}{P(y)}\Big)\\
&amp;= H(X|Y) + H(Y)\\
\end{align}\]</span></p>
<p>Since entropy is symmetric, just like joint probability, the full relations are:</p>
<p><span class="math display">\[ H(X,Y) = H(Y,X) = H(X|Y) + H(Y) = H(Y|X) + H(X)\tag{6}\]</span></p>
</section>
<section id="mutual-information-between-x-and-y" class="level3">
<h3 class="anchored" data-anchor-id="mutual-information-between-x-and-y">Mutual Information Between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span></h3>
<p>We can define mutual information as the reduction of uncertainty of one variable when another is known.</p>
<p><span class="math display">\[\begin{align}
I(X;Y) &amp;\equiv H(X)- H(X|Y)\tag{7}\\
&amp;= H(X) +H(Y) - H(X,Y)\tag{from 6}\\
&amp;= I(Y;X)\\
&amp;=H(Y)- H(Y|X)\tag{from symmetry}\\
\end{align}\]</span></p>
<p>When put in terms of probabilities, we have the following equivalent definition:</p>
<p><span class="math display">\[\begin{align}I(X;Y) &amp;\equiv \sum_{x, y \in A_XA_Y} P(x,y)\log\Big(\frac{P(x, y)}{P(x)P(y)}\Big)\tag{8a} \\
&amp;=\sum_{x \in A_X}\sum_{y \in A_Y} P(x,y)\log\Big(\frac{P(x, y)}{P(x)P(y)}\Big) \\
&amp;=\sum_{x \in A_X}\sum_{y \in A_Y} P(x,y)\Big[\log\Big(\frac{1}{P(x)}\Big) + \log\Big(\frac{1}{P(y)}\Big) + \log \Big(P(x, y)\Big)\Big] \\
&amp;= H(X) +H(Y) - H(X,Y)
\end{align}\]</span></p>
<p>Just as entropy was the expected value of Shannon information, mutual information is the expected value of <em>pairwise mutual information</em> between points <span class="math inline">\(x_i\)</span> and <span class="math inline">\(y_j\)</span>:</p>
<p><span class="math display">\[M(x_i, y_j) = \log\Big(\frac{P(x_i, y_j)}{P(x_i)P(y_j)}\Big) \tag{8b}\]</span></p>
<p>We can also combine (6) and (7) to rewrite joint entropy in terms of conditional entropies and mutual information:</p>
<p><span class="math display">\[\begin{align}
H(X, Y) &amp;= H(X|Y) + H(Y) \\
&amp;= H(X|Y) + H(Y|X) + I(X;Y)\tag{9}\\
\end{align}\]</span></p>
</section>
<section id="relative-entropy-kullback-leibler-divergence-and-cross-entropy" class="level3">
<h3 class="anchored" data-anchor-id="relative-entropy-kullback-leibler-divergence-and-cross-entropy">Relative Entropy, Kullback-Leibler Divergence and Cross-Entropy</h3>
<p>The <strong>cross entropy</strong> of distributions <span class="math inline">\(Q(x)\)</span> relative to distribution <span class="math inline">\(P(x)\)</span> is similar to (2), but using different distributions:</p>
<p><span class="math display">\[H(p,q) = \sum_x P(x)\log\Big(\frac{1}{Q(x)}\Big)\tag{10}\]</span></p>
<p>The <strong>relative entropy</strong> or <strong>Kullback-Leibler divergence</strong> is a measure of the distance two distributions are from each other, defined as:</p>
<p><span class="math display">\[ D_{KL}(P||Q) = \sum_x P(x) \log\Big(\frac{P(x)}{Q(x)}\Big) \tag{11}\]</span></p>
<p>By separating the logarithm in (8) and using (2), we can see the relation between (8) and (9) as:</p>
<p><span class="math display">\[\begin{align}
D_{KL}(P||Q) &amp;= \sum_x P(x) \log\Big(\frac{P(x)}{Q(x)}\Big)\\
&amp;= \sum_x P(x) \log\Big(\frac{1}{Q(x)}\Big) + \sum_x P(x) \log(P(x))\\
&amp;= H(p,q) - H(X)\\
\end{align}
\]</span></p>
<p>Or better yet:</p>
<p><span class="math display">\[H(p,q) = H(X) + D_{KL}(P||Q)\tag{12}\]</span></p>
<p>This is why the <strong>cross entropy</strong> is used as a loss function in machine learning applications, comparing the distance between ground truth distribution <span class="math inline">\(y\)</span> and estimate <span class="math inline">\(\hat{y}\)</span>. It inherently has the KL divergence within it.</p>
<p><strong>Mutual information</strong> can be seen as the KL divergence between probablity distributions <span class="math inline">\(P(x, y)\)</span> and <span class="math inline">\(P(x)P(y)\)</span>:</p>
<p><span class="math display">\[\begin{align}I(X;Y) &amp;\equiv \sum_{x, y \in A_XA_Y} P(x,y)\log\Big(\frac{P(x, y)}{P(x)P(y)}\Big) \equiv D_{KL} \Big(P(x,y)||P(x)P(y)\Big)\\ \tag{13a}
\end{align}\]</span></p>
<p>We can define a quantity called the <strong>Kullback-Leibler information</strong> which which expresses the reduction of uncertainty of <span class="math inline">\(Y\)</span> after event <span class="math inline">\(x_i\)</span> has been observed. It can be written as the KL Divergence between <span class="math inline">\(P(Y|x_i)\)</span> and <span class="math inline">\(P(Y)\)</span>:</p>
<p><span class="math display">\[ D_{KL} \Big(P(Y|x_i)||P(Y)\Big) = \sum_{ y_j \in A_Y} P(y_j|x_i)\log\Big(\frac{P(y_j|x_i)}{P(y_j)}\Big) \tag{13b}\]</span></p>
<p>Similarly, we have the symmetric case of between <span class="math inline">\(X\)</span> and event <span class="math inline">\(y_j\)</span>:</p>
<p><span class="math display">\[ D_{KL} \Big(P(X|y_j)||P(X)\Big) = \sum_{ x_i \in A_X} P(x_i|y_j)\log\Big(\frac{P(x_i|y_j)}{P(x_i)}\Big) \tag{13c}\]</span></p>
<p>Noticing that joint and conditional probabilities are related by averaging and the log arguments in (13b, 13c) are both equal and equivalent to <span class="math inline">\(\frac{P(x_i,y_j)}{P(x_i)P(y_j)}\)</span> we see can that <strong>mutual information</strong> and <strong>KL-information</strong> are related by:</p>
<p><span class="math display">\[I(X;Y) = \sum_{ x_i \in A_X} P(x_i)D_{KL} \Big(P(Y|x_i)||P(Y)\Big) = \sum_{ y_j \in A_Y}P(y_j)D_{KL} \Big(P(X|y_j)||P(X)\Big)\tag{13d}\]</span></p>
</section>
<section id="jensen-shannon-divergence" class="level3">
<h3 class="anchored" data-anchor-id="jensen-shannon-divergence">Jensen-Shannon Divergence</h3>
<p>KL Divergence is not a true distance in the sense that it lacks symmetry: <span class="math inline">\(D_{KL}(P||Q) \ne D_{KL}(Q||P)\)</span>.</p>
<p>If we define a distance by defining an average distributions <span class="math inline">\(M = \frac{1}{2}(P + Q)\)</span>, we can define a symmetric divergence as</p>
<p><span class="math display">\[ JSD = \frac{1}{2} \Big(D_{KL}(P||M) + D_{KL}(Q||M) \Big)\tag{14}\]</span></p>
</section>
<section id="why-shannon-entropy" class="level2">
<h2 class="anchored" data-anchor-id="why-shannon-entropy">Why Shannon Entropy?</h2>
<p>Of all functional forms <span class="math inline">\(f(x)\)</span>, what merits are there to having <span class="math inline">\(h(x=a_i) =\log_2\frac{1}{p_i}\)</span>? These are similar arguments for the functional form of entropy in physics.</p>
<section id="additive-property-of-independent-variables" class="level3">
<h3 class="anchored" data-anchor-id="additive-property-of-independent-variables">Additive Property of Independent Variables</h3>
<p><span class="math display">\[H(X,Y) = H(X) + H(Y) \tag{15}\]</span></p>
<section id="proof" class="level4">
<h4 class="anchored" data-anchor-id="proof">Proof:</h4>
<p>This stems from the additive properties of logarithms and marginlizing over joint distributions.</p>
<p><span class="math display">\[\begin{align}
H(X,Y) &amp;= \sum_{x, y \in A_XA_Y}P(x,y)\log\Big(\frac{1}{P(x, y)}\Big)\\
       &amp;= \sum_{x, y \in A_XA_Y}P(x)P(y)\log\Big(\frac{1}{P(x)P(y)}\Big)\\
       &amp;= \sum_{x, y \in A_XA_Y}P(x)P(y)\log\Big(\frac{1}{P(x)}\Big) + \sum_{x, y \in A_XA_Y}P(x)P(y)\log\Big(\frac{1}{P(x)}\Big)\\
       &amp;= \sum_{x \in A_X}P(x)\log\Big(\frac{1}{P(x)}\Big) + \sum_{y \in A_Y}P(y)\log\Big(\frac{1}{P(y)}\Big)\\
       &amp;= H(X) + H(Y)\\
\end{align}\]</span></p>
</section>
</section>
<section id="entropy-is-maximized-with-uniform-probability-distributions" class="level3">
<h3 class="anchored" data-anchor-id="entropy-is-maximized-with-uniform-probability-distributions">Entropy is Maximized with Uniform Probability Distributions</h3>
<section id="jensens-inequality" class="level4">
<h4 class="anchored" data-anchor-id="jensens-inequality"><strong>Jensen’s Inequality</strong></h4>
<p>For convex functions:</p>
<p><span class="math display">\[E[f(x)] \ge f(E[x])\tag{16}\]</span></p>
<p>For concave functions:</p>
<p><span class="math display">\[E[f(x)] \le f(E[x])\tag{17}\]</span></p>
</section>
<section id="expectation-of-inverse-probability" class="level4">
<h4 class="anchored" data-anchor-id="expectation-of-inverse-probability"><strong>Expectation of Inverse Probability</strong></h4>
<p><span class="math display">\[E(1/P(x)) = \sum_{x \in A_x} P(x)\frac{1}{P(x)} = |A_x|\tag{18}\]</span> where <span class="math inline">\(|A_x|\)</span> is the size of the set, ie. its cardinality.</p>
</section>
<section id="applications-to-information" class="level4">
<h4 class="anchored" data-anchor-id="applications-to-information">Applications to Information</h4>
<p>Letting <span class="math inline">\(x = \frac{1}{P(x)}\)</span> and <span class="math inline">\(f = \log\)</span>, which is a concave, by Jensen’s equality we have:</p>
<p><span class="math display">\[E\Big[\log\Big(\frac{1}{P(x)}\Big)\Big] \le \log\Big(E\big[\frac{1}{P(x)}\big]\Big)\tag{19}\]</span></p>
<p>The left hand side is the entropy and since we’ve already found the expectation of inverse probability we have:</p>
<p><span class="math display">\[H(X) \le \log A_x\tag{20}\]</span></p>
<p>In a uniform distribution, <span class="math inline">\(P(x) = \frac{1}{A_x}\)</span> so we’d meet equality if that were the distribution of <span class="math inline">\(X\)</span>:</p>
<p><span class="math display">\[\begin{align}
H(X) &amp;=  \sum_{x \in A_X} P(x)\log\Big(\frac{1}{P(x)}\Big)\\
     &amp;= \sum_{x \in A_X} \frac{1}{A_x}\log(A_x)\\
     &amp;= \log(A_x)\tag{21}\\
\end{align}\]</span></p>
<p>In contrast, for a discrete delta distribution, with only one possible outcome <span class="math inline">\(a, P(a)=1\)</span> we have:</p>
<p><span class="math display">\[\begin{align}
H(X) &amp;=  \sum_{x \in A_X} P(x)\log\Big(\frac{1}{P(x)}\Big)\\
     &amp;= 1*\log(1)\\
     &amp;= 0\tag{22}\\
\end{align}\]</span></p>
<p>This shows how entropy can be used to describe how concentrated or spread out a distribution can be with values ranging from <span class="math inline">\(0\)</span> to <span class="math inline">\(\log(A_x)\)</span>. This can be interpreted as the uncertainty of value of the random variable.</p>
</section>
</section>
</section>
<section id="relation-to-log-likelihood" class="level2">
<h2 class="anchored" data-anchor-id="relation-to-log-likelihood">Relation to Log-Likelihood</h2>
<p>Wikipedia’s <a href="https://en.wikipedia.org/wiki/Cross_entropy">article</a> has a nice exposition:</p>
<p>In a classification problem we denote the estimated probability of outcome <span class="math inline">\(i\)</span> as <span class="math inline">\(q_i\)</span> and the empirical probability from the training set of the same event as <span class="math inline">\(p_i\)</span>, where <span class="math inline">\(i \in \{1,..., N\}\)</span>, where the samples are conditionally independent,</p>
<p>the likelihood is:</p>
<p><span class="math display">\[ \mathcal{L} _N(\theta) = \prod_i^N\text{probability of }i^{\text{number of occurences of }i} = \prod_i q_i^{Np_i}\tag{23}\]</span></p>
<p>The log-likelihood is then:</p>
<p><span class="math display">\[l(\theta) = \log \mathcal{L}(\theta) = N\sum^N_{i=1} p_i\log q_i\tag{24}\]</span></p>
<p>Dividing the log-likelihood <span class="math inline">\(l\)</span> by <span class="math inline">\(N\)</span> gives us:</p>
<p><span class="math display">\[ \frac{1}{N}l(\theta) = \sum^N_{i=1} p_i\log q_i = -H(p, q)\tag{25}\]</span></p>
<p><strong>Maximizing the log-likelihood is then equivalent to minizing the cross entropy, and thus the difference between <span class="math inline">\(q_i\)</span> and <span class="math inline">\(p_i\)</span>.</strong></p>
<section id="variation-of-information" class="level3">
<h3 class="anchored" data-anchor-id="variation-of-information">Variation of Information</h3>
<section id="background-relationships" class="level4">
<h4 class="anchored" data-anchor-id="background-relationships">Background Relationships</h4>
<p>For the task of comparing different clusterings, <span class="math inline">\(C, C'\)</span> of the same data <span class="math inline">\(D\)</span>, some useful metrics build upon the concept of entropy. For a clustering <span class="math inline">\(C\)</span>, the probability of a data point being in a cluster <span class="math inline">\(C_k\)</span> is given by:</p>
<p><span class="math display">\[P(k) = \frac{n_k}{n}\tag{26}\]</span></p>
<p>where <span class="math inline">\(n = |D|\)</span> and <span class="math inline">\(n_k = |C_k|\)</span>.</p>
<p>The joint probability in this context refers to the probability a data point belongs to cluster <span class="math inline">\(C_k\)</span> in clustering <span class="math inline">\(C\)</span> and cluster <span class="math inline">\(C'_{k'}\)</span> in clustering <span class="math inline">\(C'\)</span>:</p>
<p><span class="math display">\[P(k,k') = \frac{|C_k \cap C'_{k'}|}{n}\]</span></p>
<p>This leads to defining the conditional probability between a data point belonging to cluster <span class="math inline">\(C_k\)</span> given it’s in <span class="math inline">\(C'_{k'}\)</span> as:</p>
<p><span class="math display">\[P(k | k') = \frac{P(k, k')}{P(k')} = \frac{|C_k \cap C'_{k'}|}{n_{k'}}\tag{27}\]</span></p>
<p>The associated entropy with the clustering is then:</p>
<p><span class="math display">\[ H(C) = -\sum^K_{k=1}P(k)\log\Big(P(k)\Big)\tag{28}\]</span></p>
<p>Mutual information between clusterings is then:</p>
<p><span class="math display">\[I(C; C') = -\sum^K_{k=1}\sum^{K'}_{k'=1}P(k, k')\log\Big(\frac{P(k,k')}{P(k)P(k')}\Big)\tag{29}\]</span></p>
<p>Treating clusterings <span class="math inline">\(C, C'\)</span> as probability distributions of data points, their associated conditional entropies are given by:</p>
<p><span class="math display">\[H(C|C') = -\sum^K_{k=1}\sum^{K'}_{k'=1}P(k, k')\log\Big(\frac{P(k, k')}{P(k')}\Big) \tag{30}\]</span></p>
</section>
<section id="core-definitions" class="level4">
<h4 class="anchored" data-anchor-id="core-definitions">Core Definitions</h4>
<p>The <a href="https://www.sciencedirect.com/science/article/pii/S0047259X06002016"><strong>variation of information</strong></a> (VI) builds on these clustering entropies:</p>
<p><span class="math display">\[ VI(C,C')\equiv H(C) + H(C') - 2I(C;C') \tag{31}\]</span></p>
<p>Grouping mutual information difference to each marginal entropy, we see VI is a sum of conditional entropies:</p>
<p><span class="math display">\[\begin{align}
VI(C,C')&amp;\equiv H(C) - I(C;C') + H(C') - I(C;C') \tag{32}\\
&amp;= H(C|C') + H(C'|C)\\
\end{align}\]</span></p>
<p>This can also be rewritten in terms of joint entropy using (9) to substitute out the conditional entropy terms in (32):</p>
<p><span class="math display">\[VI(C,C') = H(C, C') - I(C; C')\tag{33}\]</span></p>
<p>A computationally useful form is (32) in terms of explicit probabilities:</p>
<p><span class="math display">\[\begin{align}
VI(C, C') &amp;= H(C|C') + H(C'|C)\\
&amp;= -\sum^K_{k=1}\sum^{K'}_{k'=1}P(k, k')\log\Big(\frac{P(k, k')}{P(k')}\Big) + P(k, k')\log\Big(\frac{P(k, k')}{P(k)}\Big) \\
&amp;= -\sum^K_{k=1}\sum^{K'}_{k'=1}P(k, k')\Bigg[\log\Big(\frac{P(k, k')}{P(k')}\Big) + \log\Big(\frac{P(k, k')}{P(k)}\Big)\Bigg]\\
\end{align}\tag{34}\]</span></p>
</section>
<section id="useful-properties" class="level4">
<h4 class="anchored" data-anchor-id="useful-properties">Useful Properties</h4>
<p>To make better sense of the VI distance, it helps to consider the total set of clusterings of a dataset <span class="math inline">\(D\)</span>. The most extreme members, at least in regards to entropy values are: - <span class="math inline">\(\hat{1}\)</span>:<br>
- <span class="math inline">\(K = 1\)</span> clusters - <span class="math inline">\(H(\hat{1}) = 0\)</span><br>
- <span class="math inline">\(\hat{0}\)</span>: - <span class="math inline">\(K = n\)</span> clusters - with <span class="math inline">\(H(\hat{0}) = \log n\)</span> - <span class="math inline">\(C^U_K\)</span>: - <span class="math inline">\(K\)</span> equal sized clusters, <span class="math inline">\(K \ge 1\)</span> - with <span class="math inline">\(H(C^U_K) = \log K\)</span></p>
<p>Joint probability between any clustering <span class="math inline">\(C\)</span> with index <span class="math inline">\(k\)</span> and <span class="math inline">\(\hat{1}\)</span> index <span class="math inline">\(k' = 1\)</span> is given depends on terms <span class="math inline">\(P(k, k') = \frac{|C_k \cap C'_{k'}|}{n} = \frac{|C_k|}{n} = P(k) * 1\)</span>.</p>
<p>This implies: - the clusterings <span class="math inline">\(\hat{1}\)</span> and <span class="math inline">\(C\)</span> are independent of each other - <span class="math inline">\(I(C; \hat{1}) = 0\)</span> - <span class="math inline">\(H(C, \hat{1})  = H(C) + H(\hat{1}) = H(C)\)</span> - <span class="math inline">\(VI(C, \hat{1}) = H(C)\)</span></p>
<ul>
<li><p><span class="math inline">\(VI(C,C') \le \log n\)</span>, where equality is in the case of clusters <span class="math inline">\(\hat{1}\)</span> (only one cluster) and <span class="math inline">\(\hat{0}\)</span> (<span class="math inline">\(n\)</span> clusters).</p></li>
<li><p>If <span class="math inline">\(C\)</span> and <span class="math inline">\(C'\)</span> have at most <span class="math inline">\(K\)</span> clusters each, with <span class="math inline">\(K \le \sqrt{n}\)</span>, then <span class="math inline">\(VI(C, C') \le 2 \log K\)</span></p></li>
</ul>
</section>
</section>
<section id="practical-implementations" class="level3">
<h3 class="anchored" data-anchor-id="practical-implementations">Practical Implementations</h3>
<div id="cell-64" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> entropy</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.special <span class="im">import</span> rel_entr, kl_div</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.spatial.distance <span class="im">import</span> jensenshannon</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd </span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> igraph.clustering <span class="im">import</span> compare_communities, Clustering </span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> List</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> defaultdict</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> itertools <span class="im">import</span> chain</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="jensen-shannon-divergence-1" class="level4">
<h4 class="anchored" data-anchor-id="jensen-shannon-divergence-1">Jensen-Shannon Divergence</h4>
<div id="cell-66" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> jsd(a: np.array, b: np.array, base: <span class="bu">int</span> <span class="op">=</span> <span class="va">None</span>):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Calculate and return the Jensen-Shannon Divergence"""</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> (a <span class="op">+</span> b) <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (entropy(a, m, base<span class="op">=</span>base) <span class="op">+</span> entropy(b, m, base<span class="op">=</span>base)) <span class="op">/</span> <span class="dv">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-67" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>seq <span class="op">=</span> [<span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>]</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>s <span class="op">=</span> pd.Series(seq).value_counts()</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>vals <span class="op">=</span> s.index</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>probs <span class="op">=</span> s.values<span class="op">/</span><span class="bu">len</span>(seq)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-68" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> np.array([<span class="dv">9</span>, <span class="dv">12</span>, <span class="dv">4</span>])</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>])</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> np.array([<span class="fl">0.36</span>, <span class="fl">0.48</span>, <span class="fl">0.16</span>])</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>q <span class="op">=</span> np.array([<span class="fl">0.30</span>, <span class="fl">0.50</span>, <span class="fl">0.20</span>])</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> (a <span class="op">+</span> b)<span class="op">/</span><span class="dv">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-69" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>np.sqrt(jsd(p, q))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>0.050803321756356906</code></pre>
</div>
</div>
<div id="cell-70" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>entropy(a, b)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>0.0852996013183706</code></pre>
</div>
</div>
<div id="cell-71" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>kl_div(a, b)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>array([11.7750212 , 18.8188798 ,  2.54517744])</code></pre>
</div>
</div>
<div id="cell-72" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>rel_entr(a, b)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<pre><code>array([19.7750212 , 29.8188798 ,  5.54517744])</code></pre>
</div>
</div>
<div id="cell-73" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>jsd(a, b, base<span class="op">=</span><span class="va">None</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>0.03793843282690725</code></pre>
</div>
</div>
<div id="cell-74" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>np.sqrt(jsd(a, b, base<span class="op">=</span><span class="va">None</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="11">
<pre><code>0.19477790641370815</code></pre>
</div>
</div>
<div id="cell-75" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>jensenshannon(p, q, base<span class="op">=</span><span class="va">None</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="12">
<pre><code>0.050803321756356906</code></pre>
</div>
</div>
</section>
<section id="variation-of-information-1" class="level4">
<h4 class="anchored" data-anchor-id="variation-of-information-1">Variation of Information</h4>
<ul>
<li><p>Variation of information (VI) can be computed by using (34) and looping through the <span class="math inline">\(n_k \times n_{k'}\)</span> cluster intersections, setting entropies of disjoint clusters to zero.</p></li>
<li><p>The <code>igraph</code> <a href="https://igraph.org/python/api/0.9.8/igraph.clustering.html">library</a> function <code>compare_communities</code> has an option to compute VI, with the <code>method='vi'</code> option.</p>
<ul>
<li><p><code>igraph</code> accepts clusterings as <em>membership lists</em>, where the list’s indices correspond to nodes, and the values correspond to cluster indices.</p></li>
<li><p>Membership lists constrain cluster indices to range from <span class="math inline">\(\{0, 1, ..., n-1\}\)</span> where <span class="math inline">\(n\)</span> is the number of nodes or data points</p></li>
<li><p>Membership lists can be converted to a <em>cluster list</em>, a list of lists where each inner list corresponds to a cluster of data points, represented by unique integer indices</p></li>
<li><p>Converting a cluster list to a membership list requires the following:</p>
<ul>
<li>each data point must be deterministically assigned to a list index, such as sorting</li>
<li>each cluster must be given a cluster id in the range <span class="math inline">\(\{0, 1, ..., n-1\}\)</span></li>
<li>each list index must be assigned the cluster id to which it is a member of</li>
<li>the VI calculation should be invariant to cluster id assignment within the same clusterings, (swapping what cluster is labeled 0 vs 1 should have no effect)</li>
</ul></li>
</ul></li>
</ul>
<p>Meilă, Marina. “Comparing Clusterings—an Information Based Distance.” Journal of Multivariate Analysis 98, no. 5 (May 1, 2007): 873–95. https://doi.org/10.1016/j.jmva.2006.11.013.</p>
<div id="cell-79" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> var_info(C1: List[List[<span class="bu">int</span>]], C2: List[List[<span class="bu">int</span>]], base: <span class="bu">float</span> <span class="op">=</span> <span class="va">None</span>):</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Variation of information between two clusterings"""</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> <span class="bu">sum</span>(<span class="bu">len</span>(i) <span class="cf">for</span> i <span class="kw">in</span> C1)</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> n <span class="op">==</span> <span class="bu">sum</span>(<span class="bu">len</span>(j) <span class="cf">for</span> j <span class="kw">in</span> C2)</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>    total <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> C1:</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>        p_i <span class="op">=</span> <span class="bu">len</span>(i) <span class="op">/</span> n</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> C2:</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>            p_j <span class="op">=</span> <span class="bu">len</span>(j) <span class="op">/</span> n</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>            p_ij <span class="op">=</span> <span class="bu">len</span>(<span class="bu">set</span>(i) <span class="op">&amp;</span> <span class="bu">set</span>(j)) <span class="op">/</span> n</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> p_ij <span class="op">&gt;</span> <span class="fl">0.0</span>:</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>                total <span class="op">-=</span> p_ij <span class="op">*</span> (np.log(p_ij <span class="op">/</span> p_i) <span class="op">+</span> np.log(p_ij <span class="op">/</span> p_j))</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> base <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>        total <span class="op">/=</span> np.log(base)</span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> total</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_membership_lists(clusters: List[List[<span class="bu">int</span>]]):</span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Return a list of cluster indices each element of a set belongs to"""</span></span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> <span class="bu">sorted</span>(<span class="bu">list</span>(chain(<span class="op">*</span>clusters)))</span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>    mem_list <span class="op">=</span> [<span class="op">-</span><span class="dv">1</span> <span class="cf">for</span> i <span class="kw">in</span> data]</span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> cluster_id, cluster <span class="kw">in</span> <span class="bu">enumerate</span>(clusters):</span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> data_point <span class="kw">in</span> cluster:</span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a>            mem_list_idx <span class="op">=</span> data.index(data_point)</span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a>            mem_list[mem_list_idx] <span class="op">=</span> cluster_id</span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> mem_list</span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_cluster_lists(mem_list: List[<span class="bu">int</span>]):</span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Convert membership list to list of lists corresponding to clusters"""</span></span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a>    d <span class="op">=</span> defaultdict(<span class="bu">list</span>)</span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> idx, cluster <span class="kw">in</span> <span class="bu">enumerate</span>(mem_list):</span>
<span id="cb19-31"><a href="#cb19-31" aria-hidden="true" tabindex="-1"></a>        d[cluster].append(idx)</span>
<span id="cb19-32"><a href="#cb19-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">list</span>(d.values())        </span>
<span id="cb19-33"><a href="#cb19-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-34"><a href="#cb19-34" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> map_indices(data: List) <span class="op">-&gt;</span> List[<span class="bu">int</span>]:</span>
<span id="cb19-35"><a href="#cb19-35" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Dictionary of indices of ordered data"""</span></span>
<span id="cb19-36"><a href="#cb19-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> <span class="bu">len</span>(data) <span class="op">==</span> <span class="bu">len</span>(<span class="bu">set</span>(data))</span>
<span id="cb19-37"><a href="#cb19-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> {val: idx <span class="cf">for</span> idx, val <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">sorted</span>(data))}</span>
<span id="cb19-38"><a href="#cb19-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-39"><a href="#cb19-39" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> standardize_cluster_list(clusters: List[List[<span class="bu">int</span>]]) <span class="op">-&gt;</span> List[List[<span class="bu">int</span>]]:</span>
<span id="cb19-40"><a href="#cb19-40" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Replace a set of values with ordered indices in cluster membership lists"""</span></span>
<span id="cb19-41"><a href="#cb19-41" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> <span class="bu">list</span>(chain(<span class="op">*</span>clusters))</span>
<span id="cb19-42"><a href="#cb19-42" aria-hidden="true" tabindex="-1"></a>    data_idx <span class="op">=</span> map_indices(data)</span>
<span id="cb19-43"><a href="#cb19-43" aria-hidden="true" tabindex="-1"></a>    standardized <span class="op">=</span> []</span>
<span id="cb19-44"><a href="#cb19-44" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> cluster <span class="kw">in</span> clusters:</span>
<span id="cb19-45"><a href="#cb19-45" aria-hidden="true" tabindex="-1"></a>        standardized_cluster <span class="op">=</span> [data_idx[val] <span class="cf">for</span> val <span class="kw">in</span> cluster]</span>
<span id="cb19-46"><a href="#cb19-46" aria-hidden="true" tabindex="-1"></a>        standardized.append(standardized_cluster)</span>
<span id="cb19-47"><a href="#cb19-47" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> standardized</span>
<span id="cb19-48"><a href="#cb19-48" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb19-49"><a href="#cb19-49" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb19-50"><a href="#cb19-50" aria-hidden="true" tabindex="-1"></a>    </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-80" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Cluster lists</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>X1 <span class="op">=</span> [ [<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>], [<span class="dv">6</span>,<span class="dv">7</span>,<span class="dv">8</span>,<span class="dv">9</span>,<span class="dv">10</span>] ]</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>Y1 <span class="op">=</span> [ [<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>], [<span class="dv">6</span>,<span class="dv">7</span>,<span class="dv">8</span>,<span class="dv">9</span>,<span class="dv">10</span>] ]</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>X2 <span class="op">=</span> [ [<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>], [<span class="dv">5</span>,<span class="dv">6</span>,<span class="dv">7</span>,<span class="dv">8</span>,<span class="dv">9</span>,<span class="dv">10</span>] ]</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>Y2 <span class="op">=</span> [ [<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>], [<span class="dv">7</span>,<span class="dv">8</span>,<span class="dv">9</span>,<span class="dv">10</span>] ]</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>X3 <span class="op">=</span> [ [<span class="dv">1</span>,<span class="dv">2</span>], [<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>], [<span class="dv">6</span>,<span class="dv">7</span>,<span class="dv">8</span>], [<span class="dv">9</span>,<span class="dv">10</span>]]</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>Y3 <span class="op">=</span> [ [<span class="dv">10</span>,<span class="dv">2</span>,<span class="dv">3</span>], [<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>,<span class="dv">7</span>], [<span class="dv">8</span>,<span class="dv">9</span>,<span class="dv">1</span>] ]</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>X4 <span class="op">=</span> [ [<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>,<span class="dv">7</span>,<span class="dv">8</span>,<span class="dv">9</span>,<span class="dv">10</span>] ]</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>Y4 <span class="op">=</span> [ [<span class="dv">1</span>], [<span class="dv">2</span>], [<span class="dv">3</span>], [<span class="dv">4</span>], [<span class="dv">5</span>], [<span class="dv">6</span>], [<span class="dv">7</span>], [<span class="dv">8</span>], [<span class="dv">9</span>], [<span class="dv">10</span>] ]</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>X5 <span class="op">=</span> [ [<span class="dv">10</span>,<span class="dv">2</span>,<span class="dv">3</span>], [<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>,<span class="dv">7</span>], [<span class="dv">8</span>,<span class="dv">9</span>,<span class="dv">1</span>] ]</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>Y5 <span class="op">=</span> [[<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>,<span class="dv">7</span>], [<span class="dv">8</span>,<span class="dv">9</span>,<span class="dv">1</span>], [<span class="dv">10</span>,<span class="dv">2</span>,<span class="dv">3</span>]]</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>X_ <span class="op">=</span> [X1, X2, X3, X4]</span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a>Y_ <span class="op">=</span> [Y1, Y2, Y3, Y4]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-81" class="cell" data-execution_count="163">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> create_membership_lists(X5)</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> create_membership_lists(Y5)</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(a)</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(b)</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>compare_communities(a, b, method<span class="op">=</span><span class="st">'vi'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[2, 0, 0, 1, 1, 1, 1, 2, 2, 0]
[1, 2, 2, 0, 0, 0, 0, 1, 1, 2]</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="163">
<pre><code>0.0</code></pre>
</div>
</div>
<div id="cell-82" class="cell" data-execution_count="76">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>var_info(X5, Y5)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="76">
<pre><code>0.0</code></pre>
</div>
</div>
<div id="cell-83" class="cell" data-execution_count="136">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>compare_communities([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>], [<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">0</span>], method<span class="op">=</span><span class="st">'vi'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="136">
<pre><code>0.4620981203732968</code></pre>
</div>
</div>
<div id="cell-84" class="cell" data-execution_count="165">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>d <span class="op">=</span> { <span class="st">'X'</span>: X_,</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>      <span class="st">'Y'</span>: Y_,</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>      <span class="st">'VI custom (nats)'</span>: [var_info(X, Y, <span class="dv">2</span>) <span class="cf">for</span> X, Y <span class="kw">in</span> <span class="bu">zip</span>(X_, Y_)],</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>      <span class="st">'VI-igraph (nats)'</span>: [compare_communities(create_membership_lists(X), create_membership_lists(Y), method<span class="op">=</span><span class="st">'vi'</span>)</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>                           <span class="cf">for</span> X, Y <span class="kw">in</span> <span class="bu">zip</span>(X_, Y_)]</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>     }</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(d)</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>df</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="165">
<div>
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">X</th>
<th data-quarto-table-cell-role="th">Y</th>
<th data-quarto-table-cell-role="th">VI custom (nats)</th>
<th data-quarto-table-cell-role="th">VI-igraph (nats)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>[[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]</td>
<td>[[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>[[1, 2, 3, 4], [5, 6, 7, 8, 9, 10]]</td>
<td>[[1, 2, 3, 4, 5, 6], [7, 8, 9, 10]]</td>
<td>1.101955</td>
<td>1.101955</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>[[1, 2], [3, 4, 5], [6, 7, 8], [9, 10]]</td>
<td>[[10, 2, 3], [4, 5, 6, 7], [8, 9, 1]]</td>
<td>2.301955</td>
<td>2.301955</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]]</td>
<td>[[1], [2], [3], [4], [5], [6], [7], [8], [9], ...</td>
<td>3.321928</td>
<td>3.321928</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</div>
<div id="cell-85" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>Y3 <span class="op">=</span> [ [<span class="dv">10</span>,<span class="dv">2</span>,<span class="dv">3</span>], [<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>,<span class="dv">7</span>], [<span class="dv">8</span>,<span class="dv">9</span>,<span class="dv">1</span>] ]</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>Y31 <span class="op">=</span> [[<span class="dv">9</span>, <span class="dv">1</span>, <span class="dv">2</span>], [<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>], [<span class="dv">7</span>, <span class="dv">8</span>, <span class="dv">0</span>]]</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>Y3_ <span class="op">=</span> [<span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">0</span>]</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>created <span class="op">=</span> create_cluster_lists(Y3_)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-86" class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>created <span class="op">==</span> Y31</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="32">
<pre><code>False</code></pre>
</div>
</div>
<div id="cell-87" class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>created</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="31">
<pre><code>[[0, 7, 8], [1, 2, 9], [3, 4, 5, 6]]</code></pre>
</div>
</div>
<div id="cell-88" class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>var_info(created, Y31)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="33">
<pre><code>0.0</code></pre>
</div>
</div>


</section>
</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>