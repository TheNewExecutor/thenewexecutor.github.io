{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From David McKay's [*Information Theory, Inference, and Learning Algorithms*](https://www.inference.org.uk/itprnn/book.pdf), we acknowledge the following notation: \n",
    "\n",
    "- **ensemble or random variable X**: consists of the triple $(x, A_x, P_x)$ \n",
    "    - $x$ is the *outcome* of the random variable $X$\n",
    "    - $A_x$ is the *alphabet* or set of possible values: $\\{a_1, a_2, ..., a_i, ..., a_I\\}$\n",
    "        - this is sometimes noted as $\\{x_1, x_2, ..., x_i, ..., x_I\\}$\n",
    "    - $P_x$ are the associated probabilities, $P(x=a_i) = p_i$ of these outcomes: $\\{p_1, p_2, ..., p_i, ..., p_I\\}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shannon Information of Measurement $x_i$\n",
    "\n",
    "$$h(x_i) = \\log\\Big(\\frac{1}{P(x_i)}\\Big)\\tag{1}$$\n",
    "\n",
    "where $P(x_i)$ is probability of outcome $x_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shannon Entropy of Distribution X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$H(X) = E\\big[h(x_i)\\big] = \\sum_{x \\in A_X} P(x)\\log\\Big(\\frac{1}{P(x)}\\Big)\\tag{2}$$\n",
    "\n",
    "where $A_X$ is the **alphabet** or set of outcomes for random variable $X$. It's also known as the **marginal entropy**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joint Entropy of Distributions X, Y:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$H(X,Y) = \\sum_{x, y \\in A_XA_Y} P(x,y)\\log\\Big(\\frac{1}{P(x, y)}\\Big)\\tag{3}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Independent Distributions\n",
    "\n",
    "$$\\begin{align}\n",
    "H(X,Y) &= \\sum_{x, y \\in A_XA_Y} P(x,y)\\log\\Big(\\frac{1}{P(x, y)}\\Big)\\\\\n",
    "       &= \\sum_{x \\in A_X}\\sum_{y \\in A_Y} P(x)P(y)\\log\\Big(\\frac{1}{P(x)P(y)}\\Big)\\\\\n",
    "       &= \\sum_{x \\in A_X}\\sum_{y \\in A_Y} P(x)P(y)\\log\\Big(\\frac{1}{P(x)}\\Big) + \\sum_{x \\in A_X}\\sum_{y \\in A_Y} P(x)P(y)\\log\\Big(\\frac{1}{P(y)}\\Big)\\\\\n",
    "       &= \\sum_{x \\in A_X} P(x)\\log\\Big(\\frac{1}{P(x)}\\Big) + \\sum_{y \\in A_Y} P(y)\\log\\Big(\\frac{1}{P(y)}\\Big)\\\\\n",
    "       &= H(X) + H(Y)\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Entropies:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conditional Entropy of $X$ given $y=b_k$: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$H(X|y=b_k) = \\sum_i P(x_i|y=b_k)\\log\\Big(\\frac{1}{P(x_i|y=b_k)}\\Big)\\tag{4}$$\n",
    "\n",
    "which is simply the entropy of distribution $P(X|y=b_k)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conditional Entropy of $X$ given $Y$\n",
    "\n",
    "is the average of (4) over $y$:\n",
    "\n",
    "$$\\begin{align}\n",
    "H(X|Y) &= \\sum_{y \\in A_y}P(y)\\Bigg[\\sum_{x \\in A_x} P(x|y)\\log\\Big(\\frac{1}{P(x|y)}\\Big)\\Bigg]\\\\ \\tag{5}\n",
    "&= \\sum_{x,y \\in A_XA_Y} P(x, y)\\log\\Big(\\frac{1}{P(x|y)}\\Big)\n",
    "\\end{align}$$\n",
    "\n",
    "which measures the uncertainty that remains about $x$ when $y$ is known.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joint Entropy and Conditional Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align}H(X,Y) &= \\sum_{x, y \\in A_XA_Y} P(x,y)\\log\\Big(\\frac{1}{P(x, y)}\\Big) \\\\\n",
    "&=\\sum_{x, y \\in A_XA_Y} P(x, y)\\log\\Big(\\frac{1}{P(x|y)P(y)}\\Big) \\\\\n",
    "&= \\sum_{x, y \\in A_XA_Y} P(x, y)\\log\\Big(\\frac{1}{P(x|y)}\\Big) +\\sum_{x, y \\in A_XA_Y} P(x, y)\\log\\Big(\\frac{1}{P(y)}\\Big)\\\\\n",
    "&= H(X|Y) + H(Y)\\\\\n",
    "\\end{align}$$\n",
    "\n",
    "\n",
    "\n",
    "Since entropy is symmetric, just like joint probability, the full relations are:\n",
    "\n",
    "$$ H(X,Y) = H(Y,X) = H(X|Y) + H(Y) = H(Y|X) + H(X)\\tag{6}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual Information Between $X$ and $Y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define mutual information as the reduction of uncertainty of one variable when another is known. \n",
    "\n",
    "$$\\begin{align}\n",
    "I(X;Y) &\\equiv H(X)- H(X|Y)\\tag{7}\\\\\n",
    "&= H(X) +H(Y) - H(X,Y)\\tag{from 6}\\\\\n",
    "&= I(Y;X)\\\\\n",
    "&=H(Y)- H(Y|X)\\tag{from symmetry}\\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When put in terms of probabilities, we have the following equivalent definition:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align}I(X;Y) &\\equiv \\sum_{x, y \\in A_XA_Y} P(x,y)\\log\\Big(\\frac{P(x, y)}{P(x)P(y)}\\Big)\\tag{8a} \\\\ \n",
    "&=\\sum_{x \\in A_X}\\sum_{y \\in A_Y} P(x,y)\\log\\Big(\\frac{P(x, y)}{P(x)P(y)}\\Big) \\\\\n",
    "&=\\sum_{x \\in A_X}\\sum_{y \\in A_Y} P(x,y)\\Big[\\log\\Big(\\frac{1}{P(x)}\\Big) + \\log\\Big(\\frac{1}{P(y)}\\Big) + \\log \\Big(P(x, y)\\Big)\\Big] \\\\\n",
    "&= H(X) +H(Y) - H(X,Y)\n",
    "\\end{align}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as entropy was the expected value of Shannon information, mutual information is the expected value of *pairwise mutual information* between points $x_i$ and $y_j$:\n",
    "\n",
    "$$M(x_i, y_j) = \\log\\Big(\\frac{P(x_i, y_j)}{P(x_i)P(y_j)}\\Big) \\tag{8b}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also combine (6) and (7) to rewrite joint entropy in terms of conditional entropies and mutual information:\n",
    "\n",
    "$$\\begin{align}\n",
    "H(X, Y) &= H(X|Y) + H(Y) \\\\ \n",
    "&= H(X|Y) + H(Y|X) + I(X;Y)\\tag{9}\\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative Entropy, Kullback-Leibler Divergence and Cross-Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **cross entropy** of distributions $Q(x)$ relative to distribution $P(x)$ is similar to (2), but using different distributions:\n",
    "\n",
    "$$H(p,q) = \\sum_x P(x)\\log\\Big(\\frac{1}{Q(x)}\\Big)\\tag{10}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **relative entropy** or **Kullback-Leibler divergence** is a measure of the distance two distributions are from each other, defined as:\n",
    "\n",
    "$$ D_{KL}(P||Q) = \\sum_x P(x) \\log\\Big(\\frac{P(x)}{Q(x)}\\Big) \\tag{11}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By separating the logarithm in (8) and using (2), we can see the relation between (8) and (9) as:\n",
    "\n",
    "$$\\begin{align}\n",
    "D_{KL}(P||Q) &= \\sum_x P(x) \\log\\Big(\\frac{P(x)}{Q(x)}\\Big)\\\\\n",
    "&= \\sum_x P(x) \\log\\Big(\\frac{1}{Q(x)}\\Big) + \\sum_x P(x) \\log(P(x))\\\\\n",
    "&= H(p,q) - H(X)\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Or better yet:\n",
    "\n",
    "$$H(p,q) = H(X) + D_{KL}(P||Q)\\tag{12}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is why the **cross entropy** is used as a loss function in machine learning applications, comparing the distance between ground truth distribution $y$ and estimate $\\hat{y}$. It inherently has the KL divergence within it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mutual information** can be seen as the KL divergence between probablity distributions $P(x, y)$ and $P(x)P(y)$:\n",
    "\n",
    "$$\\begin{align}I(X;Y) &\\equiv \\sum_{x, y \\in A_XA_Y} P(x,y)\\log\\Big(\\frac{P(x, y)}{P(x)P(y)}\\Big) \\equiv D_{KL} \\Big(P(x,y)||P(x)P(y)\\Big)\\\\ \\tag{13a}\n",
    "\\end{align}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define a quantity called the **Kullback-Leibler information** which which expresses the reduction of uncertainty of $Y$ after event $x_i$ has been observed. It can be written as the KL Divergence between $P(Y|x_i)$ and $P(Y)$:\n",
    "\n",
    "$$ D_{KL} \\Big(P(Y|x_i)||P(Y)\\Big) = \\sum_{ y_j \\in A_Y} P(y_j|x_i)\\log\\Big(\\frac{P(y_j|x_i)}{P(y_j)}\\Big) \\tag{13b}$$\n",
    "\n",
    "Similarly, we have the symmetric case of between $X$ and event $y_j$:\n",
    "\n",
    "$$ D_{KL} \\Big(P(X|y_j)||P(X)\\Big) = \\sum_{ x_i \\in A_X} P(x_i|y_j)\\log\\Big(\\frac{P(x_i|y_j)}{P(x_i)}\\Big) \\tag{13c}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noticing that joint and conditional probabilities are related by averaging and the log arguments in (13b, 13c) are both equal and equivalent to $\\frac{P(x_i,y_j)}{P(x_i)P(y_j)}$ we see can that **mutual information** and **KL-information** are related by:\n",
    "\n",
    "$$I(X;Y) = \\sum_{ x_i \\in A_X} P(x_i)D_{KL} \\Big(P(Y|x_i)||P(Y)\\Big) = \\sum_{ y_j \\in A_Y}P(y_j)D_{KL} \\Big(P(X|y_j)||P(X)\\Big)\\tag{13d}$$ \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jensen-Shannon Divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KL Divergence is not a true distance in the sense that it lacks symmetry: $D_{KL}(P||Q) \\ne D_{KL}(Q||P)$. \n",
    "\n",
    "If we define a distance by defining an average distributions $M = \\frac{1}{2}(P + Q)$, we can define a symmetric divergence as\n",
    "\n",
    "$$ JSD = \\frac{1}{2} \\Big(D_{KL}(P||M) + D_{KL}(Q||M) \\Big)\\tag{14}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Shannon Entropy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of all functional forms $f(x)$, what merits are there to having $h(x=a_i) =\\log_2\\frac{1}{p_i}$? These are similar arguments for the functional form of entropy in physics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additive Property of Independent Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$H(X,Y) = H(X) + H(Y) \\tag{15}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proof:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This stems from the additive properties of logarithms and marginlizing over joint distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "H(X,Y) &= \\sum_{x, y \\in A_XA_Y}P(x,y)\\log\\Big(\\frac{1}{P(x, y)}\\Big)\\\\\n",
    "       &= \\sum_{x, y \\in A_XA_Y}P(x)P(y)\\log\\Big(\\frac{1}{P(x)P(y)}\\Big)\\\\\n",
    "       &= \\sum_{x, y \\in A_XA_Y}P(x)P(y)\\log\\Big(\\frac{1}{P(x)}\\Big) + \\sum_{x, y \\in A_XA_Y}P(x)P(y)\\log\\Big(\\frac{1}{P(x)}\\Big)\\\\\n",
    "       &= \\sum_{x \\in A_X}P(x)\\log\\Big(\\frac{1}{P(x)}\\Big) + \\sum_{y \\in A_Y}P(y)\\log\\Big(\\frac{1}{P(y)}\\Big)\\\\\n",
    "       &= H(X) + H(Y)\\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy is Maximized with Uniform Probability Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Jensen's Inequality**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convex functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$E[f(x)] \\ge f(E[x])\\tag{16}$$\n",
    "\n",
    "For concave functions:\n",
    "\n",
    "$$E[f(x)] \\le f(E[x])\\tag{17}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Expectation of Inverse Probability**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$E(1/P(x)) = \\sum_{x \\in A_x} P(x)\\frac{1}{P(x)} = |A_x|\\tag{18}$$ where $|A_x|$ is the size of the set, ie. its cardinality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applications to Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letting $x = \\frac{1}{P(x)}$ and $f = \\log$, which is a concave, by Jensen's equality we have:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$E\\Big[\\log\\Big(\\frac{1}{P(x)}\\Big)\\Big] \\le \\log\\Big(E\\big[\\frac{1}{P(x)}\\big]\\Big)\\tag{19}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The left hand side is the entropy and since we've already found the expectation of inverse probability we have:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$H(X) \\le \\log A_x\\tag{20}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a uniform distribution, $P(x) = \\frac{1}{A_x}$ so we'd meet equality if that were the distribution of $X$:\n",
    "\n",
    "$$\\begin{align}\n",
    "H(X) &=  \\sum_{x \\in A_X} P(x)\\log\\Big(\\frac{1}{P(x)}\\Big)\\\\\n",
    "     &= \\sum_{x \\in A_X} \\frac{1}{A_x}\\log(A_x)\\\\\n",
    "     &= \\log(A_x)\\tag{21}\\\\\n",
    "\\end{align}$$\n",
    "\n",
    "In contrast, for a discrete delta distribution, with only one possible outcome $a, P(a)=1$ we have:\n",
    "\n",
    "$$\\begin{align}\n",
    "H(X) &=  \\sum_{x \\in A_X} P(x)\\log\\Big(\\frac{1}{P(x)}\\Big)\\\\\n",
    "     &= 1*\\log(1)\\\\\n",
    "     &= 0\\tag{22}\\\\\n",
    "\\end{align}$$\n",
    " \n",
    " This shows how entropy can be used to describe how concentrated or spread out a distribution can be with values ranging from $0$ to $\\log(A_x)$. This can be interpreted as the uncertainty of value of the random variable.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relation to Log-Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wikipedia's [article](https://en.wikipedia.org/wiki/Cross_entropy) has a nice exposition:\n",
    "\n",
    "In a classification problem we denote the estimated probability of outcome $i$ as $q_i$ and the empirical probability from the training set of the same event as $p_i$, where $i \\in \\{1,..., N\\}$, where the samples are conditionally independent,\n",
    "\n",
    "the likelihood is:\n",
    "\n",
    "$$ \\mathcal{L} _N(\\theta) = \\prod_i^N\\text{probability of }i^{\\text{number of occurences of }i} = \\prod_i q_i^{Np_i}\\tag{23}$$\n",
    "\n",
    "The log-likelihood is then:\n",
    "\n",
    "\n",
    "$$l(\\theta) = \\log \\mathcal{L}(\\theta) = N\\sum^N_{i=1} p_i\\log q_i\\tag{24}$$\n",
    "\n",
    "\n",
    "Dividing the log-likelihood $l$ by $N$ gives us:\n",
    "\n",
    "$$ \\frac{1}{N}l(\\theta) = \\sum^N_{i=1} p_i\\log q_i = -H(p, q)\\tag{25}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Maximizing the log-likelihood is then equivalent to minizing the cross entropy, and thus the difference between $q_i$ and $p_i$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variation of Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Background Relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the task of comparing different clusterings, $C, C'$ of the same data $D$, some useful metrics build upon the concept of entropy. For a clustering $C$, the probability of a data point being in a cluster $C_k$ is given by:\n",
    "\n",
    "$$P(k) = \\frac{n_k}{n}\\tag{26}$$\n",
    "\n",
    "where $n = |D|$ and $n_k = |C_k|$.\n",
    "\n",
    "The joint probability in this context refers to the probability a data point belongs to cluster $C_k$ in clustering $C$ and cluster $C'_{k'}$ in clustering $C'$:\n",
    "\n",
    "$$P(k,k') = \\frac{|C_k \\cap C'_{k'}|}{n}$$\n",
    "\n",
    "\n",
    "This leads to defining the conditional probability between a data point belonging to cluster $C_k$ given it's in $C'_{k'}$ as:  \n",
    "\n",
    "$$P(k | k') = \\frac{P(k, k')}{P(k')} = \\frac{|C_k \\cap C'_{k'}|}{n_{k'}}\\tag{27}$$\n",
    "\n",
    "\n",
    "The associated entropy with the clustering is then:\n",
    "\n",
    "$$ H(C) = -\\sum^K_{k=1}P(k)\\log\\Big(P(k)\\Big)\\tag{28}$$\n",
    "\n",
    "\n",
    "\n",
    "Mutual information between clusterings is then:\n",
    "\n",
    "$$I(C; C') = -\\sum^K_{k=1}\\sum^{K'}_{k'=1}P(k, k')\\log\\Big(\\frac{P(k,k')}{P(k)P(k')}\\Big)\\tag{29}$$\n",
    "\n",
    "Treating clusterings $C, C'$ as probability distributions of data points, their associated conditional entropies are given by:\n",
    "\n",
    "$$H(C|C') = -\\sum^K_{k=1}\\sum^{K'}_{k'=1}P(k, k')\\log\\Big(\\frac{P(k, k')}{P(k')}\\Big) \\tag{30}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Core Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [**variation of information**](https://www.sciencedirect.com/science/article/pii/S0047259X06002016) (VI) builds on these clustering entropies:\n",
    "\n",
    "$$ VI(C,C')\\equiv H(C) + H(C') - 2I(C;C') \\tag{31}$$\n",
    "\n",
    "\n",
    "Grouping mutual information difference to each marginal entropy, we see VI is a sum of conditional entropies:\n",
    "\n",
    "$$\\begin{align}\n",
    "VI(C,C')&\\equiv H(C) - I(C;C') + H(C') - I(C;C') \\tag{32}\\\\\n",
    "&= H(C|C') + H(C'|C)\\\\\n",
    "\\end{align}$$\n",
    "\n",
    "\n",
    "This can also be rewritten in terms of joint entropy using (9) to substitute out the conditional entropy terms in (32):\n",
    "\n",
    "$$VI(C,C') = H(C, C') - I(C; C')\\tag{33}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A computationally useful form is (32) in terms of explicit probabilities:\n",
    "\n",
    "\n",
    "$$\\begin{align}\n",
    "VI(C, C') &= H(C|C') + H(C'|C)\\\\\n",
    " &= -\\sum^K_{k=1}\\sum^{K'}_{k'=1}P(k, k')\\log\\Big(\\frac{P(k, k')}{P(k')}\\Big) + P(k, k')\\log\\Big(\\frac{P(k, k')}{P(k)}\\Big) \\\\\n",
    " &= -\\sum^K_{k=1}\\sum^{K'}_{k'=1}P(k, k')\\Bigg[\\log\\Big(\\frac{P(k, k')}{P(k')}\\Big) + \\log\\Big(\\frac{P(k, k')}{P(k)}\\Big)\\Bigg]\\\\\n",
    "\\end{align}\\tag{34}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Useful Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make better sense of the VI distance, it helps to consider the total set of clusterings of a dataset $D$. The most extreme members, at least in regards to entropy values are:\n",
    "- $\\hat{1}$:  \n",
    "    - $K = 1$ clusters\n",
    "    - $H(\\hat{1}) = 0$  \n",
    "- $\\hat{0}$: \n",
    "    - $K = n$ clusters\n",
    "    - with $H(\\hat{0}) = \\log n$\n",
    "- $C^U_K$:\n",
    "    - $K$ equal sized clusters, $K \\ge 1$\n",
    "    - with $H(C^U_K) = \\log K$\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joint probability between any clustering $C$ with index $k$ and $\\hat{1}$ index $k' = 1$ is given depends on terms $P(k, k') = \\frac{|C_k \\cap C'_{k'}|}{n} = \\frac{|C_k|}{n} = P(k) * 1$. \n",
    "\n",
    "This implies:\n",
    "- the clusterings $\\hat{1}$ and $C$ are independent of each other\n",
    "- $I(C; \\hat{1}) = 0$\n",
    "- $H(C, \\hat{1})  = H(C) + H(\\hat{1}) = H(C)$\n",
    "- $VI(C, \\hat{1}) = H(C)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - $VI(C,C') \\le \\log n$, where equality is in the case of clusters $\\hat{1}$ (only one cluster) and $\\hat{0}$ ($n$ clusters).\n",
    " \n",
    " \n",
    " - If $C$ and $C'$ have at most $K$ clusters each, with $K \\le \\sqrt{n}$, then $VI(C, C') \\le 2 \\log K$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "from scipy.special import rel_entr, kl_div\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from igraph.clustering import compare_communities, Clustering \n",
    "from typing import List\n",
    "from collections import defaultdict\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jensen-Shannon Divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsd(a: np.array, b: np.array, base: int = None):\n",
    "    \"\"\"Calculate and return the Jensen-Shannon Divergence\"\"\"\n",
    "    m = (a + b) / 2\n",
    "    return (entropy(a, m, base=base) + entropy(b, m, base=base)) / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = [2, 1, 2, 2, 2, 2, 2]\n",
    "\n",
    "s = pd.Series(seq).value_counts()\n",
    "vals = s.index\n",
    "probs = s.values/len(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([9, 12, 4])\n",
    "b = np.array([1, 1, 1])\n",
    "p = np.array([0.36, 0.48, 0.16])\n",
    "q = np.array([0.30, 0.50, 0.20])\n",
    "m = (a + b)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.050803321756356906"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(jsd(p, q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0852996013183706"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11.7750212 , 18.8188798 ,  2.54517744])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl_div(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([19.7750212 , 29.8188798 ,  5.54517744])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_entr(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03793843282690725"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsd(a, b, base=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19477790641370815"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(jsd(a, b, base=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.050803321756356906"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jensenshannon(p, q, base=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variation of Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Variation of information (VI) can be computed by using (34) and looping through the $n_k \\times n_{k'}$ cluster intersections, setting entropies of disjoint clusters to zero.  \n",
    "\n",
    "\n",
    "- The `igraph` [library](https://igraph.org/python/api/0.9.8/igraph.clustering.html) function `compare_communities` has an option to compute VI, with the `method='vi'` option.\n",
    "\n",
    "    - `igraph` accepts clusterings as *membership lists*, where the list's indices correspond to nodes, and the values correspond to cluster indices.\n",
    "    - Membership lists constrain cluster indices to range from $\\{0, 1, ..., n-1\\}$ where $n$ is the number of nodes or data points\n",
    "    - Membership lists can be converted to a *cluster list*, a list of lists where each inner list corresponds to a cluster of data points, represented by unique integer indices     \n",
    "    \n",
    "    - Converting a cluster list to a membership list requires the following:\n",
    "        - each data point must be deterministically assigned to a list index, such as sorting\n",
    "        - each cluster must be given a cluster id in the range $\\{0, 1, ..., n-1\\}$\n",
    "        - each list index must be assigned the cluster id to which it is a member of\n",
    "        - the VI calculation should be invariant to cluster id assignment within the same clusterings, (swapping what cluster is labeled 0 vs 1 should have no effect) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meilă, Marina. “Comparing Clusterings—an Information Based Distance.” Journal of Multivariate Analysis 98, no. 5 (May 1, 2007): 873–95. https://doi.org/10.1016/j.jmva.2006.11.013.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_info(C1: List[List[int]], C2: List[List[int]], base: float = None):\n",
    "    \"\"\"Variation of information between two clusterings\"\"\"\n",
    "    n = sum(len(i) for i in C1)\n",
    "    assert n == sum(len(j) for j in C2)\n",
    "    total = 0.0\n",
    "    for i in C1:\n",
    "        p_i = len(i) / n\n",
    "        for j in C2:\n",
    "            p_j = len(j) / n\n",
    "            p_ij = len(set(i) & set(j)) / n\n",
    "            if p_ij > 0.0:\n",
    "                total -= p_ij * (np.log(p_ij / p_i) + np.log(p_ij / p_j))\n",
    "    if base is not None:\n",
    "        total /= np.log(base)\n",
    "    return total\n",
    "\n",
    "def create_membership_lists(clusters: List[List[int]]):\n",
    "    \"\"\"Return a list of cluster indices each element of a set belongs to\"\"\"\n",
    "    data = sorted(list(chain(*clusters)))\n",
    "    mem_list = [-1 for i in data]\n",
    "    for cluster_id, cluster in enumerate(clusters):\n",
    "        for data_point in cluster:\n",
    "            mem_list_idx = data.index(data_point)\n",
    "            mem_list[mem_list_idx] = cluster_id\n",
    "    return mem_list\n",
    "\n",
    "def create_cluster_lists(mem_list: List[int]):\n",
    "    \"\"\"Convert membership list to list of lists corresponding to clusters\"\"\"\n",
    "    d = defaultdict(list)\n",
    "    for idx, cluster in enumerate(mem_list):\n",
    "        d[cluster].append(idx)\n",
    "    return list(d.values())        \n",
    "\n",
    "def map_indices(data: List) -> List[int]:\n",
    "    \"\"\"Dictionary of indices of ordered data\"\"\"\n",
    "    assert len(data) == len(set(data))\n",
    "    return {val: idx for idx, val in enumerate(sorted(data))}\n",
    "\n",
    "def standardize_cluster_list(clusters: List[List[int]]) -> List[List[int]]:\n",
    "    \"\"\"Replace a set of values with ordered indices in cluster membership lists\"\"\"\n",
    "    data = list(chain(*clusters))\n",
    "    data_idx = map_indices(data)\n",
    "    standardized = []\n",
    "    for cluster in clusters:\n",
    "        standardized_cluster = [data_idx[val] for val in cluster]\n",
    "        standardized.append(standardized_cluster)\n",
    "    return standardized\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster lists\n",
    "X1 = [ [1,2,3,4,5], [6,7,8,9,10] ]\n",
    "Y1 = [ [1,2,3,4,5], [6,7,8,9,10] ]\n",
    "\n",
    "X2 = [ [1,2,3,4], [5,6,7,8,9,10] ]\n",
    "Y2 = [ [1,2,3,4,5,6], [7,8,9,10] ]\n",
    "\n",
    "X3 = [ [1,2], [3,4,5], [6,7,8], [9,10]]\n",
    "Y3 = [ [10,2,3], [4,5,6,7], [8,9,1] ]\n",
    "\n",
    "X4 = [ [1,2,3,4,5,6,7,8,9,10] ]\n",
    "Y4 = [ [1], [2], [3], [4], [5], [6], [7], [8], [9], [10] ]\n",
    "\n",
    "X5 = [ [10,2,3], [4,5,6,7], [8,9,1] ]\n",
    "Y5 = [[4,5,6,7], [8,9,1], [10,2,3]]\n",
    "\n",
    "\n",
    "X_ = [X1, X2, X3, X4]\n",
    "Y_ = [Y1, Y2, Y3, Y4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 0, 0, 1, 1, 1, 1, 2, 2, 0]\n",
      "[1, 2, 2, 0, 0, 0, 0, 1, 1, 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = create_membership_lists(X5)\n",
    "b = create_membership_lists(Y5)\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "compare_communities(a, b, method='vi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_info(X5, Y5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4620981203732968"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_communities([1, 2, 0], [2, 2, 0], method='vi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>VI custom (nats)</th>\n",
       "      <th>VI-igraph (nats)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]</td>\n",
       "      <td>[[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[1, 2, 3, 4], [5, 6, 7, 8, 9, 10]]</td>\n",
       "      <td>[[1, 2, 3, 4, 5, 6], [7, 8, 9, 10]]</td>\n",
       "      <td>1.101955</td>\n",
       "      <td>1.101955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[1, 2], [3, 4, 5], [6, 7, 8], [9, 10]]</td>\n",
       "      <td>[[10, 2, 3], [4, 5, 6, 7], [8, 9, 1]]</td>\n",
       "      <td>2.301955</td>\n",
       "      <td>2.301955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]]</td>\n",
       "      <td>[[1], [2], [3], [4], [5], [6], [7], [8], [9], ...</td>\n",
       "      <td>3.321928</td>\n",
       "      <td>3.321928</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         X  \\\n",
       "0      [[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]   \n",
       "1      [[1, 2, 3, 4], [5, 6, 7, 8, 9, 10]]   \n",
       "2  [[1, 2], [3, 4, 5], [6, 7, 8], [9, 10]]   \n",
       "3        [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]]   \n",
       "\n",
       "                                                   Y  VI custom (nats)  \\\n",
       "0                [[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]          0.000000   \n",
       "1                [[1, 2, 3, 4, 5, 6], [7, 8, 9, 10]]          1.101955   \n",
       "2              [[10, 2, 3], [4, 5, 6, 7], [8, 9, 1]]          2.301955   \n",
       "3  [[1], [2], [3], [4], [5], [6], [7], [8], [9], ...          3.321928   \n",
       "\n",
       "   VI-igraph (nats)  \n",
       "0          0.000000  \n",
       "1          1.101955  \n",
       "2          2.301955  \n",
       "3          3.321928  "
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = { 'X': X_,\n",
    "      'Y': Y_,\n",
    "      'VI custom (nats)': [var_info(X, Y, 2) for X, Y in zip(X_, Y_)],\n",
    "      'VI-igraph (nats)': [compare_communities(create_membership_lists(X), create_membership_lists(Y), method='vi')\n",
    "                           for X, Y in zip(X_, Y_)]\n",
    "     }\n",
    "\n",
    "df = pd.DataFrame(d)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y3 = [ [10,2,3], [4,5,6,7], [8,9,1] ]\n",
    "Y31 = [[9, 1, 2], [3,4,5,6], [7, 8, 0]]\n",
    "Y3_ = [2, 0, 0, 1, 1, 1, 1, 2, 2, 0]\n",
    "created = create_cluster_lists(Y3_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "created == Y31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 7, 8], [1, 2, 9], [3, 4, 5, 6]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_info(created, Y31)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
